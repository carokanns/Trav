{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En ny learning och validering   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beskrivning:  \n",
    "\n",
    "Normal learning:  \n",
    "- Learning: gör webscrape av omgång(ar) och kör en learning\n",
    "\n",
    "GridSearch optimering:  \n",
    "- gör gridsearch av de olika modellerna typ1, typ6, typ9 och typ16 samt meta_modellen\n",
    "  - inkludera imbalanced-lösningar\n",
    "  - spara de bästa hyperparametrarna\n",
    "  \n",
    "Värdera:  \n",
    "- Cross_validate: ts_split and ts cross_validate of the stack of models plus the meta_model\n",
    "  - save the results in pickle\n",
    "- Vinst: beräknar vinsten för de olika modellerna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Först kolla artiklar om stacking, cv för stacking samt cv för stacking av timeseries \n",
    "- https://machinelearningmastery.com/implementing-stacking-scratch-python/   (Även kod i Pieces)  \n",
    "Även allmänt om stacking ensembles  \n",
    "- https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning/  \n",
    "Slutligen CV för Timeseries stacking  (se kod i Pieces)  \n",
    "- https://datascience.stackexchange.com/questions/41378/how-to-apply-stacking-cross-validation-for-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generella funktioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moduler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from IPython.display import display\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "import typ as tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# läs in data\n",
    "def läs_in_data_och_förbered():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    \n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # läs in FEATIRES.txt\n",
    "    with open('..\\\\FEATURES.txt', 'r') as f:    \n",
    "        features = f.read().splitlines()\n",
    "    \n",
    "    assert len(features) == len(X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'   \n",
    "    assert set(features) == set(X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#              namn, ant_hästar, proba, Kelly,  motst_ant, motst_diff, ant_favoriter,   only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,       False,          0,            False,    True)\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,       True,           2,            True,     False)\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,       True,           2,            True,     True)\n",
    "typ16= tp.Typ('typ16',True,       True, True,      2,       True,           2,            False,    True)\n",
    "\n",
    "typer = [typ6, typ1, typ9, typ16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "             'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV av typ6, typ1, typ9, typ16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "    # Läs in data\n",
    "    # remove_features\n",
    "\n",
    "# modeller konfigurering, CatBoostClassifier  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a GridSearch for CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def create_grid_search(X_, y_, typ, verbose=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    # remove features\n",
    "    X = typ.prepare_for_model(X)\n",
    "    if not typ.streck:\n",
    "        X.drop('streck', axis=1, inplace=True)\n",
    "    \n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features\\n',cat_features)\n",
    "    X = remove_features(X, remove_mer=['datum','avd'])\n",
    "    # get numerical features and cat_features\n",
    "    num_features = list(X.select_dtypes(include=[np.number]).columns)\n",
    "    cat_features = list(X.select_dtypes(include=['object']).columns)\n",
    "    # print('cat_features:\\n',cat_features,'\\n')\n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "    # create a GridSearch\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "        'iterations': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        'bagging_temperature': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'rsm': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'depth_per_tree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'border_count': [32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "        'fold_permutation_block_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'od_pval': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'od_wait': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3,  1.5, 1.7, 1.9],\n",
    "        'od_type': ['IncToDec', 'Iter'],\n",
    "    }\n",
    "\n",
    "    # X.datum = pd.to_datetime(X.datum)\n",
    "    # X.datum = X.datum.dt.date\n",
    "    # alla_datum = X.datum.unique()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # for train_index, test_index in tscv.split(X):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        # X_train, X_test = X[train_index], X[test_index]\n",
    "        # y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = CatBoostClassifier(iterations=500, loss_function='Logloss', eval_metric='AUC',\n",
    "                               use_best_model=False, early_stopping_rounds=100, verbose=verbose,)\n",
    "\n",
    "    grid = {'learning_rate': [0.002,0.005],\n",
    "            'depth': [2,4,6,8],\n",
    "            'l2_leaf_reg': [8, 9, 10,11]}\n",
    "    \n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "\n",
    "    grid_search_result = model.grid_search(grid,\n",
    "                                        X=Pool(X, y, cat_features=cat_features),\n",
    "                                        cv=tscv.split(X),\n",
    "                                        shuffle=False,\n",
    "                                        search_by_train_test_split=False,\n",
    "                                        verbose=verbose,\n",
    "                                        plot=True)\n",
    "    return grid_search_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testa grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total GridSearchCV där jag använder egen TimeseriesSplit\n",
    "if False:\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results6 = create_grid_search(X, y, typ6)\n",
    "\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results1 = create_grid_search(X, y, typ1)\n",
    "\n",
    "    X,y = läs_in_data_och_förbered()\n",
    "    results9 = create_grid_search(X, y, typ9)\n",
    "\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results16 = create_grid_search(X, y, typ16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults = pd.read_csv('results.csv')\n",
    "def results(typ, dfResults, comm=[]):\n",
    "    s = \"results\"+str(typ)\n",
    "    temp1 =globals()[s]\n",
    "    temp = pd.DataFrame(temp1['cv_results'])\n",
    "    temp['typ'] = typ\n",
    "    max=temp['test-AUC-mean'].max()\n",
    "    temp=temp[temp['test-AUC-mean']==max]\n",
    "    if len(comm)>0:\n",
    "        temp['comm'] = comm\n",
    "    str(temp1['params'])\n",
    "    temp['best_params'] = str(temp1['params']).replace('{','').replace('}','').replace('depth','d').replace('l2_leaf_reg','l2').replace('learning_rate','LR')\n",
    "    dfResults = pd.concat([dfResults,temp])\n",
    "    return dfResults\n",
    "\n",
    "#### Slår ihop alla resultaten från GridSearch i funktionen ovan\n",
    "if False:\n",
    "    dfResults = results(1, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(6, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(9, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(16, dfResults, 'ökade iter=500')\n",
    "    dfResults.to_csv('results.csv', index=False)\n",
    "dfResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spara modeller och optimala parms (kanske skall läras på del 1-4?)\n",
    "- Predict på den sista 5:e delen och skapa input till meta-model\n",
    "- gridsearchcv meta_model på detta. Spara optimala parms\n",
    "- avsluta med en learn på hela 5:- delen med optimala parms och spara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_temp(typ, X_, y_, iterations, depth, learning_rate, l2_leaf_reg,save=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    \n",
    "    typ.learn(X, y, learning_rate=learning_rate, iterations=iterations, depth=depth, l2_leaf_reg=l2_leaf_reg, save=save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def learn_all_models(totals = False, save=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_until = train_index[-1:][0]\n",
    "        test_from = train_until+1\n",
    "\n",
    "    if totals == False:\n",
    "        ### sista 5:e delen av data\n",
    "        print('train_until',train_until, 'test_from',test_from)\n",
    "\n",
    "        X_train, X_test = X.iloc[:test_from], X.iloc[test_from:]\n",
    "        y_train, y_test = y[:test_from], y[test_from:]\n",
    "    else:\n",
    "        ### all data\n",
    "        print('train all')\n",
    "\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "    results = pd.read_csv('results.csv')\n",
    "\n",
    "    # typ6: 'd': 2, 'l2': 9, 'LR': 0.005\n",
    "    print(results[results.typ==6][['typ','iterations','best_params']].iloc[-1].values)\n",
    "    iter=460\n",
    "    d=2\n",
    "    LR=0.005\n",
    "    L2=9\n",
    "    learn_temp(typ6, X_train, y_train, iterations=iter, depth=d, learning_rate=LR, l2_leaf_reg=L2,save=save)\n",
    "\n",
    "    # typ1: 'd': 4, 'l2': 8, 'LR': 0.005\n",
    "    print(results[results.typ == 1][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 432\n",
    "    d = 4\n",
    "    LR = 0.005\n",
    "    L2 = 9\n",
    "    learn_temp(typ1, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "    # typ9: 'd': 2, 'l2': 10, 'LR': 0.005\n",
    "    print(results[results.typ == 9][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 496\n",
    "    d = 2\n",
    "    LR = 0.005\n",
    "    L2 = 10\n",
    "    learn_temp(typ9, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "\n",
    "    # typ16:'d': 2, 'l2': 10, 'LR': 0.005\n",
    "    print(results[results.typ == 16][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 496\n",
    "    d = 2\n",
    "    LR = 0.005\n",
    "    L2 = 10\n",
    "    learn_temp(typ16, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "\n",
    "if False:\n",
    "    learn_all_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapa ett Kelly-värde baserat på streck omvandlat till odds\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open('rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack for learning meta_model\n",
    "def skapa_stack_learning(X_, y, save=True):\n",
    "    X=X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    for typ in typer:\n",
    "            nr = typ.name[3:]\n",
    "            stacked_data['proba'+nr] = typ.predict(X)\n",
    "            stacked_data['kelly' + nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "\n",
    "    print(stacked_data.columns)\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    return stacked_data,y   # enbart stack-info\n",
    "\n",
    "if True: # förbered data och kör stacking\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_from = test_index[0]   # sic!\n",
    "\n",
    "    ### sista 5:e delen av data\n",
    "    print('train_from for meta', train_from)\n",
    "\n",
    "    X_train, y_train = X.iloc[train_from:], y[train_from:]\n",
    "    X_stack,y_stack = skapa_stack_learning(X_train, y_train, False)\n",
    "    display(X_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit meta_model\n",
    "def learn_meta_model(X, y,  save=True,\n",
    "                     n_estimators=100, \n",
    "                     max_depth=None,\n",
    "                     min_samples_split=2, \n",
    "                     min_samples_leaf=1, \n",
    "                     max_features='auto', \n",
    "                     max_leaf_nodes=None,\n",
    "                     max_samples=None,):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "\n",
    "    meta_model = RandomForestClassifier(\n",
    "       class_weight='balanced',    #{0:1,1:10},\n",
    "       oob_score=True, verbose=1, n_jobs=8, random_state=2022,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_samples=max_samples,)\n",
    "    \n",
    "    meta_model.fit(X, y)\n",
    "\n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9430916552667579\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open('../modeller/meta_model.model', 'wb') as f:\n",
    "            pickle.dump(meta_model, f)\n",
    "\n",
    "    return meta_model\n",
    "\n",
    "if True:\n",
    "    grid ={\n",
    "        'n_estimators': [10,100,200],\n",
    "        'max_depth': [None,2,4,6],\n",
    "        'min_samples_split': [2,4,6],\n",
    "        'min_samples_leaf':[1],  # 2,3],\n",
    "        'max_features': ['auto'], # 'log2', None],\n",
    "        'max_leaf_nodes': [None], # 20, 10, 5],\n",
    "        'max_samples':[None], # 0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    }\n",
    "    \n",
    "    oob_dict = {}\n",
    "    n=1\n",
    "    keys = list(grid.keys())\n",
    "    for i, key in enumerate(keys[:3]):\n",
    "        # loop over all values in key\n",
    "        for value in grid[key]:\n",
    "            for key2 in keys[i+1:3]:\n",
    "                for value2 in grid[key2]:\n",
    "                    for key3 in keys[i+2:3]:\n",
    "                        for value3 in grid[key3]:\n",
    "                            \n",
    "                            print('konf', n, key, value, key2, value2, key3, value3)\n",
    "                \n",
    "                            meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                                  n_estimators = value,\n",
    "                                                  max_depth = value2,\n",
    "                                                  min_samples_split = value3,\n",
    "                                                  min_samples_leaf = 1,  # 2,3], \n",
    "                                                  max_features = 'auto', # 'log2', None], \n",
    "                                                  max_leaf_nodes = None, # 20, 10, 5],  \n",
    "                                                  max_samples    = None,)\n",
    "                            # keep n:oob_score in dict\n",
    "                            oob_dict[n] = meta_model.oob_score_\n",
    "        \n",
    "                            n+=1\n",
    "    print(oob_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print key where max value in oob_dict\n",
    "print(max(oob_dict, key=oob_dict.get))  \n",
    "print('Manuellt hittade jag: n_estimators=100, max_depth=None,min_samples_split=2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final: learn meta_model med optimala parms och learn allt för modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #####    X_stack och y_stack måste vara definierade ovan   #####\n",
    "    meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=None,\n",
    "                                  min_samples_split=2,\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ### alla värden enligt ovan körningar för modellerna ###\n",
    "    learn_all_models(totals=True, save=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktioner för att prioritera mellan hästar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se även kelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för en omgång (ett datum) ta ut största diff för streck per avd \n",
    "# om only_clear=True, enbart för diff >= 25\n",
    "def lista_med_favoriter(df_, ant, only_clear):\n",
    "    df = df_.copy()\n",
    "    min_diff = 25 if only_clear else 0\n",
    "    # sortera på avd,streck\n",
    "    df = df.sort_values(['avd', 'streck'], ascending=[False, False])\n",
    "    diff_list = []\n",
    "    for avd in range(1, 8):\n",
    "        diff = df.loc[df.avd == avd].streck.iloc[0] - \\\n",
    "            df.loc[df.avd == avd].streck.iloc[1]\n",
    "        if diff >= min_diff:\n",
    "            diff_list.append((avd, diff))\n",
    "\n",
    "     # sortera på diff\n",
    "    diff_list = sorted(diff_list, key=lambda x: x[1], reverse=True)\n",
    "    return diff_list[:ant]\n",
    "\n",
    "# temp is a list of tuples (avd, diff). check if avd is in the list\n",
    "def check_avd(avd, temp):\n",
    "    for t in temp:\n",
    "        if t[0] == avd:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_insats(df):\n",
    "    insats = 0\n",
    "    # group by avd\n",
    "    summa = df.groupby('avd').avd.count().prod() / 2\n",
    "    return summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning-fasen\n",
    "Bygg en separat v75_learning_ny.ipynb   \n",
    "Bygg en separat v75_validate_ny.ipynb  \n",
    "Bygg en separat v75_hyperparms.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gör en scrape på senaste veckan (behövs inte i denna test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum < test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "# _, rf = model_streck_to_odds(X)   # used in next cell\n",
    "\n",
    "# # spara rf\n",
    "# import pickle\n",
    "# with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "#     pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all_data.csv \n",
    "Baka ihop senaste vekan med all_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skapa_stack_learning(X_, y, features, iterations=1000, random_state=2022, verbose=False, save=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack med proba och kelly\n",
    "    X måste ha datum och avd\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    \n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        model = typ.learn(X, y, features, iterations=iterations, save=save, verbose=verbose)\n",
    "        stacked_data['proba'+nr] = typ.predict(X) \n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "    \n",
    "    # print(stacked_data.columns)\n",
    "    return stacked_data   # enbart stack-info\n",
    "\n",
    "# fit meta_model\n",
    "def learn_meta_model(X,y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "    \n",
    "    meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "    meta_model.fit(X, y)\n",
    "    \n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "    # pickle save stacking\n",
    "    pickle.dump(meta_model, open('..\\\\modeller\\\\meta.model', 'wb'))\n",
    "    \n",
    "    return meta_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature list from a file (ej plac)\n",
    "def read_feature_list(file='../FEATURES.txt'):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liten_cv_timeseries_demo(x_,y_):\n",
    "    X = x_.copy()\n",
    "    y = y_.copy()\n",
    "    print('Holdout validation data from X = X[~validation]')\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        print(f\"TRAIN each model on this: {train_index[0]}-{train_index[-1]}, Predict on: {test_index[0]}-{test_index[-1]}\")\n",
    "        print('save the predictions of each model')\n",
    "        X_train = X.loc[train_index]\n",
    "        X_test = X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    print('Train meta_model on all models predictions')\n",
    "    print('validate meta_model on the validation data')\n",
    "    \n",
    "    print('\\nHandle stratified and unbalanced data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör learning-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = read_feature_list(\"../FEATURES.txt\")\n",
    "\n",
    "print(\"hur mycket skall sparas för input till learning meta-model?\")\n",
    "print('kör vs.v75_sraping')\n",
    "print('tvätta data')\n",
    "X, y = läs_in_data_och_förbered()\n",
    "print(' concat med all_data')\n",
    "\n",
    "assert X.shape[1] == len(FEATURES), f'X_train.shape[1] {X.shape[1]} != len(FEATURES) {len(FEATURES)}'\n",
    "assert set(X.columns) == set(FEATURES), f'set(X_train.columns) {set(X.columns)} != set(FEATURES) {set(FEATURES)}'\n",
    "X = X[FEATURES]  # för att få kolumner i rätt ordning\n",
    "andel_train = 0.60\n",
    "andel_meta_train = 0.75\n",
    "alla_datum = X.datum.unique()\n",
    "train_datum=alla_datum[:int(len(alla_datum)*andel_train)]\n",
    "test_datum=alla_datum[int(len(alla_datum)*andel_train):]\n",
    "meta_train = test_datum[:int(len(test_datum)*andel_meta_train)]\n",
    "meta_test = test_datum[int(len(test_datum)*andel_meta_train):]\n",
    "print(f'alla_datum {len(alla_datum)} varav train_datum {len(train_datum)} och test_datum {len(test_datum)}')\n",
    "print(f'meta_train {len(meta_train)} och meta_test {len(meta_test)}')\n",
    "\n",
    "if True:\n",
    "    liten_cv_timeseries_demo(X, y)\n",
    "else:    \n",
    "    X_stacked = skapa_stack_learning(X_train, y_train, FEATURES, iterations=100,random_state=2022, verbose=False, save=True)\n",
    "    # display(X_stacked)\n",
    "    meta_model = learn_meta_model(X_stacked, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spela-fasen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "import V75_scraping as vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape-funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v75_scrape():\n",
    "    df, strukna = vs.v75_scraping(history=True, resultat=False)\n",
    "    \n",
    "    # df = pd.read_csv('../sparad_scrape.csv')\n",
    "    for f in ['häst','bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ metod\n",
    "# Ta fram rader för varje typ enligt test-resultaten innan\n",
    "# låt meta_model välja mellan typerna - hur? Hur maximer insatsen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som bygger stack-data från modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för stacking ta med alla hästar per typ och proba plus kelly\n",
    "def build_stack_df(X_):\n",
    "    X = X_.copy()\n",
    "    stacked_data = X[['datum','avd', 'startnr','häst']].copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba'+nr], X[['streck']], None)\n",
    "    return stacked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion där meta_model gör predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_predict(X_):\n",
    "    # X_ innehåller även datum,startnr och avd\n",
    "    extra = ['datum', 'avd', 'startnr', 'häst']\n",
    "    assert list(X_.columns[:4]) == extra, 'meta_model måste ha datum, avd och startnr, häst för att kunna välja'\n",
    "    X = X_.copy()\n",
    "    with open('../modeller\\\\meta.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "        \n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict_proba(X.iloc[:,-8:])[:,1]\n",
    "    my_columns = extra + list(X.columns)[-9:] \n",
    "    \n",
    "    return X[my_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som väljer rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_cost(antal_rader):\n",
    "    cost = (antal_rader**2)/2\n",
    "    return antal_rader,cost\n",
    "\n",
    "def välj_rad(X_):\n",
    "    \n",
    "    max_insats=320\n",
    "    veckans_rad = X_.copy()\n",
    "    veckans_rad['välj'] = False\n",
    "\n",
    "    for avd in veckans_rad.avd.unique():\n",
    "        max_pred = veckans_rad[veckans_rad.avd == avd]['meta_predict'].max()\n",
    "        veckans_rad.loc[(veckans_rad.avd == avd) & (veckans_rad.meta_predict == max_pred), 'välj'] = True\n",
    "    antal_rader=1    \n",
    "    veckans_rad = veckans_rad.sort_values(by=['meta_predict'], ascending=False)\n",
    "    \n",
    "    # 3. Använda ensam favorit för ett par avd? Kolla test-resultat\n",
    "    # for each row in rad, välj=True if select_func(cost,avd) == True\n",
    "    cost = antal_rader*0.5\n",
    "    for i, row in veckans_rad.iterrows():\n",
    "        new_antal,new_cost = comp_cost(antal_rader+1)\n",
    "        # print(the_cost)\n",
    "        if new_cost > max_insats:\n",
    "            break\n",
    "        \n",
    "        antal_rader = new_antal\n",
    "        cost = new_cost\n",
    "        veckans_rad.loc[i, 'välj'] = True\n",
    "        # print(cost)\n",
    "    veckans_rad.sort_values(by=['välj', 'avd'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    return veckans_rad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör hela välj-rad-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = v75_scrape()\n",
    "print(X.datum.unique())\n",
    "df_stack = build_stack_df(X,FEATURES)\n",
    "df_meta = meta_predict(df_stack)\n",
    "df_meta.reset_index(drop=True, inplace=True)\n",
    "veckans_rad = välj_rad(df_meta)\n",
    "# rename columns \n",
    "veckans_rad.rename(columns={'startnr':'nr', 'meta_predict':'Meta', 'välj':'Välj'}, inplace=True)\n",
    "\n",
    "display(veckans_rad[veckans_rad.välj])\n",
    "print('kostnad', veckans_rad.välj.sum()**2/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En massa gammal - kanske reusable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for Learn!\n",
    "def init_learn():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    # X = ordinal_enc(X, 'häst')\n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features:', cat_features)\n",
    "    X.head()\n",
    "    return X, y, cat_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modell för streck_to_odds - skall vara fix och inte ändras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum <\n",
    "                             test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "linreg, rf = model_streck_to_odds(X)   # used in next cell\n",
    "# spara rf\n",
    "import pickle\n",
    "with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engångsgrej för att initiera typ-instanserna med learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bara första gången. Initierar Typ-klassen\n",
    "def learn(X_train, y_train, X_test=None, y_test=None, cat_features=[],  iterations=1000,  verbose=False):\n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    X_train = remove_features(X_train, remove_mer=['avd','datum'])\n",
    "    cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "    if X_test is not None:\n",
    "        X_test = remove_features(X_test, remove_mer=['avd', 'datum'])\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "        cbc.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100, use_best_model=True, verbose=verbose)\n",
    "    else:\n",
    "        cbc.fit(train_pool, use_best_model=True, verbose=verbose)\n",
    "    return cbc\n",
    "\n",
    "def beräkna_datum(X,fract=0.75):\n",
    "    ix_break = int(len(X.datum.unique())*fract)\n",
    "    test_start = X.datum.unique()[ix_break]\n",
    "    return test_start\n",
    "\n",
    "if False: # Kör en initiering av typerna \n",
    "    Xlearn, cat_features= prepare_for_catboost(X)  \n",
    "    # print(Xlearn.columns)\n",
    "    for typ in [typ6, typ1, typ9, typ16]:\n",
    "        print(typ.name)\n",
    "        Xtyp = typ.prepare_for_model(Xlearn)                                 ###########\n",
    "\n",
    "        if not typ.streck:                                                ################\n",
    "            Xtyp.drop('streck', axis=1, inplace=True)\n",
    "            \n",
    "        if True: # använda X_test    \n",
    "            test_start = beräkna_datum(Xtyp)    \n",
    "            X_train, X_test = Xtyp[Xtyp.datum < test_start], Xtyp[Xtyp.datum >= test_start]\n",
    "            y_train, y_test = y[X_train.index], y[X_test.index]\n",
    "            # print('innan learn',X_train.columns)\n",
    "            typ_model = learn(X_train, y_train, X_test, y_test)  ##########\n",
    "            print('best iteration',typ_model.best_iteration_)                             ##########\n",
    "            print('best score',    typ_model.best_score_)                                 ##########\n",
    "        # save model\n",
    "        typ.save_model(typ_model)                                                       ##########                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa typ6 till typ16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = läs_in_data_och_förbered()\n",
    "X,cat_features = tp.prepare_for_catboost(X)\n",
    "typ6.learn(X,y, iterations=33) # best iter = 25 {'Logloss': 0.23245952928761984, 'AUC': 0.8262112132692319}\n",
    "typ1.learn(X,y, iterations=39) # best iter = 39 {'Logloss': 0.23278308932319106, 'AUC': 0.826883367187688}\n",
    "typ9.learn(X,y, iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}\n",
    "typ16.learn(X,y,iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa stack predict med alla typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack predict for all models\n",
    "def stack_predict(X_, models):\n",
    "    X = X_.copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        X['proba'+nr] = typ.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "    # cols=X.columns[-8]    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete learning process with all steps in stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()  # The meta model\n",
    "    \n",
    "# fit my models on split date for timeseries   \n",
    "print('START fitting and predicting TimeseriesSplit') \n",
    "\n",
    "\n",
    "cross_val_predict=pd.DataFrame()\n",
    "for id_train, id_test in TimeSeriesSplit(n_splits=5).split(df_stack):  \n",
    "    for typ in [typ6, typ1, typ9, typ16]:\n",
    "        typ.learn(df_stack.loc[id_train],y.loc[id_train], iterations=25)\n",
    "    df_pred = stack_predict(df_stack.loc[id_test], [typ6, typ1, typ9, typ16])\n",
    "    df_pred['y']=y.loc[id_test]\n",
    "    cross_val_predict = pd.concat([cross_val_predict, df_pred.iloc[:,-9:]])\n",
    "       \n",
    "print('\\nFitting my models with df_stack')\n",
    "# final fit with all the available data\n",
    "for typ in [typ6, typ1, typ9, typ16]:\n",
    "    typ.learn(df_stack, y, iterations=20)\n",
    "\n",
    "print('\\nFitting meta_model on predicted above')\n",
    "# fit a rf meta_model on cross_val_predict\n",
    "meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "meta_model.fit(cross_val_predict.iloc[:, :-1], cross_val_predict.iloc[:, -1])\n",
    "print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "# pickle save stacking\n",
    "pickle.dump(meta_model, open('..\\\\modeller\\\\meta_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on unseen data\n",
    "def unseen_predictions(X_, models, meta_model):\n",
    "    X = X_.copy()\n",
    "    for model in models:\n",
    "        nr = model.name[3:]\n",
    "        X['proba'+nr] = model.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "        \n",
    "    return(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "\n",
    "# a small test:\n",
    "unseen_predictions(df_stack.iloc[-80:,:], [typ6, typ1, typ9, typ16], meta_model)[:,1],y.iloc[-80:].values\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
