{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En ny learning och validering   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beskrivning:  \n",
    "\n",
    "Normal learning:  \n",
    "- Learning: gör webscrape av omgång(ar) och kör en learning\n",
    "\n",
    "GridSearch optimering:  \n",
    "- gör gridsearch av de olika modellerna typ1, typ6, typ9 och typ16 samt meta_modellen\n",
    "  - inkludera imbalanced-lösningar\n",
    "  - spara de bästa hyperparametrarna\n",
    "  \n",
    "Värdera:  \n",
    "- Cross_validate: ts_split and ts cross_validate of the stack of models plus the meta_model\n",
    "  - save the results in pickle\n",
    "- Vinst: beräknar vinsten för de olika modellerna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Först kolla artiklar om stacking, cv för stacking samt cv för stacking av timeseries \n",
    "- https://machinelearningmastery.com/implementing-stacking-scratch-python/   (Även kod i Pieces)  \n",
    "Även allmänt om stacking ensembles  \n",
    "- https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning/  \n",
    "Slutligen CV för Timeseries stacking  (se kod i Pieces)  \n",
    "- https://datascience.stackexchange.com/questions/41378/how-to-apply-stacking-cross-validation-for-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generella funktioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moduler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from IPython.display import display\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "import typ as tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# läs in data\n",
    "def läs_in_data_och_förbered():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    \n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # läs in FEATIRES.txt\n",
    "    with open('../FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        features = f.read().splitlines()\n",
    "\n",
    "    assert len(features) == len(X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'   \n",
    "    assert set(features) == set(X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#              namn, ant_hästar, proba, Kelly,  motst_ant, motst_diff, ant_favoriter,   only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,       False,          0,            False,    True)\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,       True,           2,            True,     False)\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,       True,           2,            True,     True)\n",
    "typ16= tp.Typ('typ16',True,       True, True,      2,       True,           2,            False,    True)\n",
    "\n",
    "typer = [typ6, typ1, typ9, typ16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "             'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV av typ6, typ1, typ9, typ16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "    # Läs in data\n",
    "    # remove_features\n",
    "\n",
    "# modeller konfigurering, CatBoostClassifier  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a GridSearch for CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def create_grid_search(X_, y_, typ, verbose=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    # remove features\n",
    "    X = typ.prepare_for_model(X)\n",
    "    if not typ.streck:\n",
    "        X.drop('streck', axis=1, inplace=True)\n",
    "    \n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features\\n',cat_features)\n",
    "    X = remove_features(X, remove_mer=['datum','avd'])\n",
    "    # get numerical features and cat_features\n",
    "    num_features = list(X.select_dtypes(include=[np.number]).columns)\n",
    "    cat_features = list(X.select_dtypes(include=['object']).columns)\n",
    "    # print('cat_features:\\n',cat_features,'\\n')\n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "    # create a GridSearch\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "        'iterations': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        'bagging_temperature': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'rsm': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'depth_per_tree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'border_count': [32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "        'fold_permutation_block_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'od_pval': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'od_wait': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3,  1.5, 1.7, 1.9],\n",
    "        'od_type': ['IncToDec', 'Iter'],\n",
    "    }\n",
    "\n",
    "    # X.datum = pd.to_datetime(X.datum)\n",
    "    # X.datum = X.datum.dt.date\n",
    "    # alla_datum = X.datum.unique()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # for train_index, test_index in tscv.split(X):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        # X_train, X_test = X[train_index], X[test_index]\n",
    "        # y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = CatBoostClassifier(iterations=500, loss_function='Logloss', eval_metric='AUC',\n",
    "                               use_best_model=False, early_stopping_rounds=100, verbose=verbose,)\n",
    "\n",
    "    grid = {'learning_rate': [0.002,0.005],\n",
    "            'depth': [2,4,6,8],\n",
    "            'l2_leaf_reg': [8, 9, 10,11]}\n",
    "    \n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "\n",
    "    grid_search_result = model.grid_search(grid,\n",
    "                                        X=Pool(X, y, cat_features=cat_features),\n",
    "                                        cv=tscv.split(X),\n",
    "                                        shuffle=False,\n",
    "                                        search_by_train_test_split=False,\n",
    "                                        verbose=verbose,\n",
    "                                        plot=True)\n",
    "    return grid_search_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testa grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN in cat before: 246\n",
      "Number of NaN in num before: 65332\n",
      "Number of NaN in cat after: 0\n",
      "Number of NaN in num after: 0\n"
     ]
    }
   ],
   "source": [
    "# Total GridSearchCV där jag använder egen TimeseriesSplit\n",
    "if False:   # modellerna typ1, typ9, typ16\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results6 = create_grid_search(X, y, typ6)\n",
    "\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results1 = create_grid_search(X, y, typ1)\n",
    "\n",
    "    X,y = läs_in_data_och_förbered()\n",
    "    results9 = create_grid_search(X, y, typ9)\n",
    "\n",
    "    # X, y = läs_in_data_och_förbered()\n",
    "    # results16 = create_grid_search(X, y, typ16)\n",
    "\n",
    "if True: # prova KNN\n",
    "    # KNN GridSearch\n",
    "    # import KNN\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    X,y = läs_in_data_och_förbered()\n",
    "    X = remove_features(X, remove_mer=['avd', 'datum', 'streck'])\n",
    "    \n",
    "    # get numerical features and cat_features\n",
    "    num_features = list(X.select_dtypes(include=[np.number]).columns)\n",
    "    cat_features = list(X.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    print('Number of NaN in cat before:', X[cat_features].isna().sum()[\n",
    "        X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "    print('Number of NaN in num before:', X[num_features].isna().sum()[\n",
    "        X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "\n",
    "    # impute 'missing' for all NaN in cat_features\n",
    "    X[cat_features] = X[cat_features].fillna('missing')\n",
    "    X[num_features] = X[num_features].fillna(0)\n",
    "    \n",
    "    print('Number of NaN in cat after:', X[cat_features].isna().sum()[\n",
    "        X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "    print('Number of NaN in num after:', X[num_features].isna().sum()[\n",
    "        X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "10632\n",
      "1112\n",
      "3\n",
      "1676\n",
      "620\n",
      "1750\n",
      "662\n",
      "1825\n",
      "695\n",
      "1929\n",
      "701\n",
      "2003\n",
      "745\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bana</th>\n",
       "      <th>häst</th>\n",
       "      <th>kusk</th>\n",
       "      <th>kr</th>\n",
       "      <th>spår</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>ålder</th>\n",
       "      <th>kön</th>\n",
       "      <th>pris</th>\n",
       "      <th>h1_kusk</th>\n",
       "      <th>h1_bana</th>\n",
       "      <th>h1_spår</th>\n",
       "      <th>h1_plac</th>\n",
       "      <th>h1_pris</th>\n",
       "      <th>h1_odds</th>\n",
       "      <th>h1_kmtid</th>\n",
       "      <th>h2_kusk</th>\n",
       "      <th>h2_bana</th>\n",
       "      <th>h2_spår</th>\n",
       "      <th>h2_plac</th>\n",
       "      <th>h2_pris</th>\n",
       "      <th>h2_odds</th>\n",
       "      <th>h2_kmtid</th>\n",
       "      <th>h3_kusk</th>\n",
       "      <th>h3_bana</th>\n",
       "      <th>h3_spår</th>\n",
       "      <th>h3_plac</th>\n",
       "      <th>h3_pris</th>\n",
       "      <th>h3_odds</th>\n",
       "      <th>h3_kmtid</th>\n",
       "      <th>h4_kusk</th>\n",
       "      <th>h4_bana</th>\n",
       "      <th>h4_spår</th>\n",
       "      <th>h4_plac</th>\n",
       "      <th>h4_pris</th>\n",
       "      <th>h4_odds</th>\n",
       "      <th>h4_kmtid</th>\n",
       "      <th>h5_kusk</th>\n",
       "      <th>h5_bana</th>\n",
       "      <th>h5_spår</th>\n",
       "      <th>h5_plac</th>\n",
       "      <th>h5_pris</th>\n",
       "      <th>h5_odds</th>\n",
       "      <th>h5_kmtid</th>\n",
       "      <th>h1_dist</th>\n",
       "      <th>h2_dist</th>\n",
       "      <th>h3_dist</th>\n",
       "      <th>h4_dist</th>\n",
       "      <th>h5_dist</th>\n",
       "      <th>h1_auto</th>\n",
       "      <th>h2_auto</th>\n",
       "      <th>h3_auto</th>\n",
       "      <th>h4_auto</th>\n",
       "      <th>h5_auto</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45063</th>\n",
       "      <td>17</td>\n",
       "      <td>1708</td>\n",
       "      <td>479</td>\n",
       "      <td>111705.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>151</td>\n",
       "      <td>448</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>57.59</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1674</td>\n",
       "      <td>351</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>40.02</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1581</td>\n",
       "      <td>489</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>166.22</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1118</td>\n",
       "      <td>496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>22.38</td>\n",
       "      <td>13.9</td>\n",
       "      <td>1152</td>\n",
       "      <td>475</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>23.89</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4034.287935</td>\n",
       "      <td>1817.682555</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.324555</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45064</th>\n",
       "      <td>17</td>\n",
       "      <td>7861</td>\n",
       "      <td>1007</td>\n",
       "      <td>150350.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1528</td>\n",
       "      <td>247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.9</td>\n",
       "      <td>1304</td>\n",
       "      <td>273</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1363</td>\n",
       "      <td>489</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>14.4</td>\n",
       "      <td>1431</td>\n",
       "      <td>154</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>12.31</td>\n",
       "      <td>12.6</td>\n",
       "      <td>1624</td>\n",
       "      <td>534</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>12.2</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8146.288038</td>\n",
       "      <td>9406.523184</td>\n",
       "      <td>18813.046367</td>\n",
       "      <td>9406.523184</td>\n",
       "      <td>38.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45065</th>\n",
       "      <td>17</td>\n",
       "      <td>3802</td>\n",
       "      <td>1062</td>\n",
       "      <td>151054.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1604</td>\n",
       "      <td>330</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1674</td>\n",
       "      <td>89</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.41</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1749</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>13.5</td>\n",
       "      <td>396</td>\n",
       "      <td>368</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11.75</td>\n",
       "      <td>13.7</td>\n",
       "      <td>1743</td>\n",
       "      <td>650</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2852.672356</td>\n",
       "      <td>2852.672356</td>\n",
       "      <td>938.647235</td>\n",
       "      <td>1817.682555</td>\n",
       "      <td>742.065796</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45066</th>\n",
       "      <td>17</td>\n",
       "      <td>1406</td>\n",
       "      <td>209</td>\n",
       "      <td>81001.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>339</td>\n",
       "      <td>102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>11.6</td>\n",
       "      <td>290</td>\n",
       "      <td>565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>13.1</td>\n",
       "      <td>308</td>\n",
       "      <td>590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.30</td>\n",
       "      <td>13.4</td>\n",
       "      <td>1495</td>\n",
       "      <td>595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1558</td>\n",
       "      <td>638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>4150.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45067</th>\n",
       "      <td>17</td>\n",
       "      <td>9907</td>\n",
       "      <td>722</td>\n",
       "      <td>60206.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1109</td>\n",
       "      <td>561</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>19.47</td>\n",
       "      <td>11.8</td>\n",
       "      <td>397</td>\n",
       "      <td>566</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>12.4</td>\n",
       "      <td>417</td>\n",
       "      <td>594</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.30</td>\n",
       "      <td>12.8</td>\n",
       "      <td>487</td>\n",
       "      <td>111</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>13.4</td>\n",
       "      <td>518</td>\n",
       "      <td>115</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2047.239585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45068</th>\n",
       "      <td>17</td>\n",
       "      <td>2926</td>\n",
       "      <td>170</td>\n",
       "      <td>58025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>269</td>\n",
       "      <td>494</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>12.2</td>\n",
       "      <td>280</td>\n",
       "      <td>442</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>12.0</td>\n",
       "      <td>298</td>\n",
       "      <td>41</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>12.8</td>\n",
       "      <td>307</td>\n",
       "      <td>491</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.70</td>\n",
       "      <td>13.4</td>\n",
       "      <td>314</td>\n",
       "      <td>535</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>5.93</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3460.466492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2098.879024</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>2570.591321</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45069</th>\n",
       "      <td>17</td>\n",
       "      <td>1072</td>\n",
       "      <td>956</td>\n",
       "      <td>44830.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>821</td>\n",
       "      <td>330</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.73</td>\n",
       "      <td>12.9</td>\n",
       "      <td>868</td>\n",
       "      <td>230</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>88.69</td>\n",
       "      <td>30.0</td>\n",
       "      <td>913</td>\n",
       "      <td>630</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.70</td>\n",
       "      <td>13.6</td>\n",
       "      <td>971</td>\n",
       "      <td>160</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.59</td>\n",
       "      <td>13.1</td>\n",
       "      <td>1555</td>\n",
       "      <td>638</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.40</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.071068</td>\n",
       "      <td>1.005332</td>\n",
       "      <td>2852.672356</td>\n",
       "      <td>2551.507722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45070</th>\n",
       "      <td>17</td>\n",
       "      <td>195</td>\n",
       "      <td>814</td>\n",
       "      <td>32013.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>269</td>\n",
       "      <td>448</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>13.6</td>\n",
       "      <td>280</td>\n",
       "      <td>443</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>15.0</td>\n",
       "      <td>298</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>14.4</td>\n",
       "      <td>307</td>\n",
       "      <td>500</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>14.1</td>\n",
       "      <td>314</td>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>13.1</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5152.164935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1659.309563</td>\n",
       "      <td>2346.618088</td>\n",
       "      <td>1659.309563</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45071</th>\n",
       "      <td>17</td>\n",
       "      <td>8750</td>\n",
       "      <td>431</td>\n",
       "      <td>29357.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>677</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>30.0</td>\n",
       "      <td>721</td>\n",
       "      <td>230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>27.29</td>\n",
       "      <td>30.0</td>\n",
       "      <td>752</td>\n",
       "      <td>633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>13.7</td>\n",
       "      <td>803</td>\n",
       "      <td>306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>13.4</td>\n",
       "      <td>1152</td>\n",
       "      <td>673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.96</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.247449</td>\n",
       "      <td>9497.121738</td>\n",
       "      <td>13430.958366</td>\n",
       "      <td>13430.958366</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45072</th>\n",
       "      <td>17</td>\n",
       "      <td>1142</td>\n",
       "      <td>1107</td>\n",
       "      <td>40665.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>966</td>\n",
       "      <td>555</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>8.74</td>\n",
       "      <td>12.4</td>\n",
       "      <td>157</td>\n",
       "      <td>420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.88</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1819</td>\n",
       "      <td>41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.27</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1921</td>\n",
       "      <td>286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>4.36</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>673</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3486.348504</td>\n",
       "      <td>13430.958366</td>\n",
       "      <td>772.134443</td>\n",
       "      <td>1091.963001</td>\n",
       "      <td>1220.851749</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bana  häst  kusk        kr  spår    dist  lopp_dist  start  ålder  kön      pris  h1_kusk  h1_bana  h1_spår  h1_plac  h1_pris  h1_odds  h1_kmtid  h2_kusk  h2_bana  h2_spår  h2_plac  h2_pris  \\\n",
       "45063    17  1708   479  111705.0   3.0  2640.0     2640.0      0      5    0  150000.0      151      448      4.0      3.0    100.0    57.59      11.1     1674      351      4.0      5.0    150.0   \n",
       "45064    17  7861  1007  150350.0   4.0  2640.0     2640.0      0      5    0  150000.0     1528      247      2.0     15.0      0.0     0.00      16.9     1304      273      2.0      2.0    150.0   \n",
       "45065    17  3802  1062  151054.0   5.0  2640.0     2640.0      0      7    0  150000.0     1604      330      2.0      3.0     50.0     4.86      11.6     1674       89      2.0      3.0     50.0   \n",
       "45066    17  1406   209   81001.0   6.0  2640.0     2640.0      0      8    0  150000.0      339      102      0.0      2.0      0.0    15.60      11.6      290      565      0.0      5.0      0.0   \n",
       "45067    17  9907   722   60206.0   7.0  2640.0     2640.0      0      8    2  150000.0     1109      561      9.0      4.0     70.0    19.47      11.8      397      566      9.0     15.0      0.0   \n",
       "45068    17  2926   170   58025.0   8.0  2640.0     2640.0      0      8    2  150000.0      269      494      6.0      4.0    200.0    17.00      12.2      280      442      6.0      2.0      0.0   \n",
       "45069    17  1072   956   44830.0   9.0  2640.0     2640.0      0     11    2  150000.0      821      330      3.0     15.0     50.0    40.73      12.9      868      230      3.0     20.0    150.0   \n",
       "45070    17   195   814   32013.0  10.0  2640.0     2640.0      0      9    2  150000.0      269      448      5.0      2.0     60.0     2.64      13.6      280      443      5.0      2.0      0.0   \n",
       "45071    17  8750   431   29357.0  11.0  2640.0     2640.0      0      8    2  150000.0      677      109      0.0     15.0      0.0     4.75      30.0      721      230      0.0     15.0    150.0   \n",
       "45072    17  1142  1107   40665.0  12.0  2640.0     2640.0      0      8    2  150000.0      966      555      5.0      6.0   1500.0     8.74      12.4      157      420      5.0      1.0    150.0   \n",
       "\n",
       "       h2_odds  h2_kmtid  h3_kusk  h3_bana  h3_spår  h3_plac  h3_pris  h3_odds  h3_kmtid  h4_kusk  h4_bana  h4_spår  h4_plac  h4_pris  h4_odds  h4_kmtid  h5_kusk  h5_bana  h5_spår  h5_plac  h5_pris  \\\n",
       "45063    40.02      12.3     1581      489      4.0     15.0    100.0   166.22      11.0     1118      496      4.0     15.0     40.0    22.38      13.9     1152      475      4.0     15.0    125.0   \n",
       "45064     2.25      14.3     1363      489      2.0      2.0    200.0     2.30      14.4     1431      154      2.0      2.0    800.0    12.31      12.6     1624      534      2.0      2.0    200.0   \n",
       "45065    15.41      11.0     1749       18      2.0      5.0     40.0     7.85      13.5      396      368      2.0      5.0    150.0    11.75      13.7     1743      650      2.0      5.0     25.0   \n",
       "45066    15.80      13.1      308      590      0.0      4.0      0.0    42.30      13.4     1495      595      0.0      4.0      0.0    67.00      30.0     1558      638      0.0      4.0      0.0   \n",
       "45067    32.00      12.4      417      594      9.0     15.0      0.0    34.30      12.8      487      111      9.0     15.0      0.0    28.00      13.4      518      115      9.0     15.0      0.0   \n",
       "45068     4.30      12.0      298       41      6.0      5.0    200.0     3.90      12.8      307      491      6.0      5.0    100.0    19.70      13.4      314      535      6.0      5.0    300.0   \n",
       "45069    88.69      30.0      913      630      3.0      3.0     50.0    24.70      13.6      971      160      3.0      3.0     40.0    24.59      13.1     1555      638      3.0      3.0      0.0   \n",
       "45070     4.70      15.0      298       39      5.0      5.0    125.0     3.73      14.4      307      500      5.0      5.0    250.0     6.91      14.1      314       44      5.0      5.0    125.0   \n",
       "45071    27.29      30.0      752      633      0.0      1.0     75.0     3.15      13.7      803      306      0.0      1.0    150.0    12.18      13.4     1152      673      0.0      1.0    150.0   \n",
       "45072     4.88      11.9     1819       41      5.0      7.0    200.0     4.27      12.8     1921      286      5.0      7.0    400.0     4.36      30.0     1997      673      5.0      7.0    500.0   \n",
       "\n",
       "       h5_odds  h5_kmtid  h1_dist  h2_dist  h3_dist  h4_dist  h5_dist  h1_auto  h2_auto  h3_auto  h4_auto  h5_auto      h1_perf       h2_perf      h3_perf       h4_perf       h5_perf  senast  \\\n",
       "45063    23.89      13.5   1640.0   2140.0   1640.0   2140.0   2140.0        1        1        1        1        1  4034.287935   1817.682555    10.000000      6.324555     11.180340    24.0   \n",
       "45064     4.10      12.2   2140.0   2640.0   2640.0   2140.0   2140.0        0        1        1        1        1     0.000000   8146.288038  9406.523184  18813.046367   9406.523184    38.0   \n",
       "45065     1.58      12.8   1640.0   1640.0   2140.0   2640.0   2120.0        1        1        1        1        1  2852.672356   2852.672356   938.647235   1817.682555    742.065796    19.0   \n",
       "45066    16.00      11.1   2011.0   2700.0   2850.0   4150.0   2100.0        1        0        0        0        1     0.000000      0.000000     0.000000      0.000000      0.000000    20.0   \n",
       "45067    36.00      13.2   1640.0   2100.0   2700.0   2925.0   2950.0        1        1        0        0        0  2047.239585      0.000000     0.000000      0.000000      0.000000    17.0   \n",
       "45068     5.93      30.0   2140.0   2100.0   2140.0   2140.0   2140.0        1        1        1        1        1  3460.466492      0.000000  2098.879024   1484.131591   2570.591321    21.0   \n",
       "45069   168.40      30.0   1640.0   2140.0   2140.0   2140.0   2700.0        1        1        1        1        0     7.071068      1.005332  2852.672356   2551.507722      0.000000    19.0   \n",
       "45070     1.87      13.1   2640.0   3120.0   2140.0   2140.0   2140.0        1        0        0        1        1  5152.164935      0.000000  1659.309563   2346.618088   1659.309563    17.0   \n",
       "45071     4.96      30.0   2990.0   2140.0   3200.0   2140.0   2160.0        0        1        0        1        0     0.000000     12.247449  9497.121738  13430.958366  13430.958366    20.0   \n",
       "45072    12.18      12.7   2140.0   2140.0   2140.0   2640.0   2640.0        1        1        1        1        1  3486.348504  13430.958366   772.134443   1091.963001   1220.851749    28.0   \n",
       "\n",
       "       delta1  delta2  delta3  delta4  \n",
       "45063    16.0     5.0     7.0    12.0  \n",
       "45064   382.0   136.0    17.0    14.0  \n",
       "45065    20.0    14.0   311.0     9.0  \n",
       "45066    29.0    21.0    27.0    15.0  \n",
       "45067    32.0    21.0    13.0    23.0  \n",
       "45068    21.0    21.0    17.0   109.0  \n",
       "45069    23.0    23.0    25.0    50.0  \n",
       "45070    25.0    21.0    97.0    22.0  \n",
       "45071    22.0    31.0    18.0    21.0  \n",
       "45072    20.0    15.0   154.0    21.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN in cat after: 0\n",
      "Number of NaN in num after: 0\n"
     ]
    }
   ],
   "source": [
    "    X[cat_features] = X[cat_features].astype('category')\n",
    "    # make categorical features numeric\n",
    "    for col in cat_features:\n",
    "        X[col] = X[col].cat.codes\n",
    "        print(len(X[col].unique()))\n",
    "\n",
    "    display(X.tail(10))\n",
    "    \n",
    "    print('Number of NaN in cat after:', X[cat_features].isna().sum()[\n",
    "        X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "    print('Number of NaN in num after:', X[num_features].isna().sum()[\n",
    "        X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance',\n",
       " 'p': 5,\n",
       " 'n_neighbors': 40,\n",
       " 'metric': 'euclidean',\n",
       " 'leaf_size': 4,\n",
       " 'algorithm': 'ball_tree'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6456397198945681"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if True:\n",
    "    # create a gridsearch for KNN\n",
    "    param_grid = {  'n_neighbors': [14,16, 18, 20,22,24,30, 35,40,50],\n",
    "                    'weights': ['uniform','distance'],\n",
    "                    'algorithm': ['auto','ball_tree', 'kd_tree', 'brute'],\n",
    "                    'leaf_size': [4,5,6,7,10,20,30],\n",
    "                    'p': [1,2,3,5,10],\n",
    "                    'metric': ['euclidean','manhattan', 'chebyshev']\n",
    "                 }\n",
    "\n",
    "    grid_search = RandomizedSearchCV(KNeighborsClassifier(), param_grid, n_jobs=6, cv=tscv.split(X), scoring='roc_auc', return_train_score=True, verbose=5)\n",
    "\n",
    "    \n",
    "    resultat = grid_search.fit(X, y)\n",
    "    display(resultat.best_params_, resultat.best_score_, max(resultat.cv_results_['mean_train_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance',\n",
       " 'p': 3,\n",
       " 'n_neighbors': 30,\n",
       " 'metric': 'manhattan',\n",
       " 'leaf_size': 5,\n",
       " 'algorithm': 'auto'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.639233446637985"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(resultat.best_params_, resultat.best_score_,\n",
    "        max(resultat.cv_results_['mean_train_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'algorithm': 'auto',\n",
       "  'leaf_size': 6,\n",
       "  'metric': 'euclidean',\n",
       "  'n_neighbors': 10,\n",
       "  'p': 1,\n",
       "  'weights': 'uniform'},\n",
       " 0.6033588988495121)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultat.best_params_, resultat.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults = pd.read_csv('results.csv')\n",
    "def results(typ, dfResults, comm=[]):\n",
    "    s = \"results\"+str(typ)\n",
    "    temp1 =globals()[s]\n",
    "    temp = pd.DataFrame(temp1['cv_results'])\n",
    "    temp['typ'] = typ\n",
    "    max=temp['test-AUC-mean'].max()\n",
    "    temp=temp[temp['test-AUC-mean']==max]\n",
    "    if len(comm)>0:\n",
    "        temp['comm'] = comm\n",
    "    str(temp1['params'])\n",
    "    temp['best_params'] = str(temp1['params']).replace('{','').replace('}','').replace('depth','d').replace('l2_leaf_reg','l2').replace('learning_rate','LR')\n",
    "    dfResults = pd.concat([dfResults,temp])\n",
    "    return dfResults\n",
    "\n",
    "#### Slår ihop alla resultaten från GridSearch i funktionen ovan\n",
    "if False:\n",
    "    dfResults = results(1, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(6, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(9, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(16, dfResults, 'ökade iter=500')\n",
    "    dfResults.to_csv('results.csv', index=False)\n",
    "dfResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spara modeller och optimala parms (kanske skall läras på del 1-4?)\n",
    "- Predict på den sista 5:e delen och skapa input till meta-model\n",
    "- gridsearchcv meta_model på detta. Spara optimala parms\n",
    "- avsluta med en learn på hela 5:- delen med optimala parms och spara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_temp(typ, X_, y_, iterations, depth, learning_rate, l2_leaf_reg,save=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    \n",
    "    typ.learn(X, y, learning_rate=learning_rate, iterations=iterations, depth=depth, l2_leaf_reg=l2_leaf_reg, save=save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def learn_all_models(totals = False, save=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_until = train_index[-1:][0]\n",
    "        test_from = train_until+1\n",
    "\n",
    "    if totals == False:\n",
    "        ### sista 5:e delen av data\n",
    "        print('train_until',train_until, 'test_from',test_from)\n",
    "\n",
    "        X_train, X_test = X.iloc[:test_from], X.iloc[test_from:]\n",
    "        y_train, y_test = y[:test_from], y[test_from:]\n",
    "    else:\n",
    "        ### all data\n",
    "        print('train all')\n",
    "\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "    results = pd.read_csv('results.csv')\n",
    "\n",
    "    # typ6: 'd': 2, 'l2': 9, 'LR': 0.005\n",
    "    print(results[results.typ==6][['typ','iterations','best_params']].iloc[-1].values)\n",
    "    iter=460\n",
    "    d=2\n",
    "    LR=0.005\n",
    "    L2=9\n",
    "    learn_temp(typ6, X_train, y_train, iterations=iter, depth=d, learning_rate=LR, l2_leaf_reg=L2,save=save)\n",
    "\n",
    "    # typ1: 'd': 4, 'l2': 8, 'LR': 0.005\n",
    "    print(results[results.typ == 1][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 432\n",
    "    d = 4\n",
    "    LR = 0.005\n",
    "    L2 = 9\n",
    "    learn_temp(typ1, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "    # typ9: 'd': 2, 'l2': 10, 'LR': 0.005\n",
    "    print(results[results.typ == 9][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 496\n",
    "    d = 2\n",
    "    LR = 0.005\n",
    "    L2 = 10\n",
    "    learn_temp(typ9, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "\n",
    "    # typ16:'d': 2, 'l2': 10, 'LR': 0.005\n",
    "    print(results[results.typ == 16][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 496\n",
    "    d = 2\n",
    "    LR = 0.005\n",
    "    L2 = 10\n",
    "    learn_temp(typ16, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "\n",
    "if False:\n",
    "    learn_all_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapa ett Kelly-värde baserat på streck omvandlat till odds\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open('rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack for learning meta_model\n",
    "def skapa_stack_learning(X_, y, save=True):\n",
    "    X=X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    for typ in typer:\n",
    "            nr = typ.name[3:]\n",
    "            stacked_data['proba'+nr] = typ.predict(X)\n",
    "            stacked_data['kelly' + nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "\n",
    "    print(stacked_data.columns)\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    return stacked_data,y   # enbart stack-info\n",
    "\n",
    "if True: # förbered data och kör stacking\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_from = test_index[0]   # sic!\n",
    "\n",
    "    ### sista 5:e delen av data\n",
    "    print('train_from for meta', train_from)\n",
    "\n",
    "    X_train, y_train = X.iloc[train_from:], y[train_from:]\n",
    "    X_stack,y_stack = skapa_stack_learning(X_train, y_train, False)\n",
    "    display(X_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit meta_model\n",
    "def learn_meta_model(X, y,  save=True,\n",
    "                     n_estimators=100, \n",
    "                     max_depth=None,\n",
    "                     min_samples_split=2, \n",
    "                     min_samples_leaf=1, \n",
    "                     max_features='auto', \n",
    "                     max_leaf_nodes=None,\n",
    "                     max_samples=None,):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "\n",
    "    meta_model = RandomForestClassifier(\n",
    "       class_weight='balanced',    #{0:1,1:10},\n",
    "       oob_score=True, verbose=1, n_jobs=8, random_state=2022,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_samples=max_samples,)\n",
    "    \n",
    "    meta_model.fit(X, y)\n",
    "\n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9430916552667579\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open('../modeller/meta_model.model', 'wb') as f:\n",
    "            pickle.dump(meta_model, f)\n",
    "\n",
    "    return meta_model\n",
    "\n",
    "if True:\n",
    "    grid ={\n",
    "        'n_estimators': [10,100,200],\n",
    "        'max_depth': [None,2,4,6],\n",
    "        'min_samples_split': [2,4,6],\n",
    "        'min_samples_leaf':[1],  # 2,3],\n",
    "        'max_features': ['auto'], # 'log2', None],\n",
    "        'max_leaf_nodes': [None], # 20, 10, 5],\n",
    "        'max_samples':[None], # 0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    }\n",
    "    \n",
    "    oob_dict = {}\n",
    "    n=1\n",
    "    keys = list(grid.keys())\n",
    "    for i, key in enumerate(keys[:3]):\n",
    "        # loop over all values in key\n",
    "        for value in grid[key]:\n",
    "            for key2 in keys[i+1:3]:\n",
    "                for value2 in grid[key2]:\n",
    "                    for key3 in keys[i+2:3]:\n",
    "                        for value3 in grid[key3]:\n",
    "                            \n",
    "                            print('konf', n, key, value, key2, value2, key3, value3)\n",
    "                \n",
    "                            meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                                  n_estimators = value,\n",
    "                                                  max_depth = value2,\n",
    "                                                  min_samples_split = value3,\n",
    "                                                  min_samples_leaf = 1,  # 2,3], \n",
    "                                                  max_features = 'auto', # 'log2', None], \n",
    "                                                  max_leaf_nodes = None, # 20, 10, 5],  \n",
    "                                                  max_samples    = None,)\n",
    "                            # keep n:oob_score in dict\n",
    "                            oob_dict[n] = meta_model.oob_score_\n",
    "        \n",
    "                            n+=1\n",
    "    print(oob_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print key where max value in oob_dict\n",
    "print(max(oob_dict, key=oob_dict.get))  \n",
    "print('Manuellt hittade jag: n_estimators=100, max_depth=None,min_samples_split=2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final: learn meta_model med optimala parms och learn allt för modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #####    X_stack och y_stack måste vara definierade ovan   #####\n",
    "    meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=None,\n",
    "                                  min_samples_split=2,\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ### alla värden enligt ovan körningar för modellerna ###\n",
    "    learn_all_models(totals=True, save=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktioner för att prioritera mellan hästar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se även kelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för en omgång (ett datum) ta ut största diff för streck per avd \n",
    "# om only_clear=True, enbart för diff >= 25\n",
    "def lista_med_favoriter(df_, ant, only_clear):\n",
    "    df = df_.copy()\n",
    "    min_diff = 25 if only_clear else 0\n",
    "    # sortera på avd,streck\n",
    "    df = df.sort_values(['avd', 'streck'], ascending=[False, False])\n",
    "    diff_list = []\n",
    "    for avd in range(1, 8):\n",
    "        diff = df.loc[df.avd == avd].streck.iloc[0] - \\\n",
    "            df.loc[df.avd == avd].streck.iloc[1]\n",
    "        if diff >= min_diff:\n",
    "            diff_list.append((avd, diff))\n",
    "\n",
    "     # sortera på diff\n",
    "    diff_list = sorted(diff_list, key=lambda x: x[1], reverse=True)\n",
    "    return diff_list[:ant]\n",
    "\n",
    "# temp is a list of tuples (avd, diff). check if avd is in the list\n",
    "def check_avd(avd, temp):\n",
    "    for t in temp:\n",
    "        if t[0] == avd:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_insats(df):\n",
    "    insats = 0\n",
    "    # group by avd\n",
    "    summa = df.groupby('avd').avd.count().prod() / 2\n",
    "    return summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning-fasen\n",
    "Bygg en separat v75_learning_ny.ipynb   \n",
    "Bygg en separat v75_validate_ny.ipynb  \n",
    "Bygg en separat v75_hyperparms.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gör en scrape på senaste veckan (behövs inte i denna test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum < test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "# _, rf = model_streck_to_odds(X)   # used in next cell\n",
    "\n",
    "# # spara rf\n",
    "# import pickle\n",
    "# with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "#     pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all_data.csv \n",
    "Baka ihop senaste vekan med all_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skapa_stack_learning(X_, y, features, iterations=1000, random_state=2022, verbose=False, save=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack med proba och kelly\n",
    "    X måste ha datum och avd\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    \n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        model = typ.learn(X, y, features, iterations=iterations, save=save, verbose=verbose)\n",
    "        stacked_data['proba'+nr] = typ.predict(X) \n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "    \n",
    "    # print(stacked_data.columns)\n",
    "    return stacked_data   # enbart stack-info\n",
    "\n",
    "# fit meta_model\n",
    "def learn_meta_model(X,y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "    \n",
    "    meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "    meta_model.fit(X, y)\n",
    "    \n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "    # pickle save stacking\n",
    "    pickle.dump(meta_model, open('..\\\\modeller\\\\meta.model', 'wb'))\n",
    "    \n",
    "    return meta_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature list from a file (ej plac)\n",
    "def read_feature_list(file='../FEATURES.txt'):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liten_cv_timeseries_demo(x_,y_):\n",
    "    X = x_.copy()\n",
    "    y = y_.copy()\n",
    "    print('Holdout validation data from X = X[~validation]')\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        print(f\"TRAIN each model on this: {train_index[0]}-{train_index[-1]}, Predict on: {test_index[0]}-{test_index[-1]}\")\n",
    "        print('save the predictions of each model')\n",
    "        X_train = X.loc[train_index]\n",
    "        X_test = X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    print('Train meta_model on all models predictions')\n",
    "    print('validate meta_model on the validation data')\n",
    "    \n",
    "    print('\\nHandle stratified and unbalanced data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör learning-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = read_feature_list(\"../FEATURES.txt\")\n",
    "\n",
    "print(\"hur mycket skall sparas för input till learning meta-model?\")\n",
    "print('kör vs.v75_sraping')\n",
    "print('tvätta data')\n",
    "X, y = läs_in_data_och_förbered()\n",
    "print(' concat med all_data')\n",
    "\n",
    "assert X.shape[1] == len(FEATURES), f'X_train.shape[1] {X.shape[1]} != len(FEATURES) {len(FEATURES)}'\n",
    "assert set(X.columns) == set(FEATURES), f'set(X_train.columns) {set(X.columns)} != set(FEATURES) {set(FEATURES)}'\n",
    "X = X[FEATURES]  # för att få kolumner i rätt ordning\n",
    "andel_train = 0.60\n",
    "andel_meta_train = 0.75\n",
    "alla_datum = X.datum.unique()\n",
    "train_datum=alla_datum[:int(len(alla_datum)*andel_train)]\n",
    "test_datum=alla_datum[int(len(alla_datum)*andel_train):]\n",
    "meta_train = test_datum[:int(len(test_datum)*andel_meta_train)]\n",
    "meta_test = test_datum[int(len(test_datum)*andel_meta_train):]\n",
    "print(f'alla_datum {len(alla_datum)} varav train_datum {len(train_datum)} och test_datum {len(test_datum)}')\n",
    "print(f'meta_train {len(meta_train)} och meta_test {len(meta_test)}')\n",
    "\n",
    "if True:\n",
    "    liten_cv_timeseries_demo(X, y)\n",
    "else:    \n",
    "    X_stacked = skapa_stack_learning(X_train, y_train, FEATURES, iterations=100,random_state=2022, verbose=False, save=True)\n",
    "    # display(X_stacked)\n",
    "    meta_model = learn_meta_model(X_stacked, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spela-fasen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "import V75_scraping as vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape-funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v75_scrape():\n",
    "    df, strukna = vs.v75_scraping(history=True, resultat=False)\n",
    "    \n",
    "    # df = pd.read_csv('../sparad_scrape.csv')\n",
    "    for f in ['häst','bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ metod\n",
    "# Ta fram rader för varje typ enligt test-resultaten innan\n",
    "# låt meta_model välja mellan typerna - hur? Hur maximer insatsen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som bygger stack-data från modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för stacking ta med alla hästar per typ och proba plus kelly\n",
    "def build_stack_df(X_):\n",
    "    X = X_.copy()\n",
    "    stacked_data = X[['datum','avd', 'startnr','häst']].copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba'+nr], X[['streck']], None)\n",
    "    return stacked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion där meta_model gör predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_predict(X_):\n",
    "    # X_ innehåller även datum,startnr och avd\n",
    "    extra = ['datum', 'avd', 'startnr', 'häst']\n",
    "    assert list(X_.columns[:4]) == extra, 'meta_model måste ha datum, avd och startnr, häst för att kunna välja'\n",
    "    X = X_.copy()\n",
    "    with open('../modeller\\\\meta.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "        \n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict_proba(X.iloc[:,-8:])[:,1]\n",
    "    my_columns = extra + list(X.columns)[-9:] \n",
    "    \n",
    "    return X[my_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som väljer rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_cost(antal_rader):\n",
    "    cost = (antal_rader**2)/2\n",
    "    return antal_rader,cost\n",
    "\n",
    "def välj_rad(X_):\n",
    "    \n",
    "    max_insats=320\n",
    "    veckans_rad = X_.copy()\n",
    "    veckans_rad['välj'] = False\n",
    "\n",
    "    for avd in veckans_rad.avd.unique():\n",
    "        max_pred = veckans_rad[veckans_rad.avd == avd]['meta_predict'].max()\n",
    "        veckans_rad.loc[(veckans_rad.avd == avd) & (veckans_rad.meta_predict == max_pred), 'välj'] = True\n",
    "    antal_rader=1    \n",
    "    veckans_rad = veckans_rad.sort_values(by=['meta_predict'], ascending=False)\n",
    "    \n",
    "    # 3. Använda ensam favorit för ett par avd? Kolla test-resultat\n",
    "    # for each row in rad, välj=True if select_func(cost,avd) == True\n",
    "    cost = antal_rader*0.5\n",
    "    for i, row in veckans_rad.iterrows():\n",
    "        new_antal,new_cost = comp_cost(antal_rader+1)\n",
    "        # print(the_cost)\n",
    "        if new_cost > max_insats:\n",
    "            break\n",
    "        \n",
    "        antal_rader = new_antal\n",
    "        cost = new_cost\n",
    "        veckans_rad.loc[i, 'välj'] = True\n",
    "        # print(cost)\n",
    "    veckans_rad.sort_values(by=['välj', 'avd'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    return veckans_rad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör hela välj-rad-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = v75_scrape()\n",
    "print(X.datum.unique())\n",
    "df_stack = build_stack_df(X,FEATURES)\n",
    "df_meta = meta_predict(df_stack)\n",
    "df_meta.reset_index(drop=True, inplace=True)\n",
    "veckans_rad = välj_rad(df_meta)\n",
    "# rename columns \n",
    "veckans_rad.rename(columns={'startnr':'nr', 'meta_predict':'Meta', 'välj':'Välj'}, inplace=True)\n",
    "\n",
    "display(veckans_rad[veckans_rad.välj])\n",
    "print('kostnad', veckans_rad.välj.sum()**2/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En massa gammal - kanske reusable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for Learn!\n",
    "def init_learn():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    # X = ordinal_enc(X, 'häst')\n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features:', cat_features)\n",
    "    X.head()\n",
    "    return X, y, cat_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modell för streck_to_odds - skall vara fix och inte ändras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum <\n",
    "                             test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "linreg, rf = model_streck_to_odds(X)   # used in next cell\n",
    "# spara rf\n",
    "import pickle\n",
    "with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engångsgrej för att initiera typ-instanserna med learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bara första gången. Initierar Typ-klassen\n",
    "def learn(X_train, y_train, X_test=None, y_test=None, cat_features=[],  iterations=1000,  verbose=False):\n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    X_train = remove_features(X_train, remove_mer=['avd','datum'])\n",
    "    cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "    if X_test is not None:\n",
    "        X_test = remove_features(X_test, remove_mer=['avd', 'datum'])\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "        cbc.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100, use_best_model=True, verbose=verbose)\n",
    "    else:\n",
    "        cbc.fit(train_pool, use_best_model=True, verbose=verbose)\n",
    "    return cbc\n",
    "\n",
    "def beräkna_datum(X,fract=0.75):\n",
    "    ix_break = int(len(X.datum.unique())*fract)\n",
    "    test_start = X.datum.unique()[ix_break]\n",
    "    return test_start\n",
    "\n",
    "if False: # Kör en initiering av typerna \n",
    "    Xlearn, cat_features= prepare_for_catboost(X)  \n",
    "    # print(Xlearn.columns)\n",
    "    for typ in [typ6, typ1, typ9, typ16]:\n",
    "        print(typ.name)\n",
    "        Xtyp = typ.prepare_for_model(Xlearn)                                 ###########\n",
    "\n",
    "        if not typ.streck:                                                ################\n",
    "            Xtyp.drop('streck', axis=1, inplace=True)\n",
    "            \n",
    "        if True: # använda X_test    \n",
    "            test_start = beräkna_datum(Xtyp)    \n",
    "            X_train, X_test = Xtyp[Xtyp.datum < test_start], Xtyp[Xtyp.datum >= test_start]\n",
    "            y_train, y_test = y[X_train.index], y[X_test.index]\n",
    "            # print('innan learn',X_train.columns)\n",
    "            typ_model = learn(X_train, y_train, X_test, y_test)  ##########\n",
    "            print('best iteration',typ_model.best_iteration_)                             ##########\n",
    "            print('best score',    typ_model.best_score_)                                 ##########\n",
    "        # save model\n",
    "        typ.save_model(typ_model)                                                       ##########                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa typ6 till typ16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = läs_in_data_och_förbered()\n",
    "X,cat_features = tp.prepare_for_catboost(X)\n",
    "typ6.learn(X,y, iterations=33) # best iter = 25 {'Logloss': 0.23245952928761984, 'AUC': 0.8262112132692319}\n",
    "typ1.learn(X,y, iterations=39) # best iter = 39 {'Logloss': 0.23278308932319106, 'AUC': 0.826883367187688}\n",
    "typ9.learn(X,y, iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}\n",
    "typ16.learn(X,y,iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa stack predict med alla typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack predict for all models\n",
    "def stack_predict(X_, models):\n",
    "    X = X_.copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        X['proba'+nr] = typ.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "    # cols=X.columns[-8]    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete learning process with all steps in stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()  # The meta model\n",
    "    \n",
    "# fit my models on split date for timeseries   \n",
    "print('START fitting and predicting TimeseriesSplit') \n",
    "\n",
    "\n",
    "cross_val_predict=pd.DataFrame()\n",
    "for id_train, id_test in TimeSeriesSplit(n_splits=5).split(df_stack):  \n",
    "    for typ in [typ6, typ1, typ9, typ16]:\n",
    "        typ.learn(df_stack.loc[id_train],y.loc[id_train], iterations=25)\n",
    "    df_pred = stack_predict(df_stack.loc[id_test], [typ6, typ1, typ9, typ16])\n",
    "    df_pred['y']=y.loc[id_test]\n",
    "    cross_val_predict = pd.concat([cross_val_predict, df_pred.iloc[:,-9:]])\n",
    "       \n",
    "print('\\nFitting my models with df_stack')\n",
    "# final fit with all the available data\n",
    "for typ in [typ6, typ1, typ9, typ16]:\n",
    "    typ.learn(df_stack, y, iterations=20)\n",
    "\n",
    "print('\\nFitting meta_model on predicted above')\n",
    "# fit a rf meta_model on cross_val_predict\n",
    "meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "meta_model.fit(cross_val_predict.iloc[:, :-1], cross_val_predict.iloc[:, -1])\n",
    "print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "# pickle save stacking\n",
    "pickle.dump(meta_model, open('..\\\\modeller\\\\meta_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on unseen data\n",
    "def unseen_predictions(X_, models, meta_model):\n",
    "    X = X_.copy()\n",
    "    for model in models:\n",
    "        nr = model.name[3:]\n",
    "        X['proba'+nr] = model.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "        \n",
    "    return(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "\n",
    "# a small test:\n",
    "unseen_predictions(df_stack.iloc[-80:,:], [typ6, typ1, typ9, typ16], meta_model)[:,1],y.iloc[-80:].values\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
