{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En ny learning och validering   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beskrivning:  \n",
    "\n",
    "Normal learning:  \n",
    "- Learning: gör webscrape av omgång(ar) och kör en learning\n",
    "\n",
    "GridSearch optimering:  \n",
    "- gör gridsearch av de olika modellerna typ1, typ6, typ9 och typ16 samt meta_modellen\n",
    "  - inkludera imbalanced-lösningar\n",
    "  - spara de bästa hyperparametrarna\n",
    "  \n",
    "Värdera:  \n",
    "- Cross_validate: ts_split and ts cross_validate of the stack of models plus the meta_model\n",
    "  - save the results in pickle\n",
    "- Vinst: beräknar vinsten för de olika modellerna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Först kolla artiklar om stacking, cv för stacking samt cv för stacking av timeseries \n",
    "- https://machinelearningmastery.com/implementing-stacking-scratch-python/   (Även kod i Pieces)  \n",
    "Även allmänt om stacking ensembles  \n",
    "- https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning/  \n",
    "Slutligen CV för Timeseries stacking  (se kod i Pieces)  \n",
    "- https://datascience.stackexchange.com/questions/41378/how-to-apply-stacking-cross-validation-for-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generella funktioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moduler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from IPython.display import display\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "import typ as tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = {\n",
    "#   \"params\": {\n",
    "#     \"depth\": 4,\n",
    "#     \"l2_leaf_reg\": 10,\n",
    "#     \"learning_rate\": 0.005\n",
    "#   },\n",
    "#   \"cv_results\": {\n",
    "#     \"iterations\": [\n",
    "#       0,\n",
    "#       1,\n",
    "#       2,\n",
    "#       3,\n",
    "#       4,\n",
    "#       5,\n",
    "#       6,\n",
    "#       7,\n",
    "#       8,\n",
    "#       9,\n",
    "#       10,\n",
    "#       11,\n",
    "#       12,\n",
    "#       13,\n",
    "#       14,\n",
    "#       15,\n",
    "#       16,\n",
    "#       17,\n",
    "#       18,\n",
    "#       19,\n",
    "#       20,\n",
    "#       21,\n",
    "#       22,\n",
    "#       23,\n",
    "#       24,\n",
    "#       25,\n",
    "#       26,\n",
    "#       27,\n",
    "#       28,\n",
    "#       29,\n",
    "#       30,\n",
    "#       31,\n",
    "#       32,\n",
    "#       33,\n",
    "#       34,\n",
    "#       35,\n",
    "#       36,\n",
    "#       37,\n",
    "#       38,\n",
    "#       39,\n",
    "#       40,\n",
    "#       41,\n",
    "#       42,\n",
    "#       43,\n",
    "#       44,\n",
    "#       45,\n",
    "#       46,\n",
    "#       47,\n",
    "#       48,\n",
    "#       49,\n",
    "#       50,\n",
    "#       51,\n",
    "#       52,\n",
    "#       53,\n",
    "#       54,\n",
    "#       55,\n",
    "#       56,\n",
    "#       57,\n",
    "#       58,\n",
    "#       59,\n",
    "#       60,\n",
    "#       61,\n",
    "#       62,\n",
    "#       63,\n",
    "#       64,\n",
    "#       65,\n",
    "#       66,\n",
    "#       67,\n",
    "#       68,\n",
    "#       69,\n",
    "#       70,\n",
    "#       71,\n",
    "#       72,\n",
    "#       73,\n",
    "#       74,\n",
    "#       75,\n",
    "#       76,\n",
    "#       77,\n",
    "#       78,\n",
    "#       79,\n",
    "#       80,\n",
    "#       81,\n",
    "#       82,\n",
    "#       83,\n",
    "#       84,\n",
    "#       85,\n",
    "#       86,\n",
    "#       87,\n",
    "#       88,\n",
    "#       89,\n",
    "#       90,\n",
    "#       91,\n",
    "#       92,\n",
    "#       93,\n",
    "#       94,\n",
    "#       95,\n",
    "#       96,\n",
    "#       97,\n",
    "#       98,\n",
    "#       99,\n",
    "#       100,\n",
    "#       101,\n",
    "#       102,\n",
    "#       103,\n",
    "#       104,\n",
    "#       105,\n",
    "#       106,\n",
    "#       107,\n",
    "#       108,\n",
    "#       109,\n",
    "#       110,\n",
    "#       111,\n",
    "#       112,\n",
    "#       113,\n",
    "#       114,\n",
    "#       115,\n",
    "#       116,\n",
    "#       117,\n",
    "#       118,\n",
    "#       119,\n",
    "#       120,\n",
    "#       121,\n",
    "#       122,\n",
    "#       123,\n",
    "#       124,\n",
    "#       125,\n",
    "#       126,\n",
    "#       127,\n",
    "#       128,\n",
    "#       129,\n",
    "#       130,\n",
    "#       131,\n",
    "#       132,\n",
    "#       133,\n",
    "#       134,\n",
    "#       135,\n",
    "#       136,\n",
    "#       137,\n",
    "#       138,\n",
    "#       139,\n",
    "#       140,\n",
    "#       141,\n",
    "#       142,\n",
    "#       143,\n",
    "#       144,\n",
    "#       145,\n",
    "#       146,\n",
    "#       147,\n",
    "#       148,\n",
    "#       149,\n",
    "#       150,\n",
    "#       151,\n",
    "#       152,\n",
    "#       153,\n",
    "#       154,\n",
    "#       155,\n",
    "#       156,\n",
    "#       157,\n",
    "#       158,\n",
    "#       159,\n",
    "#       160,\n",
    "#       161,\n",
    "#       162,\n",
    "#       163,\n",
    "#       164,\n",
    "#       165,\n",
    "#       166,\n",
    "#       167,\n",
    "#       168,\n",
    "#       169,\n",
    "#       170,\n",
    "#       171,\n",
    "#       172,\n",
    "#       173,\n",
    "#       174,\n",
    "#       175,\n",
    "#       176,\n",
    "#       177,\n",
    "#       178,\n",
    "#       179,\n",
    "#       180,\n",
    "#       181,\n",
    "#       182,\n",
    "#       183,\n",
    "#       184,\n",
    "#       185,\n",
    "#       186,\n",
    "#       187,\n",
    "#       188,\n",
    "#       189,\n",
    "#       190,\n",
    "#       191,\n",
    "#       192,\n",
    "#       193,\n",
    "#       194,\n",
    "#       195,\n",
    "#       196,\n",
    "#       197,\n",
    "#       198,\n",
    "#       199,\n",
    "#       200,\n",
    "#       201,\n",
    "#       202,\n",
    "#       203,\n",
    "#       204,\n",
    "#       205,\n",
    "#       206,\n",
    "#       207,\n",
    "#       208,\n",
    "#       209,\n",
    "#       210,\n",
    "#       211,\n",
    "#       212,\n",
    "#       213,\n",
    "#       214,\n",
    "#       215,\n",
    "#       216,\n",
    "#       217,\n",
    "#       218,\n",
    "#       219,\n",
    "#       220,\n",
    "#       221,\n",
    "#       222,\n",
    "#       223,\n",
    "#       224,\n",
    "#       225,\n",
    "#       226,\n",
    "#       227,\n",
    "#       228,\n",
    "#       229,\n",
    "#       230,\n",
    "#       231,\n",
    "#       232,\n",
    "#       233,\n",
    "#       234,\n",
    "#       235,\n",
    "#       236,\n",
    "#       237,\n",
    "#       238,\n",
    "#       239,\n",
    "#       240,\n",
    "#       241,\n",
    "#       242,\n",
    "#       243,\n",
    "#       244,\n",
    "#       245,\n",
    "#       246,\n",
    "#       247,\n",
    "#       248,\n",
    "#       249,\n",
    "#       250,\n",
    "#       251,\n",
    "#       252,\n",
    "#       253,\n",
    "#       254,\n",
    "#       255,\n",
    "#       256,\n",
    "#       257,\n",
    "#       258,\n",
    "#       259,\n",
    "#       260,\n",
    "#       261,\n",
    "#       262,\n",
    "#       263,\n",
    "#       264,\n",
    "#       265,\n",
    "#       266,\n",
    "#       267,\n",
    "#       268,\n",
    "#       269,\n",
    "#       270,\n",
    "#       271,\n",
    "#       272,\n",
    "#       273,\n",
    "#       274,\n",
    "#       275,\n",
    "#       276,\n",
    "#       277,\n",
    "#       278,\n",
    "#       279,\n",
    "#       280,\n",
    "#       281,\n",
    "#       282,\n",
    "#       283,\n",
    "#       284,\n",
    "#       285,\n",
    "#       286,\n",
    "#       287,\n",
    "#       288,\n",
    "#       289,\n",
    "#       290,\n",
    "#       291,\n",
    "#       292,\n",
    "#       293,\n",
    "#       294,\n",
    "#       295,\n",
    "#       296,\n",
    "#       297,\n",
    "#       298,\n",
    "#       299,\n",
    "#       300,\n",
    "#       301,\n",
    "#       302,\n",
    "#       303,\n",
    "#       304,\n",
    "#       305,\n",
    "#       306,\n",
    "#       307,\n",
    "#       308,\n",
    "#       309,\n",
    "#       310,\n",
    "#       311,\n",
    "#       312,\n",
    "#       313,\n",
    "#       314,\n",
    "#       315,\n",
    "#       316,\n",
    "#       317,\n",
    "#       318,\n",
    "#       319,\n",
    "#       320,\n",
    "#       321,\n",
    "#       322,\n",
    "#       323,\n",
    "#       324,\n",
    "#       325,\n",
    "#       326,\n",
    "#       327,\n",
    "#       328,\n",
    "#       329,\n",
    "#       330,\n",
    "#       331,\n",
    "#       332,\n",
    "#       333,\n",
    "#       334,\n",
    "#       335,\n",
    "#       336,\n",
    "#       337,\n",
    "#       338,\n",
    "#       339,\n",
    "#       340,\n",
    "#       341,\n",
    "#       342,\n",
    "#       343,\n",
    "#       344,\n",
    "#       345,\n",
    "#       346,\n",
    "#       347,\n",
    "#       348,\n",
    "#       349,\n",
    "#       350,\n",
    "#       351,\n",
    "#       352,\n",
    "#       353,\n",
    "#       354,\n",
    "#       355,\n",
    "#       356,\n",
    "#       357,\n",
    "#       358,\n",
    "#       359,\n",
    "#       360,\n",
    "#       361,\n",
    "#       362,\n",
    "#       363,\n",
    "#       364,\n",
    "#       365,\n",
    "#       366,\n",
    "#       367,\n",
    "#       368,\n",
    "#       369,\n",
    "#       370,\n",
    "#       371,\n",
    "#       372,\n",
    "#       373,\n",
    "#       374,\n",
    "#       375,\n",
    "#       376,\n",
    "#       377,\n",
    "#       378,\n",
    "#       379,\n",
    "#       380,\n",
    "#       381,\n",
    "#       382,\n",
    "#       383,\n",
    "#       384,\n",
    "#       385,\n",
    "#       386,\n",
    "#       387,\n",
    "#       388,\n",
    "#       389,\n",
    "#       390,\n",
    "#       391,\n",
    "#       392,\n",
    "#       393,\n",
    "#       394,\n",
    "#       395,\n",
    "#       396,\n",
    "#       397,\n",
    "#       398,\n",
    "#       399,\n",
    "#       400,\n",
    "#       401,\n",
    "#       402,\n",
    "#       403,\n",
    "#       404,\n",
    "#       405,\n",
    "#       406,\n",
    "#       407,\n",
    "#       408,\n",
    "#       409,\n",
    "#       410,\n",
    "#       411,\n",
    "#       412,\n",
    "#       413,\n",
    "#       414,\n",
    "#       415,\n",
    "#       416,\n",
    "#       417,\n",
    "#       418,\n",
    "#       419,\n",
    "#       420,\n",
    "#       421,\n",
    "#       422,\n",
    "#       423,\n",
    "#       424,\n",
    "#       425,\n",
    "#       426,\n",
    "#       427,\n",
    "#       428,\n",
    "#       429,\n",
    "#       430,\n",
    "#       431,\n",
    "#       432,\n",
    "#       433,\n",
    "#       434,\n",
    "#       435,\n",
    "#       436,\n",
    "#       437,\n",
    "#       438,\n",
    "#       439,\n",
    "#       440,\n",
    "#       441,\n",
    "#       442,\n",
    "#       443,\n",
    "#       444,\n",
    "#       445,\n",
    "#       446,\n",
    "#       447,\n",
    "#       448,\n",
    "#       449,\n",
    "#       450,\n",
    "#       451,\n",
    "#       452,\n",
    "#       453,\n",
    "#       454,\n",
    "#       455,\n",
    "#       456,\n",
    "#       457,\n",
    "#       458,\n",
    "#       459,\n",
    "#       460,\n",
    "#       461,\n",
    "#       462,\n",
    "#       463,\n",
    "#       464,\n",
    "#       465,\n",
    "#       466,\n",
    "#       467,\n",
    "#       468,\n",
    "#       469,\n",
    "#       470,\n",
    "#       471,\n",
    "#       472,\n",
    "#       473,\n",
    "#       474,\n",
    "#       475,\n",
    "#       476,\n",
    "#       477,\n",
    "#       478,\n",
    "#       479,\n",
    "#       480,\n",
    "#       481,\n",
    "#       482,\n",
    "#       483,\n",
    "#       484,\n",
    "#       485,\n",
    "#       486,\n",
    "#       487,\n",
    "#       488,\n",
    "#       489,\n",
    "#       490,\n",
    "#       491,\n",
    "#       492,\n",
    "#       493,\n",
    "#       494,\n",
    "#       495,\n",
    "#       496,\n",
    "#       497,\n",
    "#       498,\n",
    "#       499\n",
    "#     ],\n",
    "#     \"test-AUC-mean\": [\n",
    "#       0.6501445582177403,\n",
    "#       0.6830849487605111,\n",
    "#       0.7156115919711511,\n",
    "#       0.7309962355998836,\n",
    "#       0.7361312777930147,\n",
    "#       0.7459376796439459,\n",
    "#       0.7470042225562677,\n",
    "#       0.7646931913400751,\n",
    "#       0.7738183678989163,\n",
    "#       0.7741340847675183,\n",
    "#       0.7761086685248506,\n",
    "#       0.7857562515024435,\n",
    "#       0.7856748564467371,\n",
    "#       0.7872462870769878,\n",
    "#       0.7907153636368006,\n",
    "#       0.7916710061171143,\n",
    "#       0.7924948639078324,\n",
    "#       0.7921823138495224,\n",
    "#       0.7923881442556245,\n",
    "#       0.7931227158993647,\n",
    "#       0.7931143309098689,\n",
    "#       0.7927442837109074,\n",
    "#       0.7927478713657361,\n",
    "#       0.7927500232240179,\n",
    "#       0.7926548059103699,\n",
    "#       0.7927444505149921,\n",
    "#       0.7927656591342677,\n",
    "#       0.7939976520050241,\n",
    "#       0.7947231621982729,\n",
    "#       0.7951821830298288,\n",
    "#       0.7956178032931894,\n",
    "#       0.7955405839665621,\n",
    "#       0.7954689379057963,\n",
    "#       0.796554135609986,\n",
    "#       0.7984846336985245,\n",
    "#       0.7985624193485278,\n",
    "#       0.7984017652575405,\n",
    "#       0.7983866846009152,\n",
    "#       0.798392927054227,\n",
    "#       0.7984856535402474,\n",
    "#       0.8005298604398124,\n",
    "#       0.8004224210206342,\n",
    "#       0.8002517150575634,\n",
    "#       0.8001836915966718,\n",
    "#       0.8002346849836499,\n",
    "#       0.8002170991731777,\n",
    "#       0.8002607495030514,\n",
    "#       0.8002281019463469,\n",
    "#       0.8002171070464733,\n",
    "#       0.8001694394160573,\n",
    "#       0.8016083335292608,\n",
    "#       0.801552496210536,\n",
    "#       0.8019780996433884,\n",
    "#       0.8023871920933712,\n",
    "#       0.8022687434999061,\n",
    "#       0.8020331111451086,\n",
    "#       0.8021368303520676,\n",
    "#       0.8022746042175953,\n",
    "#       0.8022294786231946,\n",
    "#       0.8020609178590927,\n",
    "#       0.8018022416239727,\n",
    "#       0.8017483660471465,\n",
    "#       0.8017081575097207,\n",
    "#       0.8016365495435492,\n",
    "#       0.8016882709705669,\n",
    "#       0.8017091165113082,\n",
    "#       0.8022758271092391,\n",
    "#       0.8023152523094567,\n",
    "#       0.8022617175398254,\n",
    "#       0.8023835689765105,\n",
    "#       0.8024136095083986,\n",
    "#       0.8024408572220139,\n",
    "#       0.8024674475957949,\n",
    "#       0.8025139814868163,\n",
    "#       0.802524535732478,\n",
    "#       0.8027997786467171,\n",
    "#       0.8024424187527549,\n",
    "#       0.8024013195326549,\n",
    "#       0.8023821536755076,\n",
    "#       0.8024800540391978,\n",
    "#       0.802537308496821,\n",
    "#       0.802766991713127,\n",
    "#       0.8027983047095375,\n",
    "#       0.8027627041836493,\n",
    "#       0.8030932646182162,\n",
    "#       0.8032576920897505,\n",
    "#       0.8033857259059332,\n",
    "#       0.8033180770038882,\n",
    "#       0.803373287579776,\n",
    "#       0.8033271419906287,\n",
    "#       0.8032764470933491,\n",
    "#       0.8033946666760841,\n",
    "#       0.8034366366781697,\n",
    "#       0.8040606007299266,\n",
    "#       0.8041341006891505,\n",
    "#       0.8041641897686815,\n",
    "#       0.8041153899634962,\n",
    "#       0.8040951858298007,\n",
    "#       0.803951161521204,\n",
    "#       0.8039972145810939,\n",
    "#       0.8038605415588771,\n",
    "#       0.8046520820314405,\n",
    "#       0.8048799853745774,\n",
    "#       0.8051996235596939,\n",
    "#       0.8052032223392332,\n",
    "#       0.8051859487340531,\n",
    "#       0.8051621644906856,\n",
    "#       0.8066925078398814,\n",
    "#       0.8076747347461788,\n",
    "#       0.8080577556146522,\n",
    "#       0.8080747283669284,\n",
    "#       0.8083865375483661,\n",
    "#       0.8083536861825049,\n",
    "#       0.8084867429514372,\n",
    "#       0.809154087862494,\n",
    "#       0.8091781084698452,\n",
    "#       0.8091121808821843,\n",
    "#       0.8090630083294863,\n",
    "#       0.8090324842481105,\n",
    "#       0.8087955410302495,\n",
    "#       0.8092311069177887,\n",
    "#       0.809648347832202,\n",
    "#       0.8096645820685582,\n",
    "#       0.8096827172059722,\n",
    "#       0.8095631948971477,\n",
    "#       0.8094271701242942,\n",
    "#       0.809458285107843,\n",
    "#       0.8094795391192985,\n",
    "#       0.8094167005580376,\n",
    "#       0.8093758613864012,\n",
    "#       0.8092903970846074,\n",
    "#       0.8092551516088623,\n",
    "#       0.8092412375312927,\n",
    "#       0.8092121589844983,\n",
    "#       0.8091986302282548,\n",
    "#       0.8092385927554944,\n",
    "#       0.8092061756759161,\n",
    "#       0.8096205210103576,\n",
    "#       0.8096299918791562,\n",
    "#       0.8096996067299843,\n",
    "#       0.8097272616062154,\n",
    "#       0.8098266977964051,\n",
    "#       0.809808048693425,\n",
    "#       0.8097824183977004,\n",
    "#       0.8098521371493025,\n",
    "#       0.8098849167312613,\n",
    "#       0.8098623385182615,\n",
    "#       0.8098986317728928,\n",
    "#       0.8098472242450143,\n",
    "#       0.8098030136996854,\n",
    "#       0.8097315316765803,\n",
    "#       0.809713347117051,\n",
    "#       0.8097998435853215,\n",
    "#       0.809798084195363,\n",
    "#       0.8098231793367321,\n",
    "#       0.8098004919289143,\n",
    "#       0.8097757621785255,\n",
    "#       0.80973653847173,\n",
    "#       0.8098994011666425,\n",
    "#       0.8099443655946995,\n",
    "#       0.8100039263269814,\n",
    "#       0.8100068061798478,\n",
    "#       0.8100542619525074,\n",
    "#       0.810084602080733,\n",
    "#       0.8101022266605813,\n",
    "#       0.8101066842338274,\n",
    "#       0.8104538138400048,\n",
    "#       0.8105008136776389,\n",
    "#       0.8104646107701068,\n",
    "#       0.8104615334259597,\n",
    "#       0.810474483511442,\n",
    "#       0.8106494063214911,\n",
    "#       0.8106402120465678,\n",
    "#       0.810669910083981,\n",
    "#       0.8106622368950209,\n",
    "#       0.8106531963505482,\n",
    "#       0.8109022609326748,\n",
    "#       0.8108994225526978,\n",
    "#       0.8108367211804277,\n",
    "#       0.8108802235458343,\n",
    "#       0.8108935486621792,\n",
    "#       0.8108482332865672,\n",
    "#       0.810841663690487,\n",
    "#       0.8111288130833307,\n",
    "#       0.8111841116903173,\n",
    "#       0.8111788728650711,\n",
    "#       0.8111819776802877,\n",
    "#       0.811166342231161,\n",
    "#       0.8111444853261289,\n",
    "#       0.8111358756633619,\n",
    "#       0.8111434593254693,\n",
    "#       0.8111762822818683,\n",
    "#       0.8112649492827465,\n",
    "#       0.811251060037837,\n",
    "#       0.811245415593668,\n",
    "#       0.8112681767429152,\n",
    "#       0.8112563096868911,\n",
    "#       0.8112769234360606,\n",
    "#       0.8113546368928729,\n",
    "#       0.8113572188181575,\n",
    "#       0.811527330888255,\n",
    "#       0.8118502276267003,\n",
    "#       0.8118553472422564,\n",
    "#       0.8118677885888534,\n",
    "#       0.8118614170702282,\n",
    "#       0.8118576497793203,\n",
    "#       0.811825487082627,\n",
    "#       0.81195407672473,\n",
    "#       0.8120105604742618,\n",
    "#       0.8119941464992954,\n",
    "#       0.8120197212608915,\n",
    "#       0.81212301092749,\n",
    "#       0.8121334385435649,\n",
    "#       0.8121538595707685,\n",
    "#       0.812248662036632,\n",
    "#       0.8122588294169562,\n",
    "#       0.8122253335551081,\n",
    "#       0.8121927240639994,\n",
    "#       0.8121981026351996,\n",
    "#       0.8122332119062188,\n",
    "#       0.8122376894058319,\n",
    "#       0.8121879625768216,\n",
    "#       0.8121971656054239,\n",
    "#       0.8122102292520754,\n",
    "#       0.8121859758196823,\n",
    "#       0.8121656036798235,\n",
    "#       0.8121431812954893,\n",
    "#       0.8121484699180177,\n",
    "#       0.8121222684425154,\n",
    "#       0.8121099087078216,\n",
    "#       0.8121407976819002,\n",
    "#       0.8121535726146029,\n",
    "#       0.8123008974169595,\n",
    "#       0.8123033737224163,\n",
    "#       0.812305631069448,\n",
    "#       0.8123094238568335,\n",
    "#       0.8122812059575477,\n",
    "#       0.8122829898240369,\n",
    "#       0.8123663319798619,\n",
    "#       0.8123730879361156,\n",
    "#       0.8124045455809972,\n",
    "#       0.8124155050458434,\n",
    "#       0.8124238959572398,\n",
    "#       0.8124245964066457,\n",
    "#       0.8124241641377894,\n",
    "#       0.8124342067350673,\n",
    "#       0.8124362099609563,\n",
    "#       0.8124281670561702,\n",
    "#       0.8124062121549471,\n",
    "#       0.8124043524647225,\n",
    "#       0.8124058488288398,\n",
    "#       0.8124328720125092,\n",
    "#       0.8124922340530618,\n",
    "#       0.8124792580964272,\n",
    "#       0.8124847426050085,\n",
    "#       0.8124874485628837,\n",
    "#       0.8124818317277664,\n",
    "#       0.8124797264983418,\n",
    "#       0.8124815223173154,\n",
    "#       0.8124615112485051,\n",
    "#       0.8124582637260248,\n",
    "#       0.8124596464882139,\n",
    "#       0.8125200666407932,\n",
    "#       0.8125181562254511,\n",
    "#       0.8126766443219793,\n",
    "#       0.8126851747799908,\n",
    "#       0.812760996560481,\n",
    "#       0.8127979147965452,\n",
    "#       0.8130317239100997,\n",
    "#       0.8130307792373028,\n",
    "#       0.8132142567368126,\n",
    "#       0.8132571809083691,\n",
    "#       0.8132525591115704,\n",
    "#       0.813400034715802,\n",
    "#       0.8134755099375877,\n",
    "#       0.8135077200067039,\n",
    "#       0.8135015321126009,\n",
    "#       0.8134984930737186,\n",
    "#       0.8135034890567001,\n",
    "#       0.8135089968724593,\n",
    "#       0.8135019343637662,\n",
    "#       0.8135038934569009,\n",
    "#       0.8135042706798974,\n",
    "#       0.813508520164637,\n",
    "#       0.8135039236969842,\n",
    "#       0.8135956900101211,\n",
    "#       0.8135753050505243,\n",
    "#       0.8135720554516725,\n",
    "#       0.8136242750615363,\n",
    "#       0.8136550056930816,\n",
    "#       0.8136479751046621,\n",
    "#       0.8136307197478541,\n",
    "#       0.8136385514763399,\n",
    "#       0.8136291399150706,\n",
    "#       0.8136044094064749,\n",
    "#       0.8136064085422069,\n",
    "#       0.8136402332986702,\n",
    "#       0.8136449014485632,\n",
    "#       0.813650668322027,\n",
    "#       0.8136541559253965,\n",
    "#       0.8136482976367253,\n",
    "#       0.8136460846335242,\n",
    "#       0.8136527854325315,\n",
    "#       0.8137643607500958,\n",
    "#       0.8137489048625272,\n",
    "#       0.8137588540673519,\n",
    "#       0.8137599880221684,\n",
    "#       0.8137627283598039,\n",
    "#       0.8137939742220558,\n",
    "#       0.8137772290380461,\n",
    "#       0.8137649014443669,\n",
    "#       0.8137662395432889,\n",
    "#       0.8137752045703908,\n",
    "#       0.8137910288836088,\n",
    "#       0.8137932892114407,\n",
    "#       0.8138063944459635,\n",
    "#       0.8138093233229864,\n",
    "#       0.813820053143013,\n",
    "#       0.8138197371090842,\n",
    "#       0.8138256270688269,\n",
    "#       0.8138824631421813,\n",
    "#       0.8138851626865836,\n",
    "#       0.8139395840449234,\n",
    "#       0.8139400138762939,\n",
    "#       0.8139637131170604,\n",
    "#       0.8139794391302632,\n",
    "#       0.8139796359959716,\n",
    "#       0.8139690599108601,\n",
    "#       0.8139681963293214,\n",
    "#       0.814005921031395,\n",
    "#       0.8140114256896942,\n",
    "#       0.8140204846470084,\n",
    "#       0.8140418770730609,\n",
    "#       0.8140566890676322,\n",
    "#       0.8140532451340373,\n",
    "#       0.8140593349096641,\n",
    "#       0.8140669103508763,\n",
    "#       0.8140888735578178,\n",
    "#       0.8141919292647136,\n",
    "#       0.8141910175753561,\n",
    "#       0.8142047744204962,\n",
    "#       0.8142031180425804,\n",
    "#       0.8142560177930767,\n",
    "#       0.8142999247674915,\n",
    "#       0.8143167730772737,\n",
    "#       0.8143217032050007,\n",
    "#       0.8143186749385787,\n",
    "#       0.8143410434418785,\n",
    "#       0.8143416151822667,\n",
    "#       0.8143474515245936,\n",
    "#       0.8143597365401591,\n",
    "#       0.8144652759192313,\n",
    "#       0.8144635923436766,\n",
    "#       0.8144539917723254,\n",
    "#       0.8144522198432007,\n",
    "#       0.814442427579038,\n",
    "#       0.8144341460189171,\n",
    "#       0.8144407891296037,\n",
    "#       0.814451893095953,\n",
    "#       0.8144467844243646,\n",
    "#       0.814445020377131,\n",
    "#       0.8144462962418306,\n",
    "#       0.8144484928099814,\n",
    "#       0.8144517956853417,\n",
    "#       0.8144526477801873,\n",
    "#       0.8144401764941442,\n",
    "#       0.8144373038272305,\n",
    "#       0.814511523763462,\n",
    "#       0.814513004031391,\n",
    "#       0.8145107478914589,\n",
    "#       0.8145498188937687,\n",
    "#       0.8145363587062289,\n",
    "#       0.814535417341434,\n",
    "#       0.8145383123703558,\n",
    "#       0.814561883922227,\n",
    "#       0.8145716897762647,\n",
    "#       0.8145674107871358,\n",
    "#       0.8145717890817258,\n",
    "#       0.8145726728567848,\n",
    "#       0.8145837467261068,\n",
    "#       0.8145727553409902,\n",
    "#       0.8145614629424388,\n",
    "#       0.8145580773937964,\n",
    "#       0.814562486651182,\n",
    "#       0.814568266414747,\n",
    "#       0.8145862871198328,\n",
    "#       0.8145913829136416,\n",
    "#       0.8145905465734563,\n",
    "#       0.8145825506558422,\n",
    "#       0.8145810487821677,\n",
    "#       0.814583675099066,\n",
    "#       0.8146081430187362,\n",
    "#       0.8147020554135406,\n",
    "#       0.8147033130754847,\n",
    "#       0.8146991479222401,\n",
    "#       0.8146990661229045,\n",
    "#       0.8147007385671301,\n",
    "#       0.814713691667795,\n",
    "#       0.8147291362876226,\n",
    "#       0.8147403958346529,\n",
    "#       0.8147435072473741,\n",
    "#       0.8147408756263651,\n",
    "#       0.8147400933936659,\n",
    "#       0.8147152088787963,\n",
    "#       0.8147121463407332,\n",
    "#       0.8147103301762145,\n",
    "#       0.8147072405377467,\n",
    "#       0.814694194592081,\n",
    "#       0.814690449596549,\n",
    "#       0.8147235625814488,\n",
    "#       0.8147121247406683,\n",
    "#       0.8146932924884709,\n",
    "#       0.814695566532639,\n",
    "#       0.8147020747152574,\n",
    "#       0.8146993859468639,\n",
    "#       0.8147552200296275,\n",
    "#       0.8147441947472555,\n",
    "#       0.8147390266090792,\n",
    "#       0.8147341967918532,\n",
    "#       0.8147300830178311,\n",
    "#       0.8147519747807095,\n",
    "#       0.8147420054377461,\n",
    "#       0.8147425278916586,\n",
    "#       0.8147470863283853,\n",
    "#       0.8147495490724831,\n",
    "#       0.8147576153378745,\n",
    "#       0.8147513794712833,\n",
    "#       0.8147487075033837,\n",
    "#       0.8147542678695437,\n",
    "#       0.8147584371134418,\n",
    "#       0.8147549479292483,\n",
    "#       0.8147501406505324,\n",
    "#       0.8147500031171944,\n",
    "#       0.8147550793467133,\n",
    "#       0.8147537226967343,\n",
    "#       0.8147365779829606,\n",
    "#       0.8147456529951939,\n",
    "#       0.81474544755996,\n",
    "#       0.8147370687385138,\n",
    "#       0.8147390433478963,\n",
    "#       0.8147416127916921,\n",
    "#       0.8147418457325735,\n",
    "#       0.8147320463771891,\n",
    "#       0.8147682190906176,\n",
    "#       0.8147850467267823,\n",
    "#       0.8147940940902745,\n",
    "#       0.8147906391023488,\n",
    "#       0.8147917308896317,\n",
    "#       0.8147886274311034,\n",
    "#       0.8147929151646309,\n",
    "#       0.8147785111607501,\n",
    "#       0.8147828544717528,\n",
    "#       0.8147784657953693,\n",
    "#       0.8147779197261767,\n",
    "#       0.8147821367208001,\n",
    "#       0.8147739644820294,\n",
    "#       0.8147786738803131,\n",
    "#       0.8147597390195459,\n",
    "#       0.8147584459205225,\n",
    "#       0.8147566340893393,\n",
    "#       0.8147688152243491,\n",
    "#       0.8147505019518908,\n",
    "#       0.8147738882702352,\n",
    "#       0.8147565442253024,\n",
    "#       0.8147442237678625,\n",
    "#       0.8147339645243793,\n",
    "#       0.8147320281516093,\n",
    "#       0.8147254434866653,\n",
    "#       0.8147224234284416,\n",
    "#       0.8147226390400488,\n",
    "#       0.8147283685382016,\n",
    "#       0.8147186170522864,\n",
    "#       0.8147184928418225,\n",
    "#       0.8147243982919521,\n",
    "#       0.8147225341433291,\n",
    "#       0.8147131157342006,\n",
    "#       0.8147032789769874,\n",
    "#       0.8146983971033613,\n",
    "#       0.814707016173544,\n",
    "#       0.814705927651719,\n",
    "#       0.8147143849884666,\n",
    "#       0.8147175949681431,\n",
    "#       0.8147075317475025,\n",
    "#       0.8147161650526169,\n",
    "#       0.8147296667675981,\n",
    "#       0.8147183240160982,\n",
    "#       0.814719526404577,\n",
    "#       0.8147066401365182,\n",
    "#       0.8146870913561439,\n",
    "#       0.8146698359565507,\n",
    "#       0.8146508126876959,\n",
    "#       0.8146336445323529,\n",
    "#       0.814617392241102,\n",
    "#       0.8146000848531031,\n",
    "#       0.8145839167512927,\n",
    "#       0.8145825933261864,\n",
    "#       0.8145486514440934,\n",
    "#       0.8145337963194292,\n",
    "#       0.8144794065716823,\n",
    "#       0.8144061789126971\n",
    "#     ],\n",
    "#     \"test-AUC-std\": [\n",
    "#       0.08735102423733136,\n",
    "#       0.07642732900395896,\n",
    "#       0.028781659414518164,\n",
    "#       0.035952405475023215,\n",
    "#       0.03467484667131059,\n",
    "#       0.028910209756523333,\n",
    "#       0.028436202183533786,\n",
    "#       0.02142931316055262,\n",
    "#       0.014108925424961298,\n",
    "#       0.015011625754876435,\n",
    "#       0.014167096449077524,\n",
    "#       0.012816654944360581,\n",
    "#       0.012811354072487638,\n",
    "#       0.010765446643767735,\n",
    "#       0.0099937248504276,\n",
    "#       0.008789944193099366,\n",
    "#       0.00905755180414565,\n",
    "#       0.009118904560113052,\n",
    "#       0.009135229307334083,\n",
    "#       0.008740523077297158,\n",
    "#       0.008139209991681566,\n",
    "#       0.008117342732775706,\n",
    "#       0.008205245576302315,\n",
    "#       0.008198417160241106,\n",
    "#       0.008146692871673918,\n",
    "#       0.008131921578186102,\n",
    "#       0.008130917372945126,\n",
    "#       0.007435027357463022,\n",
    "#       0.006756465397111823,\n",
    "#       0.006909342608485994,\n",
    "#       0.0062039975163290136,\n",
    "#       0.00627559280405464,\n",
    "#       0.006253740209838669,\n",
    "#       0.006918399984908999,\n",
    "#       0.007975706502764872,\n",
    "#       0.007917250070305162,\n",
    "#       0.008039342188894282,\n",
    "#       0.00790634032512512,\n",
    "#       0.007589591986226861,\n",
    "#       0.007186278460614132,\n",
    "#       0.004642908996984821,\n",
    "#       0.004709676451315726,\n",
    "#       0.004433557091615384,\n",
    "#       0.004569992745052395,\n",
    "#       0.004314946991001574,\n",
    "#       0.004138996048536762,\n",
    "#       0.004081204957497913,\n",
    "#       0.0039501168469735,\n",
    "#       0.003970065172106791,\n",
    "#       0.003982161005412181,\n",
    "#       0.001591723724222799,\n",
    "#       0.0015470444458043227,\n",
    "#       0.0017300216168403982,\n",
    "#       0.0019772154763999807,\n",
    "#       0.0018804861388064993,\n",
    "#       0.001792804920678895,\n",
    "#       0.0018560141491675533,\n",
    "#       0.0021133823552303083,\n",
    "#       0.0022565170855437444,\n",
    "#       0.0024426933126008845,\n",
    "#       0.002551820696129142,\n",
    "#       0.0024621794661456855,\n",
    "#       0.0024557397917267763,\n",
    "#       0.0024191136540865034,\n",
    "#       0.0025986673889064945,\n",
    "#       0.0025330448094639598,\n",
    "#       0.002446666072877313,\n",
    "#       0.002472340737217346,\n",
    "#       0.002522462976111519,\n",
    "#       0.0023746387624092326,\n",
    "#       0.0024815483276636762,\n",
    "#       0.0024571140357212188,\n",
    "#       0.0023500265535940975,\n",
    "#       0.00230647742768447,\n",
    "#       0.002196248894606826,\n",
    "#       0.001857652989236371,\n",
    "#       0.0015858622159026757,\n",
    "#       0.001562343679968679,\n",
    "#       0.0015076955368626675,\n",
    "#       0.0017681916829930908,\n",
    "#       0.00179940783640214,\n",
    "#       0.0017058496862994894,\n",
    "#       0.00182070142260946,\n",
    "#       0.0018320783862376862,\n",
    "#       0.0013328298703404694,\n",
    "#       0.0009382366821664066,\n",
    "#       0.0007279510274270002,\n",
    "#       0.0008234690658861021,\n",
    "#       0.0007059262752935065,\n",
    "#       0.0006771867321252829,\n",
    "#       0.0006431955906536588,\n",
    "#       0.0006866518662799204,\n",
    "#       0.0007962516014439923,\n",
    "#       0.0007305521160166469,\n",
    "#       0.0008350606730008738,\n",
    "#       0.0007643755475048745,\n",
    "#       0.0008342749134378907,\n",
    "#       0.0008211025406112652,\n",
    "#       0.0007831469474822233,\n",
    "#       0.000735720060351046,\n",
    "#       0.000814434034760413,\n",
    "#       0.0014685277282362654,\n",
    "#       0.001940023945472554,\n",
    "#       0.002655375232039024,\n",
    "#       0.002613968607178782,\n",
    "#       0.002592363613082219,\n",
    "#       0.0026477783982961265,\n",
    "#       0.0022699032570142317,\n",
    "#       0.0022900349971619054,\n",
    "#       0.0023428309715919126,\n",
    "#       0.0022987916397755937,\n",
    "#       0.00252887937817893,\n",
    "#       0.0024734966461248706,\n",
    "#       0.0025823494466546706,\n",
    "#       0.003554436763178885,\n",
    "#       0.003544276703460187,\n",
    "#       0.0035028585918372174,\n",
    "#       0.0034848687167651454,\n",
    "#       0.0034963072189404874,\n",
    "#       0.0034187731386140095,\n",
    "#       0.004171672591303856,\n",
    "#       0.00391242927646922,\n",
    "#       0.003963117050922718,\n",
    "#       0.0038580607423007907,\n",
    "#       0.003935561254618832,\n",
    "#       0.004007198313170188,\n",
    "#       0.004102690600749024,\n",
    "#       0.0040965121437881325,\n",
    "#       0.004113107407712249,\n",
    "#       0.004050956649262081,\n",
    "#       0.004044668718808554,\n",
    "#       0.00404870702373383,\n",
    "#       0.004044683527771352,\n",
    "#       0.003974777657001148,\n",
    "#       0.003982114560271438,\n",
    "#       0.004080898228171026,\n",
    "#       0.0040889297517848425,\n",
    "#       0.004119948952252444,\n",
    "#       0.004110179562909604,\n",
    "#       0.004110195745258887,\n",
    "#       0.0041173587259824155,\n",
    "#       0.004150857164121163,\n",
    "#       0.0040978614415069035,\n",
    "#       0.004091334190814187,\n",
    "#       0.004028271115768541,\n",
    "#       0.004071899977814698,\n",
    "#       0.004082461676432368,\n",
    "#       0.004186099578897403,\n",
    "#       0.004126303700481438,\n",
    "#       0.004129097563229527,\n",
    "#       0.0042089254846514,\n",
    "#       0.004202961968677196,\n",
    "#       0.004168307169933901,\n",
    "#       0.004186051721089362,\n",
    "#       0.004182231388766756,\n",
    "#       0.0041870784174624955,\n",
    "#       0.004154697898079632,\n",
    "#       0.00419155231820799,\n",
    "#       0.004197790852283378,\n",
    "#       0.004195322554582729,\n",
    "#       0.0041864435543044285,\n",
    "#       0.004262437412820164,\n",
    "#       0.004268810156377932,\n",
    "#       0.004288504123629161,\n",
    "#       0.00430957768633306,\n",
    "#       0.004348908646952301,\n",
    "#       0.004268732988990414,\n",
    "#       0.004408467322989005,\n",
    "#       0.004340462765239393,\n",
    "#       0.004347627251287743,\n",
    "#       0.004349677352349973,\n",
    "#       0.004214939763800516,\n",
    "#       0.004217892024052231,\n",
    "#       0.0042904973468330145,\n",
    "#       0.004273622551768479,\n",
    "#       0.004270074536419633,\n",
    "#       0.004396589225143238,\n",
    "#       0.004413930099937544,\n",
    "#       0.00448955740670365,\n",
    "#       0.004520098528917731,\n",
    "#       0.004525057146630181,\n",
    "#       0.004464891323973175,\n",
    "#       0.004479514793629427,\n",
    "#       0.004195473406955841,\n",
    "#       0.004217641552160224,\n",
    "#       0.004224387948720988,\n",
    "#       0.004222710261667556,\n",
    "#       0.004199721016285514,\n",
    "#       0.004206090951270233,\n",
    "#       0.0042216439045579535,\n",
    "#       0.004283399814040301,\n",
    "#       0.004271364958848466,\n",
    "#       0.004181580311609591,\n",
    "#       0.004165221035605824,\n",
    "#       0.004177513734835014,\n",
    "#       0.004174166539235509,\n",
    "#       0.0041821160367075,\n",
    "#       0.004139270349224701,\n",
    "#       0.004186490729134239,\n",
    "#       0.004157370768661135,\n",
    "#       0.003920110888200425,\n",
    "#       0.004292449729626349,\n",
    "#       0.004307981149798728,\n",
    "#       0.004312694027783379,\n",
    "#       0.004311368723829381,\n",
    "#       0.004301112858125824,\n",
    "#       0.004332181779261742,\n",
    "#       0.004496265872672005,\n",
    "#       0.004505007286480478,\n",
    "#       0.00447223888407298,\n",
    "#       0.004486166875143344,\n",
    "#       0.0045787991313084074,\n",
    "#       0.004569766875682328,\n",
    "#       0.00461709828895334,\n",
    "#       0.004466795432052763,\n",
    "#       0.004477413310094478,\n",
    "#       0.004398814083036391,\n",
    "#       0.004434141999658532,\n",
    "#       0.004419030533594694,\n",
    "#       0.004392459867294065,\n",
    "#       0.004377860490844009,\n",
    "#       0.004363059321091003,\n",
    "#       0.004370709616183315,\n",
    "#       0.004402098924115111,\n",
    "#       0.004384883582042653,\n",
    "#       0.004399207911688513,\n",
    "#       0.004389925989030872,\n",
    "#       0.0043965400722135945,\n",
    "#       0.004384722382640267,\n",
    "#       0.004383202583787131,\n",
    "#       0.00439424525573893,\n",
    "#       0.004402924291700324,\n",
    "#       0.004302163744946915,\n",
    "#       0.004286261867423465,\n",
    "#       0.004287644937863708,\n",
    "#       0.004291414264897345,\n",
    "#       0.004295084259756595,\n",
    "#       0.004296316776766478,\n",
    "#       0.004149007048953081,\n",
    "#       0.004167577638807558,\n",
    "#       0.00415901046962831,\n",
    "#       0.004155511992283995,\n",
    "#       0.004164084453552634,\n",
    "#       0.004166812916857821,\n",
    "#       0.0041581955825713115,\n",
    "#       0.004140157628586188,\n",
    "#       0.004153595927600947,\n",
    "#       0.004143449265683424,\n",
    "#       0.0041065654933559554,\n",
    "#       0.004115045312362471,\n",
    "#       0.0041171377527910715,\n",
    "#       0.004121350759204994,\n",
    "#       0.004058600135655081,\n",
    "#       0.0041265786656182555,\n",
    "#       0.004133938199490577,\n",
    "#       0.00413980202584124,\n",
    "#       0.004144492397581951,\n",
    "#       0.004134000570076198,\n",
    "#       0.004137800601368902,\n",
    "#       0.00414422226790081,\n",
    "#       0.0041236825892545185,\n",
    "#       0.004131020172771867,\n",
    "#       0.0040847528956112,\n",
    "#       0.004085789397666194,\n",
    "#       0.004265254476081078,\n",
    "#       0.004268868303992861,\n",
    "#       0.004393914440218737,\n",
    "#       0.004361142111168208,\n",
    "#       0.004397168490022378,\n",
    "#       0.004402647609053355,\n",
    "#       0.004676215182164791,\n",
    "#       0.004614421793978303,\n",
    "#       0.004615281978616181,\n",
    "#       0.004861225823230146,\n",
    "#       0.004992517618664949,\n",
    "#       0.005034934024007013,\n",
    "#       0.005029224994044318,\n",
    "#       0.005025090271259098,\n",
    "#       0.005025295431568279,\n",
    "#       0.005033264805251195,\n",
    "#       0.005026153242077909,\n",
    "#       0.00501647777372268,\n",
    "#       0.005009731623005101,\n",
    "#       0.0050054666930489135,\n",
    "#       0.005010126614053608,\n",
    "#       0.0051276138798398785,\n",
    "#       0.005157285330357457,\n",
    "#       0.00516651183124114,\n",
    "#       0.005209136030121012,\n",
    "#       0.005216640954432435,\n",
    "#       0.005224442792803938,\n",
    "#       0.0052089270649803504,\n",
    "#       0.0051956391372224775,\n",
    "#       0.005225429589721437,\n",
    "#       0.005254620181768193,\n",
    "#       0.0052597080117404065,\n",
    "#       0.005233810444427337,\n",
    "#       0.005238387195541573,\n",
    "#       0.005257174316974114,\n",
    "#       0.005249919541864653,\n",
    "#       0.005249260199510695,\n",
    "#       0.005246414086027054,\n",
    "#       0.005229404800175512,\n",
    "#       0.005257863465556817,\n",
    "#       0.0052769924038138976,\n",
    "#       0.00527447301784025,\n",
    "#       0.005295197744462094,\n",
    "#       0.005292663918906933,\n",
    "#       0.005310794605660217,\n",
    "#       0.00531147106293476,\n",
    "#       0.005306192846961479,\n",
    "#       0.005305421918261339,\n",
    "#       0.0053288466936925565,\n",
    "#       0.005358181563961313,\n",
    "#       0.0053603404753311676,\n",
    "#       0.0053565528359022264,\n",
    "#       0.00535595965710224,\n",
    "#       0.005361777172316459,\n",
    "#       0.005361577698191366,\n",
    "#       0.005367901317340929,\n",
    "#       0.005326095830850505,\n",
    "#       0.0053235852363898864,\n",
    "#       0.005278358203312847,\n",
    "#       0.005280321116902226,\n",
    "#       0.0052804686736636144,\n",
    "#       0.005269677322593827,\n",
    "#       0.005271445974838141,\n",
    "#       0.0052656131426754346,\n",
    "#       0.00527720980175384,\n",
    "#       0.0052589354387784495,\n",
    "#       0.005281209537043429,\n",
    "#       0.005285747629139963,\n",
    "#       0.005293847504833145,\n",
    "#       0.005295444821185321,\n",
    "#       0.005294807747044476,\n",
    "#       0.0052670231205438735,\n",
    "#       0.0052818915829572505,\n",
    "#       0.005294978129957061,\n",
    "#       0.005096449679175712,\n",
    "#       0.005093349222400899,\n",
    "#       0.005099148308557382,\n",
    "#       0.005098677125365127,\n",
    "#       0.0050823913845097594,\n",
    "#       0.005071859949889653,\n",
    "#       0.005086228578589199,\n",
    "#       0.005087930897055892,\n",
    "#       0.005088609556867185,\n",
    "#       0.005093674792451779,\n",
    "#       0.005086051176317248,\n",
    "#       0.005083337311641022,\n",
    "#       0.005078006241085114,\n",
    "#       0.005226268907148006,\n",
    "#       0.005223383974393551,\n",
    "#       0.005221306593336379,\n",
    "#       0.005216494092041479,\n",
    "#       0.00523591812788402,\n",
    "#       0.005230288365421574,\n",
    "#       0.005240226086424605,\n",
    "#       0.005249299751931401,\n",
    "#       0.005241151561095445,\n",
    "#       0.005238260379375266,\n",
    "#       0.005240417573646147,\n",
    "#       0.0052474750142685895,\n",
    "#       0.005250530592060655,\n",
    "#       0.005252245038966898,\n",
    "#       0.005253057175869848,\n",
    "#       0.005250470915808405,\n",
    "#       0.005238089323384801,\n",
    "#       0.005239963672435268,\n",
    "#       0.005237601106272593,\n",
    "#       0.005203174074717011,\n",
    "#       0.005195219448000343,\n",
    "#       0.005195101743184343,\n",
    "#       0.005188657689261956,\n",
    "#       0.005191393864375839,\n",
    "#       0.005199892681952752,\n",
    "#       0.005202363876078575,\n",
    "#       0.005198653524507575,\n",
    "#       0.005196503534709249,\n",
    "#       0.00519889144626372,\n",
    "#       0.0051934375986179395,\n",
    "#       0.0051808935740123635,\n",
    "#       0.005182613352460798,\n",
    "#       0.005170936362184097,\n",
    "#       0.005172298011417583,\n",
    "#       0.005188447293135969,\n",
    "#       0.005182164465784304,\n",
    "#       0.005179864543141761,\n",
    "#       0.005180669424076136,\n",
    "#       0.00518034500251076,\n",
    "#       0.005187940112394802,\n",
    "#       0.005170997013714964,\n",
    "#       0.005005302081443029,\n",
    "#       0.005008393747685105,\n",
    "#       0.005007162985603904,\n",
    "#       0.0050052448703042955,\n",
    "#       0.0050089365567898125,\n",
    "#       0.005021115298259855,\n",
    "#       0.0049884463011042185,\n",
    "#       0.004997256862067612,\n",
    "#       0.005002841305163776,\n",
    "#       0.005002536766024773,\n",
    "#       0.004996830704793453,\n",
    "#       0.004995696388714994,\n",
    "#       0.004995040729817809,\n",
    "#       0.004985313209096681,\n",
    "#       0.004976263483693072,\n",
    "#       0.004974640105037083,\n",
    "#       0.004975662242722735,\n",
    "#       0.0049675283290887655,\n",
    "#       0.0049652168745681294,\n",
    "#       0.00495980826211035,\n",
    "#       0.004959557337402815,\n",
    "#       0.004967737532637946,\n",
    "#       0.004973076832930333,\n",
    "#       0.004901476034479943,\n",
    "#       0.0049052880215028965,\n",
    "#       0.004898232326616128,\n",
    "#       0.004886778249642237,\n",
    "#       0.004882578552920832,\n",
    "#       0.004887311992034308,\n",
    "#       0.004881366264004629,\n",
    "#       0.004890864633458381,\n",
    "#       0.004893380550270672,\n",
    "#       0.004888570441377456,\n",
    "#       0.004896123130504536,\n",
    "#       0.004889839375055185,\n",
    "#       0.004887012653222392,\n",
    "#       0.004880386079008586,\n",
    "#       0.0048818966359986055,\n",
    "#       0.004879853497661068,\n",
    "#       0.004881366218341794,\n",
    "#       0.004880037702422983,\n",
    "#       0.004887941412099846,\n",
    "#       0.004883872791058234,\n",
    "#       0.004896826932882415,\n",
    "#       0.004889501490171221,\n",
    "#       0.004883351483028708,\n",
    "#       0.004868795633908072,\n",
    "#       0.0048769255674081646,\n",
    "#       0.004868418822257594,\n",
    "#       0.004870702048337586,\n",
    "#       0.004888004476024041,\n",
    "#       0.00489404886745427,\n",
    "#       0.00488749046622241,\n",
    "#       0.00487471585021201,\n",
    "#       0.004871309945159125,\n",
    "#       0.004875296646393487,\n",
    "#       0.004882108194924558,\n",
    "#       0.004878622320760283,\n",
    "#       0.004886191952271577,\n",
    "#       0.0048895410930922464,\n",
    "#       0.004892483944867829,\n",
    "#       0.004889076266392416,\n",
    "#       0.0048812471908308685,\n",
    "#       0.00489042424817579,\n",
    "#       0.004885926320332927,\n",
    "#       0.0048637557837871385,\n",
    "#       0.00485535307169967,\n",
    "#       0.004857465922753184,\n",
    "#       0.004856510178728505,\n",
    "#       0.004842024520627854,\n",
    "#       0.004801526394358893,\n",
    "#       0.004821044230240375,\n",
    "#       0.0048074434790585365,\n",
    "#       0.0047911663910995906,\n",
    "#       0.004788553189330852,\n",
    "#       0.004789402852535681,\n",
    "#       0.004783143448259106,\n",
    "#       0.004782580733824886,\n",
    "#       0.004776058472532233,\n",
    "#       0.004783051050902064,\n",
    "#       0.004784158754334323,\n",
    "#       0.004787699621384938,\n",
    "#       0.004795832567656841,\n",
    "#       0.0048005546130309,\n",
    "#       0.00480140773822863,\n",
    "#       0.004808257055062732,\n",
    "#       0.0048075223051209995,\n",
    "#       0.004804600836190247,\n",
    "#       0.004803952453294952,\n",
    "#       0.004791162219167269,\n",
    "#       0.0047917198589807605,\n",
    "#       0.004807586401894821,\n",
    "#       0.004798405390632571,\n",
    "#       0.004813757992383091,\n",
    "#       0.004813972901690565,\n",
    "#       0.004826802097012096,\n",
    "#       0.004850301010019564,\n",
    "#       0.004873816997097298,\n",
    "#       0.004903121401363325,\n",
    "#       0.004932933857264011,\n",
    "#       0.004948061759955129,\n",
    "#       0.00497437550460425,\n",
    "#       0.005004371103324039,\n",
    "#       0.005018235476744154,\n",
    "#       0.005057373761142347,\n",
    "#       0.005093437817287725,\n",
    "#       0.005127915626556267,\n",
    "#       0.005162353905910569\n",
    "#     ],\n",
    "#     \"test-Logloss-mean\": [\n",
    "#       0.6876810214525293,\n",
    "#       0.6822861386880272,\n",
    "#       0.6769157213879007,\n",
    "#       0.6713495841493218,\n",
    "#       0.6660173140905826,\n",
    "#       0.6606400291802201,\n",
    "#       0.6556560361468958,\n",
    "#       0.6504071702380232,\n",
    "#       0.6452692754910266,\n",
    "#       0.6401614463758458,\n",
    "#       0.6349467411147574,\n",
    "#       0.6298591993115307,\n",
    "#       0.6253663121014329,\n",
    "#       0.6205969037034132,\n",
    "#       0.6159492701848851,\n",
    "#       0.6110814613349371,\n",
    "#       0.6064974931734515,\n",
    "#       0.6020725964217772,\n",
    "#       0.597542721865056,\n",
    "#       0.5930526553434579,\n",
    "#       0.58860208787221,\n",
    "#       0.5842639659709119,\n",
    "#       0.5797967744774606,\n",
    "#       0.5755637648202008,\n",
    "#       0.5713308434132189,\n",
    "#       0.5671192195522883,\n",
    "#       0.5634656285654946,\n",
    "#       0.5594588331968623,\n",
    "#       0.5555066116343879,\n",
    "#       0.5516056001608665,\n",
    "#       0.5478940804895245,\n",
    "#       0.5442079085879044,\n",
    "#       0.5407280726530503,\n",
    "#       0.5370105912900861,\n",
    "#       0.5331363636356798,\n",
    "#       0.5292797499204395,\n",
    "#       0.5259456034362988,\n",
    "#       0.5225304621292655,\n",
    "#       0.5192633029227905,\n",
    "#       0.5158056025967366,\n",
    "#       0.5123364093209003,\n",
    "#       0.5090499976786227,\n",
    "#       0.5058129952174129,\n",
    "#       0.5027282106050008,\n",
    "#       0.49949954658243517,\n",
    "#       0.4964998960570822,\n",
    "#       0.49355413763366984,\n",
    "#       0.4905325080932824,\n",
    "#       0.4874941479891106,\n",
    "#       0.4847155026276916,\n",
    "#       0.4816461318952193,\n",
    "#       0.47920846334951017,\n",
    "#       0.47631195106366153,\n",
    "#       0.47335652823323693,\n",
    "#       0.4704960470221903,\n",
    "#       0.46800532177948606,\n",
    "#       0.46527481875572885,\n",
    "#       0.4630584976839492,\n",
    "#       0.4603711547833377,\n",
    "#       0.4577813807063985,\n",
    "#       0.4552943856590462,\n",
    "#       0.452865725464285,\n",
    "#       0.4505410720547668,\n",
    "#       0.4480699813190146,\n",
    "#       0.4455882844773707,\n",
    "#       0.44339242113843663,\n",
    "#       0.4409428633996096,\n",
    "#       0.43864527362182304,\n",
    "#       0.43638638862556683,\n",
    "#       0.4341996812912482,\n",
    "#       0.4321281110432077,\n",
    "#       0.4300467797368408,\n",
    "#       0.42785829984005436,\n",
    "#       0.42568579633308523,\n",
    "#       0.4237502960750744,\n",
    "#       0.42128661877664814,\n",
    "#       0.41912556508694737,\n",
    "#       0.4174913861179844,\n",
    "#       0.4154859355620119,\n",
    "#       0.41343618460740483,\n",
    "#       0.4116449232509717,\n",
    "#       0.409711127555879,\n",
    "#       0.4076688185482144,\n",
    "#       0.40590332329325773,\n",
    "#       0.40384469076738655,\n",
    "#       0.401943147980696,\n",
    "#       0.4001348542320956,\n",
    "#       0.39852758042185066,\n",
    "#       0.3968362923382646,\n",
    "#       0.395166057398683,\n",
    "#       0.39380094365094387,\n",
    "#       0.39232345688918907,\n",
    "#       0.39100271373208617,\n",
    "#       0.3892928496582358,\n",
    "#       0.3878205244144665,\n",
    "#       0.3863418593205759,\n",
    "#       0.38462368795202184,\n",
    "#       0.38311555848693646,\n",
    "#       0.38160836140048654,\n",
    "#       0.38025083660833114,\n",
    "#       0.37870411624952305,\n",
    "#       0.3773372063224733,\n",
    "#       0.3756151974409791,\n",
    "#       0.3741119660361064,\n",
    "#       0.3727022854050574,\n",
    "#       0.3716353250989385,\n",
    "#       0.37026799170797414,\n",
    "#       0.36897982632374476,\n",
    "#       0.3675639839915842,\n",
    "#       0.3662205386384957,\n",
    "#       0.3650516319234465,\n",
    "#       0.3638314965473862,\n",
    "#       0.36254269268985306,\n",
    "#       0.3613317944651314,\n",
    "#       0.36007669108450846,\n",
    "#       0.3589403666860663,\n",
    "#       0.3576431875916497,\n",
    "#       0.3565488828935034,\n",
    "#       0.3555886865814387,\n",
    "#       0.35444852832375806,\n",
    "#       0.3532515323035365,\n",
    "#       0.3521014827207126,\n",
    "#       0.3509872850787382,\n",
    "#       0.3499315240231667,\n",
    "#       0.34874983849200825,\n",
    "#       0.3477052671403368,\n",
    "#       0.34663633399473204,\n",
    "#       0.3453511209111187,\n",
    "#       0.34423805434296684,\n",
    "#       0.3434381132149194,\n",
    "#       0.3425000460732324,\n",
    "#       0.3414006521045253,\n",
    "#       0.340548955460674,\n",
    "#       0.3394854539010103,\n",
    "#       0.3386490016706311,\n",
    "#       0.33757307144905785,\n",
    "#       0.33653672047858674,\n",
    "#       0.33564436584995855,\n",
    "#       0.33478824518951245,\n",
    "#       0.33398074548119555,\n",
    "#       0.3331306468423193,\n",
    "#       0.33220702942384317,\n",
    "#       0.3312794987023304,\n",
    "#       0.33031163191847646,\n",
    "#       0.3293228899852428,\n",
    "#       0.32849755829712884,\n",
    "#       0.3277371625220692,\n",
    "#       0.32692970569762536,\n",
    "#       0.3262213420613952,\n",
    "#       0.3255473613014067,\n",
    "#       0.32475592489764327,\n",
    "#       0.32394203738364036,\n",
    "#       0.32314103481015466,\n",
    "#       0.32226468375806633,\n",
    "#       0.32147387091628077,\n",
    "#       0.3207311728624658,\n",
    "#       0.3200394062927855,\n",
    "#       0.3192729103965582,\n",
    "#       0.31862082026554395,\n",
    "#       0.31787850952510666,\n",
    "#       0.31712720075141027,\n",
    "#       0.3163108634336658,\n",
    "#       0.31554118461526853,\n",
    "#       0.3147397817808687,\n",
    "#       0.3140033182374534,\n",
    "#       0.3133777112323954,\n",
    "#       0.31269356500401607,\n",
    "#       0.3121423677632246,\n",
    "#       0.3115247137064525,\n",
    "#       0.31099098054977126,\n",
    "#       0.310460690041389,\n",
    "#       0.30981307064463615,\n",
    "#       0.3091205989481512,\n",
    "#       0.3085089940975968,\n",
    "#       0.30785684382136363,\n",
    "#       0.30726758292772055,\n",
    "#       0.3066174382844878,\n",
    "#       0.30598769680627647,\n",
    "#       0.3053333867209672,\n",
    "#       0.3048222923234678,\n",
    "#       0.30427000943545646,\n",
    "#       0.3037089853066165,\n",
    "#       0.30325673667988357,\n",
    "#       0.30253168482987525,\n",
    "#       0.3020161388528604,\n",
    "#       0.301496977230089,\n",
    "#       0.3009502147739722,\n",
    "#       0.30046675320975236,\n",
    "#       0.3001243183937933,\n",
    "#       0.299645552285597,\n",
    "#       0.29913231882964075,\n",
    "#       0.2985582481587478,\n",
    "#       0.2979667702114305,\n",
    "#       0.2974486083005806,\n",
    "#       0.29700552623658333,\n",
    "#       0.29661982674238024,\n",
    "#       0.296139014889327,\n",
    "#       0.2956190795873478,\n",
    "#       0.2951503221460962,\n",
    "#       0.29469142504980333,\n",
    "#       0.2941150349423925,\n",
    "#       0.2936357417287142,\n",
    "#       0.29309800967236865,\n",
    "#       0.2926753592604655,\n",
    "#       0.29232475087724696,\n",
    "#       0.29191980795988204,\n",
    "#       0.29156146203696237,\n",
    "#       0.2909825604993494,\n",
    "#       0.29051801854010545,\n",
    "#       0.29010705013848526,\n",
    "#       0.28962221116804254,\n",
    "#       0.28915057794149646,\n",
    "#       0.2886858613229946,\n",
    "#       0.28827210735655345,\n",
    "#       0.2879048367519734,\n",
    "#       0.2875426728642402,\n",
    "#       0.28713185235351335,\n",
    "#       0.286777623715753,\n",
    "#       0.2863417780164398,\n",
    "#       0.2859849978152655,\n",
    "#       0.285536497627531,\n",
    "#       0.2852250016653505,\n",
    "#       0.28485876587028786,\n",
    "#       0.28445382887402804,\n",
    "#       0.28403884419033504,\n",
    "#       0.2835792223232755,\n",
    "#       0.28330557990852445,\n",
    "#       0.2829082693692621,\n",
    "#       0.2824558223404216,\n",
    "#       0.28203287270447214,\n",
    "#       0.2817131936857617,\n",
    "#       0.2813050116882388,\n",
    "#       0.2809124960503224,\n",
    "#       0.2805747298431246,\n",
    "#       0.28023197473104977,\n",
    "#       0.2798115081906646,\n",
    "#       0.2793854878343614,\n",
    "#       0.27905259134098087,\n",
    "#       0.27870947746792896,\n",
    "#       0.2784634858518613,\n",
    "#       0.2780642331207026,\n",
    "#       0.27762171638259203,\n",
    "#       0.277326447400824,\n",
    "#       0.27707094518046066,\n",
    "#       0.2767191322358713,\n",
    "#       0.2763728845170455,\n",
    "#       0.27611027908322605,\n",
    "#       0.2758070453080199,\n",
    "#       0.27555016636462343,\n",
    "#       0.2751559415975831,\n",
    "#       0.2748650613665241,\n",
    "#       0.27455182149390334,\n",
    "#       0.27419925218497276,\n",
    "#       0.2739335321673952,\n",
    "#       0.27360713169766726,\n",
    "#       0.27325918997176657,\n",
    "#       0.2729150986296737,\n",
    "#       0.27267838854896376,\n",
    "#       0.2724052155473271,\n",
    "#       0.272174202586136,\n",
    "#       0.27188928904335513,\n",
    "#       0.27166804912492193,\n",
    "#       0.27138660243533097,\n",
    "#       0.27115369470854384,\n",
    "#       0.2709179348819205,\n",
    "#       0.27071372942168204,\n",
    "#       0.27045209520853336,\n",
    "#       0.2702203875518134,\n",
    "#       0.27000711745908984,\n",
    "#       0.2697379024308626,\n",
    "#       0.26939696069478314,\n",
    "#       0.26920551762014566,\n",
    "#       0.2690212458222447,\n",
    "#       0.268779949187775,\n",
    "#       0.26860244940096345,\n",
    "#       0.26838873900121774,\n",
    "#       0.2681671053979115,\n",
    "#       0.2679669189193956,\n",
    "#       0.26782152881269516,\n",
    "#       0.2675899736983022,\n",
    "#       0.26733565026105055,\n",
    "#       0.26706944135737387,\n",
    "#       0.26686829422683495,\n",
    "#       0.26664211978758634,\n",
    "#       0.2664097520556104,\n",
    "#       0.2661612991797072,\n",
    "#       0.26592778769376063,\n",
    "#       0.2656901571232768,\n",
    "#       0.26550942227774144,\n",
    "#       0.26527600161483117,\n",
    "#       0.26505539165628855,\n",
    "#       0.26486230367870717,\n",
    "#       0.2645800335147699,\n",
    "#       0.264369056846541,\n",
    "#       0.26413013263399165,\n",
    "#       0.26392727373336167,\n",
    "#       0.26369259469277684,\n",
    "#       0.2634733805228529,\n",
    "#       0.2632980589137978,\n",
    "#       0.2631423506933592,\n",
    "#       0.26288727967864534,\n",
    "#       0.26268429075352084,\n",
    "#       0.2624962090609418,\n",
    "#       0.2622221505584482,\n",
    "#       0.2620569954458811,\n",
    "#       0.26190464839772515,\n",
    "#       0.2616501491185742,\n",
    "#       0.26145035640249603,\n",
    "#       0.26122663172546157,\n",
    "#       0.26108069908368614,\n",
    "#       0.2609420714226423,\n",
    "#       0.260801995649805,\n",
    "#       0.260623812097839,\n",
    "#       0.2603990777005327,\n",
    "#       0.26025369223988204,\n",
    "#       0.2601242564818679,\n",
    "#       0.2599419389090011,\n",
    "#       0.25978019669147906,\n",
    "#       0.25965618561038395,\n",
    "#       0.2594941802780394,\n",
    "#       0.25931016028781645,\n",
    "#       0.25918447160030683,\n",
    "#       0.25899880656027063,\n",
    "#       0.25881337098384194,\n",
    "#       0.25867124216871057,\n",
    "#       0.2585005403990157,\n",
    "#       0.2583336064035328,\n",
    "#       0.25816044667116433,\n",
    "#       0.25801090249867686,\n",
    "#       0.25787466701642636,\n",
    "#       0.25769852227368,\n",
    "#       0.25757297598269036,\n",
    "#       0.2573882732496028,\n",
    "#       0.2572493052232813,\n",
    "#       0.25710784944117215,\n",
    "#       0.25697514949606726,\n",
    "#       0.2568486771768189,\n",
    "#       0.25665830834548614,\n",
    "#       0.25648747846268516,\n",
    "#       0.2563919023309517,\n",
    "#       0.256287117247869,\n",
    "#       0.2561700492911908,\n",
    "#       0.2560047967959951,\n",
    "#       0.255894410951779,\n",
    "#       0.25577326327882155,\n",
    "#       0.25564775070244405,\n",
    "#       0.25552457163659203,\n",
    "#       0.2553677421263744,\n",
    "#       0.2552535777245346,\n",
    "#       0.2551489454560298,\n",
    "#       0.25502071993785586,\n",
    "#       0.2548901232870312,\n",
    "#       0.2548056063986871,\n",
    "#       0.25467702816062043,\n",
    "#       0.254569226458977,\n",
    "#       0.25443132728158196,\n",
    "#       0.25431362167224025,\n",
    "#       0.2542222919059217,\n",
    "#       0.2540673275215455,\n",
    "#       0.2538987743387832,\n",
    "#       0.2537974598562117,\n",
    "#       0.253697229105727,\n",
    "#       0.2535413574869921,\n",
    "#       0.25344655823136153,\n",
    "#       0.25332804759444894,\n",
    "#       0.25324683221638267,\n",
    "#       0.2531498863068198,\n",
    "#       0.25306444820736623,\n",
    "#       0.25296026254292503,\n",
    "#       0.2528491471330824,\n",
    "#       0.25273614541453376,\n",
    "#       0.2526465355163766,\n",
    "#       0.252534431669141,\n",
    "#       0.25245814484005213,\n",
    "#       0.2523306847953664,\n",
    "#       0.2522204658813469,\n",
    "#       0.25207605606127315,\n",
    "#       0.2519956365873715,\n",
    "#       0.2518836891306767,\n",
    "#       0.2517426595943986,\n",
    "#       0.2516640844975354,\n",
    "#       0.251586665494848,\n",
    "#       0.25148901559466375,\n",
    "#       0.2513596546948463,\n",
    "#       0.2512828728050929,\n",
    "#       0.2511479801142506,\n",
    "#       0.2510341049939483,\n",
    "#       0.25093108453559543,\n",
    "#       0.25081561849173806,\n",
    "#       0.2507256803017646,\n",
    "#       0.2506431467056104,\n",
    "#       0.2505393238616495,\n",
    "#       0.25045680888087896,\n",
    "#       0.2503772173532108,\n",
    "#       0.25029316257230133,\n",
    "#       0.2502042612418769,\n",
    "#       0.250154263683353,\n",
    "#       0.25002890354175367,\n",
    "#       0.24993570598073572,\n",
    "#       0.2498782210409792,\n",
    "#       0.2498025589937886,\n",
    "#       0.24972474792610674,\n",
    "#       0.24964497598748397,\n",
    "#       0.24956930324566845,\n",
    "#       0.24949633104691388,\n",
    "#       0.24944296738566138,\n",
    "#       0.2493653790644071,\n",
    "#       0.2493258781866281,\n",
    "#       0.2492633351569124,\n",
    "#       0.2491935981612999,\n",
    "#       0.2490999042241726,\n",
    "#       0.24905095990212853,\n",
    "#       0.24897632379650622,\n",
    "#       0.24888639328615078,\n",
    "#       0.24881925828327095,\n",
    "#       0.24873750813024625,\n",
    "#       0.24864274659123148,\n",
    "#       0.24857273846500347,\n",
    "#       0.24850343192332044,\n",
    "#       0.24844661820082165,\n",
    "#       0.24835398758345203,\n",
    "#       0.2482705887036536,\n",
    "#       0.24820131296703202,\n",
    "#       0.24813305537248823,\n",
    "#       0.2480576592937826,\n",
    "#       0.2479970924497692,\n",
    "#       0.24793284216501354,\n",
    "#       0.24785480280334057,\n",
    "#       0.2477738222914449,\n",
    "#       0.24770109006412538,\n",
    "#       0.24762553872108573,\n",
    "#       0.24753034389104242,\n",
    "#       0.24746423924430272,\n",
    "#       0.24740530139832254,\n",
    "#       0.2473308735000038,\n",
    "#       0.24725210059488445,\n",
    "#       0.24721662359867,\n",
    "#       0.24717295793038793,\n",
    "#       0.2470845290672116,\n",
    "#       0.24704101040638998,\n",
    "#       0.24697736476883975,\n",
    "#       0.24693136682621505,\n",
    "#       0.24688806790645473,\n",
    "#       0.24682456407305872,\n",
    "#       0.24676747083239597,\n",
    "#       0.24673445187536142,\n",
    "#       0.24666384360477817,\n",
    "#       0.24663338939510243,\n",
    "#       0.24656113077364744,\n",
    "#       0.24650279906852343,\n",
    "#       0.24646675763489245,\n",
    "#       0.2464052029228733,\n",
    "#       0.2463204923965515,\n",
    "#       0.24627355051695674,\n",
    "#       0.24621873272627676,\n",
    "#       0.24615760423807548,\n",
    "#       0.24612416750385488,\n",
    "#       0.2460612605105586,\n",
    "#       0.24600876249114978,\n",
    "#       0.2459533874296131,\n",
    "#       0.24589720319783334,\n",
    "#       0.24582535354798898,\n",
    "#       0.24574726587230272,\n",
    "#       0.24571183409736735,\n",
    "#       0.2456730460949589,\n",
    "#       0.24563469625832995,\n",
    "#       0.2455902243835853,\n",
    "#       0.24555076718067287,\n",
    "#       0.24550191115589856,\n",
    "#       0.24544533041408795,\n",
    "#       0.24538006416214797,\n",
    "#       0.24534602056104796,\n",
    "#       0.24530775669325883,\n",
    "#       0.2452733158048476,\n",
    "#       0.2452513220824491,\n",
    "#       0.2452329970694302,\n",
    "#       0.24520920249849168,\n",
    "#       0.245193161992679,\n",
    "#       0.2451643762661438,\n",
    "#       0.24512735310597217,\n",
    "#       0.24510591768385473,\n",
    "#       0.2450576779356103,\n",
    "#       0.24503259102395686,\n",
    "#       0.24500296485971065,\n",
    "#       0.24495854445251145,\n",
    "#       0.24493476958312982,\n",
    "#       0.2449092151492624,\n",
    "#       0.24490086003093828,\n",
    "#       0.2448855665718371,\n",
    "#       0.244860298479003,\n",
    "#       0.24484792013301285,\n",
    "#       0.24482340504341585,\n",
    "#       0.24481148619612964,\n",
    "#       0.24479133978337572,\n",
    "#       0.24477147990101508,\n",
    "#       0.2447446588341497,\n",
    "#       0.24471832888213552,\n",
    "#       0.24468897942332482,\n",
    "#       0.24465864366633383,\n",
    "#       0.24461775322828766\n",
    "#     ],\n",
    "#     \"test-Logloss-std\": [\n",
    "#       0.0004983387779248679,\n",
    "#       0.0007715977473926827,\n",
    "#       0.0007367464206433045,\n",
    "#       0.0009287442006785522,\n",
    "#       0.0009373359119867823,\n",
    "#       0.0010097258060540607,\n",
    "#       0.0012544629598154557,\n",
    "#       0.0010054143108844946,\n",
    "#       0.0009655805356101417,\n",
    "#       0.001144897522844898,\n",
    "#       0.0011335101061130865,\n",
    "#       0.0013899228952591446,\n",
    "#       0.0016381993910014278,\n",
    "#       0.0016608555924040663,\n",
    "#       0.0013459008005332214,\n",
    "#       0.0013650869057811267,\n",
    "#       0.0016124790773726888,\n",
    "#       0.001807951696453445,\n",
    "#       0.0017971717517213332,\n",
    "#       0.001761453114112091,\n",
    "#       0.0016031182380247392,\n",
    "#       0.0016722773215915676,\n",
    "#       0.001655989854284544,\n",
    "#       0.0015984038268866806,\n",
    "#       0.0015376255769262957,\n",
    "#       0.001429062299448689,\n",
    "#       0.0015005789027328353,\n",
    "#       0.001600624438029464,\n",
    "#       0.001808485346289139,\n",
    "#       0.001788082777560227,\n",
    "#       0.002038503463070886,\n",
    "#       0.001934119037087279,\n",
    "#       0.0021421938694883833,\n",
    "#       0.0024313843501466456,\n",
    "#       0.002288756100974861,\n",
    "#       0.0022523690605933205,\n",
    "#       0.002308324392565035,\n",
    "#       0.0023740522413147527,\n",
    "#       0.0025346715781628,\n",
    "#       0.0024641126717856055,\n",
    "#       0.002571726121776603,\n",
    "#       0.0027933443169314183,\n",
    "#       0.00282285192657496,\n",
    "#       0.0028794752980649597,\n",
    "#       0.0030165908805518085,\n",
    "#       0.0029242218856971166,\n",
    "#       0.0027712131711994216,\n",
    "#       0.0026250434981644104,\n",
    "#       0.00251835011053601,\n",
    "#       0.00259129000102891,\n",
    "#       0.0028616337313947023,\n",
    "#       0.002867241627143211,\n",
    "#       0.002903176386953762,\n",
    "#       0.002667899946421295,\n",
    "#       0.0027281370049598765,\n",
    "#       0.0026530999452958537,\n",
    "#       0.0029011637091533797,\n",
    "#       0.002985048718833112,\n",
    "#       0.0027810284352976417,\n",
    "#       0.002552944852438746,\n",
    "#       0.0025995501519680154,\n",
    "#       0.003000081492139502,\n",
    "#       0.002918651067100478,\n",
    "#       0.0028795809893608776,\n",
    "#       0.0026718310086310494,\n",
    "#       0.002698820511310341,\n",
    "#       0.002807996796040899,\n",
    "#       0.00269724273469523,\n",
    "#       0.0027079762339658545,\n",
    "#       0.0026412600925781204,\n",
    "#       0.002386854570537846,\n",
    "#       0.0023869542030812633,\n",
    "#       0.002486266059752034,\n",
    "#       0.002655603666758132,\n",
    "#       0.002769341276173634,\n",
    "#       0.002723194637108937,\n",
    "#       0.002660424225183697,\n",
    "#       0.002721335777388994,\n",
    "#       0.0027268504158462358,\n",
    "#       0.002572877267794756,\n",
    "#       0.002624984331269453,\n",
    "#       0.002792230517290087,\n",
    "#       0.002963811845911185,\n",
    "#       0.0028749109706384846,\n",
    "#       0.0028957774062408166,\n",
    "#       0.003050047814755381,\n",
    "#       0.002906818890051579,\n",
    "#       0.0028925019414245724,\n",
    "#       0.0029775281374944244,\n",
    "#       0.0029265706131917143,\n",
    "#       0.002720526599138403,\n",
    "#       0.0025396999029402975,\n",
    "#       0.002710539552273737,\n",
    "#       0.002849443149167138,\n",
    "#       0.002675457401809907,\n",
    "#       0.0026971193160871786,\n",
    "#       0.0026635490205761726,\n",
    "#       0.002808345417806513,\n",
    "#       0.0025631897581279233,\n",
    "#       0.002657590113835631,\n",
    "#       0.002793188062872107,\n",
    "#       0.0029983965290563464,\n",
    "#       0.002951039230787006,\n",
    "#       0.0028348425036236946,\n",
    "#       0.0025651961673497895,\n",
    "#       0.002397422734863124,\n",
    "#       0.0022496738814964607,\n",
    "#       0.0022709704625532906,\n",
    "#       0.0019990143406279394,\n",
    "#       0.0017134478418921084,\n",
    "#       0.0017397242724779569,\n",
    "#       0.0016933913682549413,\n",
    "#       0.0016769926940851754,\n",
    "#       0.001626126613180017,\n",
    "#       0.0014064744716558077,\n",
    "#       0.001269304733876087,\n",
    "#       0.0012144583606080285,\n",
    "#       0.0013138868518765856,\n",
    "#       0.0014657703649404896,\n",
    "#       0.0016638239216147251,\n",
    "#       0.0014240396263140427,\n",
    "#       0.0014915012990752259,\n",
    "#       0.0013869405597330788,\n",
    "#       0.0016676330062553807,\n",
    "#       0.001590797639425924,\n",
    "#       0.0017837465593422387,\n",
    "#       0.0018238081812428188,\n",
    "#       0.001824605179387843,\n",
    "#       0.00203534829162053,\n",
    "#       0.002135348926939503,\n",
    "#       0.002209796108748583,\n",
    "#       0.0023187321851772897,\n",
    "#       0.0024438993118335558,\n",
    "#       0.0024315551891631562,\n",
    "#       0.0026361831673554154,\n",
    "#       0.002485237684801482,\n",
    "#       0.00261826930630716,\n",
    "#       0.0024797571611254506,\n",
    "#       0.00264816564371458,\n",
    "#       0.002796958390961046,\n",
    "#       0.0025769432544999237,\n",
    "#       0.0025785856644526955,\n",
    "#       0.0025761277671958473,\n",
    "#       0.002622277765271899,\n",
    "#       0.002493546907897503,\n",
    "#       0.002525277062426751,\n",
    "#       0.002651863817192874,\n",
    "#       0.0027400456788535445,\n",
    "#       0.0025728983615768927,\n",
    "#       0.0027031461520197943,\n",
    "#       0.002662952175421146,\n",
    "#       0.002684219756175027,\n",
    "#       0.0027106719306520493,\n",
    "#       0.002601335988812761,\n",
    "#       0.002609576028797786,\n",
    "#       0.0024545899785038527,\n",
    "#       0.0024339134423856106,\n",
    "#       0.002539795016810407,\n",
    "#       0.0026945393755946467,\n",
    "#       0.0027007221925041238,\n",
    "#       0.0025332631614931,\n",
    "#       0.0023932155728308077,\n",
    "#       0.002471353333189655,\n",
    "#       0.0025939145548760805,\n",
    "#       0.0025820424899474443,\n",
    "#       0.002659627896491612,\n",
    "#       0.0027756462913562352,\n",
    "#       0.0027520000676391127,\n",
    "#       0.002661127033202417,\n",
    "#       0.002570553379039309,\n",
    "#       0.0025277947273853276,\n",
    "#       0.0024861098645782304,\n",
    "#       0.0025534884592636835,\n",
    "#       0.002441972534257786,\n",
    "#       0.0025619052085242627,\n",
    "#       0.0026153794662271174,\n",
    "#       0.002817786389915403,\n",
    "#       0.0027470909947407054,\n",
    "#       0.002753740149846874,\n",
    "#       0.002726744541177819,\n",
    "#       0.002751070117784998,\n",
    "#       0.002719892226320152,\n",
    "#       0.0028206963795754554,\n",
    "#       0.0027390801169816844,\n",
    "#       0.002783484659716396,\n",
    "#       0.002592204863188416,\n",
    "#       0.0025490825733260848,\n",
    "#       0.0025370660235790885,\n",
    "#       0.002516419303656581,\n",
    "#       0.0024583584235930646,\n",
    "#       0.002442696100913405,\n",
    "#       0.0024244416443350656,\n",
    "#       0.0024088225532989073,\n",
    "#       0.0024276836761266607,\n",
    "#       0.0024554029396679743,\n",
    "#       0.002491590151639426,\n",
    "#       0.002496842632135768,\n",
    "#       0.0025374661411988633,\n",
    "#       0.002589355389071241,\n",
    "#       0.00261190396185911,\n",
    "#       0.0028105382173014516,\n",
    "#       0.0027657286042482685,\n",
    "#       0.002780303878490604,\n",
    "#       0.002788711062519361,\n",
    "#       0.00287005112192936,\n",
    "#       0.0028302995622306077,\n",
    "#       0.002736620612743851,\n",
    "#       0.0025460561551395046,\n",
    "#       0.0025910603555631736,\n",
    "#       0.002614159817600299,\n",
    "#       0.0025153812879899467,\n",
    "#       0.0024407714308623913,\n",
    "#       0.002463558018908231,\n",
    "#       0.002391890814882526,\n",
    "#       0.0023710396050355543,\n",
    "#       0.0024211674689094066,\n",
    "#       0.0024737200666019752,\n",
    "#       0.002445551711180312,\n",
    "#       0.0023390925408537213,\n",
    "#       0.002272052279021273,\n",
    "#       0.002193450247652313,\n",
    "#       0.002334550600259852,\n",
    "#       0.0022523822420256545,\n",
    "#       0.0021988652138302273,\n",
    "#       0.002178020707195071,\n",
    "#       0.0022168935897471016,\n",
    "#       0.002190942831455881,\n",
    "#       0.0022908648959339127,\n",
    "#       0.002303632003405835,\n",
    "#       0.0023122105183031047,\n",
    "#       0.0024328857864504027,\n",
    "#       0.002369670145694105,\n",
    "#       0.0022701199730897075,\n",
    "#       0.0022968680345374277,\n",
    "#       0.0023310036365822018,\n",
    "#       0.002290995174113492,\n",
    "#       0.002152282790574177,\n",
    "#       0.002186590180233479,\n",
    "#       0.0021808629570173373,\n",
    "#       0.0022193109184995684,\n",
    "#       0.002242380806263212,\n",
    "#       0.0022263602261390638,\n",
    "#       0.0022410702615890243,\n",
    "#       0.00219720229244179,\n",
    "#       0.0022520951374349063,\n",
    "#       0.002161154180986586,\n",
    "#       0.002172113648307901,\n",
    "#       0.002114501045291871,\n",
    "#       0.0020864814083462737,\n",
    "#       0.0020965389910153178,\n",
    "#       0.002081832059761784,\n",
    "#       0.002072656041575505,\n",
    "#       0.0020422110498436764,\n",
    "#       0.002049623928999296,\n",
    "#       0.0020959640965374756,\n",
    "#       0.0021447802725461864,\n",
    "#       0.002079573339488367,\n",
    "#       0.002118305076630405,\n",
    "#       0.002016329506426886,\n",
    "#       0.0020083773274009588,\n",
    "#       0.001974317353343188,\n",
    "#       0.0019969445363405914,\n",
    "#       0.0019864947024232416,\n",
    "#       0.00200350265861682,\n",
    "#       0.002030184033527476,\n",
    "#       0.001996256262943958,\n",
    "#       0.0019774031578589086,\n",
    "#       0.0020720467097592155,\n",
    "#       0.00215549190344446,\n",
    "#       0.0022086764696050876,\n",
    "#       0.002206119993132783,\n",
    "#       0.002130577340310333,\n",
    "#       0.0021699197642183476,\n",
    "#       0.002100504600192462,\n",
    "#       0.0020411771562387007,\n",
    "#       0.0020599537974824666,\n",
    "#       0.0020661745592419744,\n",
    "#       0.001996111207087068,\n",
    "#       0.001960105482865106,\n",
    "#       0.0018779315744195484,\n",
    "#       0.0018678315183644734,\n",
    "#       0.0018568414526653973,\n",
    "#       0.0018654119733663044,\n",
    "#       0.001981638760949956,\n",
    "#       0.0019504532087376465,\n",
    "#       0.0019433847092488784,\n",
    "#       0.0018746638188464227,\n",
    "#       0.0018297171275687126,\n",
    "#       0.001915761279820552,\n",
    "#       0.0018402351941775653,\n",
    "#       0.0018145881890083142,\n",
    "#       0.001777557199815304,\n",
    "#       0.0017885245871505622,\n",
    "#       0.0017644176115863748,\n",
    "#       0.0017376871253270345,\n",
    "#       0.0016724140820741323,\n",
    "#       0.001604571963666238,\n",
    "#       0.001514482565257013,\n",
    "#       0.0014598835636660007,\n",
    "#       0.001437115028269605,\n",
    "#       0.0014370339043267369,\n",
    "#       0.0014152229196424512,\n",
    "#       0.0014191890773812236,\n",
    "#       0.0014661941239685907,\n",
    "#       0.0014297704569803977,\n",
    "#       0.0014047515288347992,\n",
    "#       0.0013727259730909691,\n",
    "#       0.0013775303014360377,\n",
    "#       0.0014034579158580928,\n",
    "#       0.0014139950613449132,\n",
    "#       0.0014495846775364747,\n",
    "#       0.0014040533692779972,\n",
    "#       0.001434502009834746,\n",
    "#       0.0014323430707967714,\n",
    "#       0.0014596592235908284,\n",
    "#       0.0013897893689796402,\n",
    "#       0.0013798824694742622,\n",
    "#       0.0014006014449692871,\n",
    "#       0.0014008657526381918,\n",
    "#       0.001410930838589801,\n",
    "#       0.0014175785544683068,\n",
    "#       0.0014396427498958985,\n",
    "#       0.001398604144080936,\n",
    "#       0.0013870964259920624,\n",
    "#       0.0013411860514087489,\n",
    "#       0.0013095597407602807,\n",
    "#       0.0012251029294708727,\n",
    "#       0.0011756047030520834,\n",
    "#       0.001132714963830837,\n",
    "#       0.0011298840521782188,\n",
    "#       0.0011185929985543254,\n",
    "#       0.0011353584032766275,\n",
    "#       0.0011363387196194007,\n",
    "#       0.0011078787706421114,\n",
    "#       0.001119659631463323,\n",
    "#       0.0011729414337229378,\n",
    "#       0.0011715858129315634,\n",
    "#       0.0011807133674187112,\n",
    "#       0.0011971882267528418,\n",
    "#       0.0012207823244666438,\n",
    "#       0.0012124278942740358,\n",
    "#       0.0011993472736408076,\n",
    "#       0.0011858976795142248,\n",
    "#       0.0011642395739177166,\n",
    "#       0.0011714880826544365,\n",
    "#       0.0012025046685728183,\n",
    "#       0.001165508756196279,\n",
    "#       0.0011887893737087994,\n",
    "#       0.0011899256623247871,\n",
    "#       0.001138836800627457,\n",
    "#       0.0011251824983272629,\n",
    "#       0.0010531498417866015,\n",
    "#       0.0010568176445999967,\n",
    "#       0.0010948969543415166,\n",
    "#       0.001075690396345307,\n",
    "#       0.0010144819356433796,\n",
    "#       0.000989167865127984,\n",
    "#       0.0009272840666049452,\n",
    "#       0.0009552675837195796,\n",
    "#       0.0009598771259353594,\n",
    "#       0.0010169183853880174,\n",
    "#       0.0010139747054235025,\n",
    "#       0.0010127197672781126,\n",
    "#       0.0010009443062951047,\n",
    "#       0.0009756454431078414,\n",
    "#       0.0009777552787764172,\n",
    "#       0.0010240948608163192,\n",
    "#       0.001043130091894749,\n",
    "#       0.0010483212578647385,\n",
    "#       0.001047421990293178,\n",
    "#       0.0010089732526770184,\n",
    "#       0.0009958576778326127,\n",
    "#       0.0010134749398388288,\n",
    "#       0.0010324781590001573,\n",
    "#       0.0010290013303294657,\n",
    "#       0.0010807920689686473,\n",
    "#       0.0010340434017746228,\n",
    "#       0.001024501120721884,\n",
    "#       0.001002324698927791,\n",
    "#       0.0009641626976897126,\n",
    "#       0.0009709810954348903,\n",
    "#       0.00097362344549333,\n",
    "#       0.0009739682147606378,\n",
    "#       0.0009772729462360467,\n",
    "#       0.000979242617279458,\n",
    "#       0.0010072157729742444,\n",
    "#       0.0009908245471037863,\n",
    "#       0.0009681270942134893,\n",
    "#       0.0009423780263472782,\n",
    "#       0.0008933341611352801,\n",
    "#       0.0009209971641684342,\n",
    "#       0.0008931212620170573,\n",
    "#       0.0008865878459694668,\n",
    "#       0.0008452646653359438,\n",
    "#       0.0008465886145704703,\n",
    "#       0.0008429507395915992,\n",
    "#       0.0008287674402417621,\n",
    "#       0.0008211782980534797,\n",
    "#       0.0008004438389313037,\n",
    "#       0.000832528016199712,\n",
    "#       0.0008245457121797128,\n",
    "#       0.0008552604654232204,\n",
    "#       0.0008234936389063107,\n",
    "#       0.0008323852161049593,\n",
    "#       0.0008341932266928441,\n",
    "#       0.0008323016628988645,\n",
    "#       0.0008302094831316222,\n",
    "#       0.0008201459420580958,\n",
    "#       0.0008000665005261114,\n",
    "#       0.0007944560171344628,\n",
    "#       0.000784745354395179,\n",
    "#       0.0007689982720884168,\n",
    "#       0.000794194618784869,\n",
    "#       0.0007686943331512921,\n",
    "#       0.0007625625445942038,\n",
    "#       0.0008165672391312678,\n",
    "#       0.0008122564127216362,\n",
    "#       0.0008144210220583096,\n",
    "#       0.0008121388938019025,\n",
    "#       0.0008025994083393927,\n",
    "#       0.0007764115646526168,\n",
    "#       0.0008029195395516139,\n",
    "#       0.0008328390500280285,\n",
    "#       0.0008589830776044451,\n",
    "#       0.0008790037590006972,\n",
    "#       0.0008693173046216283,\n",
    "#       0.0008829106176989782,\n",
    "#       0.0008899764460251681,\n",
    "#       0.0009047082479386359,\n",
    "#       0.0009142871250050923,\n",
    "#       0.0009534009714731326,\n",
    "#       0.000977217223822717,\n",
    "#       0.0009856399924265897,\n",
    "#       0.000990533722295678,\n",
    "#       0.0009854108520192053,\n",
    "#       0.000985086520022139,\n",
    "#       0.0009585474738384103,\n",
    "#       0.0009521530154125358,\n",
    "#       0.0009718225339033095,\n",
    "#       0.0009493587358990257,\n",
    "#       0.000951285647885124,\n",
    "#       0.0009593836942337297,\n",
    "#       0.0009708463235987417,\n",
    "#       0.0009788802388885844,\n",
    "#       0.0009644017335302265,\n",
    "#       0.0009703396254250479,\n",
    "#       0.0009404724295113345,\n",
    "#       0.0009484559572040672,\n",
    "#       0.0009937773233053458,\n",
    "#       0.000985400234536203,\n",
    "#       0.0009520318026530209,\n",
    "#       0.0009420713679337461,\n",
    "#       0.000933030320789777,\n",
    "#       0.0009350182785493423,\n",
    "#       0.0009495050277463861,\n",
    "#       0.000977562452397023,\n",
    "#       0.0009843157414568962,\n",
    "#       0.0009991464657176004,\n",
    "#       0.0010227449833801344,\n",
    "#       0.0010248193173362137,\n",
    "#       0.001027493066811346,\n",
    "#       0.0010134937548437496,\n",
    "#       0.0010176320663193965,\n",
    "#       0.0010185562431415907,\n",
    "#       0.0010117754476473432,\n",
    "#       0.001035313023799309,\n",
    "#       0.001043260121808761,\n",
    "#       0.00103470393148072,\n",
    "#       0.001013352739959674,\n",
    "#       0.0009811025622447412,\n",
    "#       0.0009837523776409847,\n",
    "#       0.0009537031307528574,\n",
    "#       0.000925477016980342,\n",
    "#       0.0009351236446151822,\n",
    "#       0.0009023132611426967,\n",
    "#       0.0008926969479662274,\n",
    "#       0.0008971603219671304,\n",
    "#       0.0008740486587777572,\n",
    "#       0.0008829085865545035,\n",
    "#       0.0009104753523810962,\n",
    "#       0.000921634668098484,\n",
    "#       0.0009292606391807723,\n",
    "#       0.0009123082811981418,\n",
    "#       0.0009308100979270455,\n",
    "#       0.0009407581060298769,\n",
    "#       0.0009178748330291359,\n",
    "#       0.0009110836194953056,\n",
    "#       0.0009098693120031885,\n",
    "#       0.0008985012604499615,\n",
    "#       0.0008735941677534738,\n",
    "#       0.000869356094838322,\n",
    "#       0.0008576972251351445,\n",
    "#       0.0008430610759085515,\n",
    "#       0.0008377555399874838,\n",
    "#       0.0008235097440149498,\n",
    "#       0.0007960904694933396,\n",
    "#       0.0007908413398466789,\n",
    "#       0.0007807024832596641,\n",
    "#       0.0007519708612569797,\n",
    "#       0.0007440145370019464\n",
    "#     ],\n",
    "#     \"train-Logloss-mean\": [\n",
    "#       0.6876617383166829,\n",
    "#       0.6822652140505454,\n",
    "#       0.6768684181944791,\n",
    "#       0.671289017458439,\n",
    "#       0.6659545371077268,\n",
    "#       0.6605555280993841,\n",
    "#       0.6555542354363846,\n",
    "#       0.6502683009117746,\n",
    "#       0.6451131460214119,\n",
    "#       0.6399858809049424,\n",
    "#       0.6347500434720378,\n",
    "#       0.6296255874314995,\n",
    "#       0.6251163459204471,\n",
    "#       0.6203222756537092,\n",
    "#       0.6156714572745261,\n",
    "#       0.6107792303389429,\n",
    "#       0.6061940316569362,\n",
    "#       0.6017547887625794,\n",
    "#       0.5972202185788213,\n",
    "#       0.5927067729745576,\n",
    "#       0.5882518130813539,\n",
    "#       0.5838958590248412,\n",
    "#       0.579396604740683,\n",
    "#       0.5751454918753913,\n",
    "#       0.57090003563916,\n",
    "#       0.5666631820966153,\n",
    "#       0.5627034809612438,\n",
    "#       0.5587727827420019,\n",
    "#       0.5547944164689113,\n",
    "#       0.5508733522640069,\n",
    "#       0.5471630426339706,\n",
    "#       0.5431541572298049,\n",
    "#       0.5396740592289088,\n",
    "#       0.5359385401791427,\n",
    "#       0.5320368809821544,\n",
    "#       0.5283029613190189,\n",
    "#       0.5246933851292191,\n",
    "#       0.5212858896839184,\n",
    "#       0.5180122213116153,\n",
    "#       0.5145368429206727,\n",
    "#       0.5110560035666597,\n",
    "#       0.5077696311007484,\n",
    "#       0.5045434577960499,\n",
    "#       0.5012005017342906,\n",
    "#       0.4979607952385557,\n",
    "#       0.4949538462516214,\n",
    "#       0.4919913109477956,\n",
    "#       0.48897471543165827,\n",
    "#       0.48591764012413385,\n",
    "#       0.48294661967027086,\n",
    "#       0.4798757270022064,\n",
    "#       0.47696457896142386,\n",
    "#       0.4740774288877434,\n",
    "#       0.4711228811527395,\n",
    "#       0.46825708779855413,\n",
    "#       0.4655340865142901,\n",
    "#       0.4628015078474862,\n",
    "#       0.46014277284076305,\n",
    "#       0.45745861307878305,\n",
    "#       0.45487974893419975,\n",
    "#       0.45216740391341237,\n",
    "#       0.4496250839376403,\n",
    "#       0.44708540582060496,\n",
    "#       0.44473050146583215,\n",
    "#       0.44225349833681477,\n",
    "#       0.4398386656392318,\n",
    "#       0.43721382645570256,\n",
    "#       0.4347404451385623,\n",
    "#       0.43250569902369157,\n",
    "#       0.4303336124552247,\n",
    "#       0.42807500427247136,\n",
    "#       0.42581679097602737,\n",
    "#       0.42363986616277244,\n",
    "#       0.4214375782441566,\n",
    "#       0.41913419521276907,\n",
    "#       0.4168372381427618,\n",
    "#       0.41473183075386666,\n",
    "#       0.41277189634415395,\n",
    "#       0.41064250339541014,\n",
    "#       0.4086183320711724,\n",
    "#       0.4066709288119065,\n",
    "#       0.40476618029708467,\n",
    "#       0.40273809383240255,\n",
    "#       0.4010002672402731,\n",
    "#       0.3989667258643574,\n",
    "#       0.39689747653786045,\n",
    "#       0.3949363285547257,\n",
    "#       0.3931556842105758,\n",
    "#       0.39132091950936526,\n",
    "#       0.3895056368739215,\n",
    "#       0.3876644270280827,\n",
    "#       0.38574020460300573,\n",
    "#       0.38399766256293355,\n",
    "#       0.3821354387464396,\n",
    "#       0.38037931520794394,\n",
    "#       0.3789466957444717,\n",
    "#       0.37729617427858003,\n",
    "#       0.3756694234281589,\n",
    "#       0.3740479393394126,\n",
    "#       0.3726120897191155,\n",
    "#       0.3710559394663443,\n",
    "#       0.3694804337959366,\n",
    "#       0.36778593852761987,\n",
    "#       0.36625310193895827,\n",
    "#       0.36458717565270493,\n",
    "#       0.3629954824603956,\n",
    "#       0.36152345406134717,\n",
    "#       0.3599203602181301,\n",
    "#       0.3583004965083745,\n",
    "#       0.35688879005478685,\n",
    "#       0.35551188009101614,\n",
    "#       0.3541277036040952,\n",
    "#       0.3526655537629783,\n",
    "#       0.351372798352397,\n",
    "#       0.34991550585019104,\n",
    "#       0.348724820530598,\n",
    "#       0.34743121868037796,\n",
    "#       0.34613845489874284,\n",
    "#       0.3451047942018455,\n",
    "#       0.34378559863146363,\n",
    "#       0.3425391709099671,\n",
    "#       0.3414456890044675,\n",
    "#       0.34010532701724083,\n",
    "#       0.33898698480581446,\n",
    "#       0.3378721753448877,\n",
    "#       0.33677219415122317,\n",
    "#       0.3355469018173026,\n",
    "#       0.3343122073459971,\n",
    "#       0.3331618821768657,\n",
    "#       0.3320514932415604,\n",
    "#       0.3309857539746651,\n",
    "#       0.32984365062179266,\n",
    "#       0.32872363260759074,\n",
    "#       0.32771776533430474,\n",
    "#       0.32658473181942976,\n",
    "#       0.32550949021062375,\n",
    "#       0.3244078394518257,\n",
    "#       0.3232347045078813,\n",
    "#       0.3223368641554577,\n",
    "#       0.32130230079544986,\n",
    "#       0.3205773376633821,\n",
    "#       0.31967836819214257,\n",
    "#       0.31881209788576026,\n",
    "#       0.31799844007632644,\n",
    "#       0.3169720997519474,\n",
    "#       0.3159133638084331,\n",
    "#       0.3150782811564339,\n",
    "#       0.3141316883043242,\n",
    "#       0.3132256779463557,\n",
    "#       0.31234764337476983,\n",
    "#       0.31153608228905805,\n",
    "#       0.3106857630037229,\n",
    "#       0.30979613137295137,\n",
    "#       0.30893866563133454,\n",
    "#       0.30818763599503624,\n",
    "#       0.307305686972864,\n",
    "#       0.3066329612708299,\n",
    "#       0.30587028330801014,\n",
    "#       0.305140952583553,\n",
    "#       0.3043032520539143,\n",
    "#       0.3034727856555509,\n",
    "#       0.3027661714532215,\n",
    "#       0.3020572411122061,\n",
    "#       0.30128474781399966,\n",
    "#       0.3004816730718317,\n",
    "#       0.29977414336091934,\n",
    "#       0.2989347359266149,\n",
    "#       0.298295461822127,\n",
    "#       0.2977363405540638,\n",
    "#       0.29707708471490235,\n",
    "#       0.2964364762058608,\n",
    "#       0.29573014831871935,\n",
    "#       0.29501831705155024,\n",
    "#       0.29437044831926223,\n",
    "#       0.2936819964303865,\n",
    "#       0.29304408082039235,\n",
    "#       0.29231668169946595,\n",
    "#       0.29166370623051674,\n",
    "#       0.29106956264924655,\n",
    "#       0.2904486535063836,\n",
    "#       0.2898589646464538,\n",
    "#       0.28936555177955176,\n",
    "#       0.2887414886449511,\n",
    "#       0.28807500299204536,\n",
    "#       0.2875577019193435,\n",
    "#       0.28696507763342805,\n",
    "#       0.28640058019291176,\n",
    "#       0.2859097672690366,\n",
    "#       0.2854073563728865,\n",
    "#       0.2848554790910022,\n",
    "#       0.28428926994821996,\n",
    "#       0.283625688460185,\n",
    "#       0.2831354061847523,\n",
    "#       0.282573732311159,\n",
    "#       0.28201247401557783,\n",
    "#       0.2814736976168449,\n",
    "#       0.28094863369672635,\n",
    "#       0.2804707996454114,\n",
    "#       0.2799638083994336,\n",
    "#       0.2794516932862058,\n",
    "#       0.2788622919847451,\n",
    "#       0.27833452161952454,\n",
    "#       0.27777276573737686,\n",
    "#       0.27731259559529525,\n",
    "#       0.276872425730998,\n",
    "#       0.2764384460148417,\n",
    "#       0.27601531543170804,\n",
    "#       0.2755393717002601,\n",
    "#       0.27511674447462753,\n",
    "#       0.274651513383698,\n",
    "#       0.2742018631289282,\n",
    "#       0.2737105226379503,\n",
    "#       0.2732515194681124,\n",
    "#       0.27284593659451967,\n",
    "#       0.2724919017831495,\n",
    "#       0.2720955112500839,\n",
    "#       0.27166192987216536,\n",
    "#       0.2713386927999172,\n",
    "#       0.2709042727383809,\n",
    "#       0.27049878171426595,\n",
    "#       0.270060883696289,\n",
    "#       0.2697167217353472,\n",
    "#       0.26934609273843935,\n",
    "#       0.2690059688213113,\n",
    "#       0.26863930286551585,\n",
    "#       0.2682050977803314,\n",
    "#       0.26787053929217064,\n",
    "#       0.26747896748949496,\n",
    "#       0.2670812382902829,\n",
    "#       0.2666724737249522,\n",
    "#       0.26630015814593705,\n",
    "#       0.26590193710646914,\n",
    "#       0.2654306845686452,\n",
    "#       0.26509469056738166,\n",
    "#       0.26469312456325345,\n",
    "#       0.26434025630043595,\n",
    "#       0.2639715039159542,\n",
    "#       0.26361322983421653,\n",
    "#       0.2632626198917814,\n",
    "#       0.26302215304095655,\n",
    "#       0.2626890110759404,\n",
    "#       0.26228099478955424,\n",
    "#       0.261994304075721,\n",
    "#       0.261669110232155,\n",
    "#       0.2613439213682963,\n",
    "#       0.2610415006541101,\n",
    "#       0.26076726040647713,\n",
    "#       0.2604570040407209,\n",
    "#       0.2601503344020438,\n",
    "#       0.25978352666953575,\n",
    "#       0.25950104555266623,\n",
    "#       0.25919951694892607,\n",
    "#       0.2588548944660044,\n",
    "#       0.2585532065004226,\n",
    "#       0.2582898506306446,\n",
    "#       0.25792655125591263,\n",
    "#       0.2576341531167906,\n",
    "#       0.2573710765793261,\n",
    "#       0.2570600929368287,\n",
    "#       0.25681510464519014,\n",
    "#       0.2565840614098879,\n",
    "#       0.25631164214092356,\n",
    "#       0.2560465954264265,\n",
    "#       0.25576398150235663,\n",
    "#       0.25550960475208895,\n",
    "#       0.2552534467875304,\n",
    "#       0.2549687938863926,\n",
    "#       0.25469450369877056,\n",
    "#       0.25442038144921847,\n",
    "#       0.2541694867147758,\n",
    "#       0.2538799660776173,\n",
    "#       0.25363517304153843,\n",
    "#       0.25341651219925254,\n",
    "#       0.25319626101730586,\n",
    "#       0.2529268542124053,\n",
    "#       0.25268150524127003,\n",
    "#       0.25241917683417425,\n",
    "#       0.25218532031976193,\n",
    "#       0.25196980491449844,\n",
    "#       0.25173486182139826,\n",
    "#       0.2515320181812183,\n",
    "#       0.2512826572686283,\n",
    "#       0.2510762238177012,\n",
    "#       0.25082787850396265,\n",
    "#       0.2506304354015612,\n",
    "#       0.2503668865329769,\n",
    "#       0.2501766852782364,\n",
    "#       0.24996728105936747,\n",
    "#       0.24974545217751878,\n",
    "#       0.24952530200074768,\n",
    "#       0.24933825874983442,\n",
    "#       0.2491314278609306,\n",
    "#       0.24889126262112446,\n",
    "#       0.2486732881068904,\n",
    "#       0.24847338877764594,\n",
    "#       0.24824961285759387,\n",
    "#       0.24804370366189127,\n",
    "#       0.24779606965567585,\n",
    "#       0.24759877556938287,\n",
    "#       0.24742900919216684,\n",
    "#       0.2471819150169991,\n",
    "#       0.24697901667815145,\n",
    "#       0.2468197170251639,\n",
    "#       0.24657163573088242,\n",
    "#       0.24641786804765067,\n",
    "#       0.24626777805502442,\n",
    "#       0.24603109209192325,\n",
    "#       0.24584017528002122,\n",
    "#       0.2456147097240045,\n",
    "#       0.24546949432296702,\n",
    "#       0.2453243263878951,\n",
    "#       0.24518739250440996,\n",
    "#       0.24498355007826245,\n",
    "#       0.24479784609589195,\n",
    "#       0.24464106649224088,\n",
    "#       0.24445621201688245,\n",
    "#       0.24432117990918592,\n",
    "#       0.24414717082243215,\n",
    "#       0.2440029052005564,\n",
    "#       0.24383033702924176,\n",
    "#       0.24364291236775246,\n",
    "#       0.24352582221772695,\n",
    "#       0.24331474830341873,\n",
    "#       0.24314375368218077,\n",
    "#       0.2429718391797004,\n",
    "#       0.24279007895444119,\n",
    "#       0.24267066529157755,\n",
    "#       0.24251814259522025,\n",
    "#       0.24238802535316198,\n",
    "#       0.24225695776157563,\n",
    "#       0.24207822859003075,\n",
    "#       0.24196646747140207,\n",
    "#       0.24179769515611058,\n",
    "#       0.24166700742503525,\n",
    "#       0.24152820223991373,\n",
    "#       0.2413727047977478,\n",
    "#       0.24123062923109498,\n",
    "#       0.24107067257554787,\n",
    "#       0.24093767117505838,\n",
    "#       0.24078876318740047,\n",
    "#       0.24063809519565388,\n",
    "#       0.2405140772289865,\n",
    "#       0.24034792532295493,\n",
    "#       0.24021397373695957,\n",
    "#       0.2400710036917287,\n",
    "#       0.23991303632136854,\n",
    "#       0.2397805874583189,\n",
    "#       0.23962774192285025,\n",
    "#       0.23949194641296007,\n",
    "#       0.2393894859622657,\n",
    "#       0.23922207049222935,\n",
    "#       0.2390641026635482,\n",
    "#       0.2389359185505327,\n",
    "#       0.2388173356574949,\n",
    "#       0.23871392695383511,\n",
    "#       0.23858554095564863,\n",
    "#       0.23845808834813695,\n",
    "#       0.23835021812841126,\n",
    "#       0.23819561574233897,\n",
    "#       0.23804249524100296,\n",
    "#       0.23795499485869082,\n",
    "#       0.2378471511879631,\n",
    "#       0.23771669063667167,\n",
    "#       0.23758164429608,\n",
    "#       0.23748961775976998,\n",
    "#       0.23737749123951396,\n",
    "#       0.23725781515122982,\n",
    "#       0.23714672169211778,\n",
    "#       0.23702539897097333,\n",
    "#       0.23690141156199643,\n",
    "#       0.2367878146368961,\n",
    "#       0.2367011516868905,\n",
    "#       0.23658233329281736,\n",
    "#       0.23650341113683554,\n",
    "#       0.236359905778401,\n",
    "#       0.23625352544037917,\n",
    "#       0.23615477308712945,\n",
    "#       0.2360771558798062,\n",
    "#       0.23595794155174862,\n",
    "#       0.23585702094160746,\n",
    "#       0.23576719117366243,\n",
    "#       0.23564985850802755,\n",
    "#       0.23557270300332026,\n",
    "#       0.23545162592850044,\n",
    "#       0.23536021014981073,\n",
    "#       0.23523612893978493,\n",
    "#       0.23514267930955796,\n",
    "#       0.235062464295032,\n",
    "#       0.23497381044485083,\n",
    "#       0.23488339190673954,\n",
    "#       0.23477674685441574,\n",
    "#       0.23466513757133084,\n",
    "#       0.23456405671552819,\n",
    "#       0.23449738890184596,\n",
    "#       0.23439158934558485,\n",
    "#       0.2342846875247803,\n",
    "#       0.23421161244589728,\n",
    "#       0.23408976739513881,\n",
    "#       0.2340010541338339,\n",
    "#       0.23391026620960295,\n",
    "#       0.2338279304642064,\n",
    "#       0.2337241206408041,\n",
    "#       0.23362929590499473,\n",
    "#       0.23354743625650992,\n",
    "#       0.23346693405546798,\n",
    "#       0.23339078309235414,\n",
    "#       0.23331622834844046,\n",
    "#       0.2332580953534352,\n",
    "#       0.2331677314358634,\n",
    "#       0.2330979475943073,\n",
    "#       0.2329964744075539,\n",
    "#       0.23292910213599108,\n",
    "#       0.23284271403505774,\n",
    "#       0.232736323232406,\n",
    "#       0.23266363628665018,\n",
    "#       0.23258123764912045,\n",
    "#       0.23248524243120278,\n",
    "#       0.23240614208216615,\n",
    "#       0.23230291687620874,\n",
    "#       0.23223477155934727,\n",
    "#       0.23216416412794624,\n",
    "#       0.23206117837159157,\n",
    "#       0.23200068792983114,\n",
    "#       0.2319248143197586,\n",
    "#       0.23186720095581642,\n",
    "#       0.2317992782075601,\n",
    "#       0.23171463720789678,\n",
    "#       0.23163206781072615,\n",
    "#       0.23155286337964257,\n",
    "#       0.2314842050737492,\n",
    "#       0.2314336955976272,\n",
    "#       0.23134654049301603,\n",
    "#       0.23128107160136402,\n",
    "#       0.23120827096536906,\n",
    "#       0.2311314945915878,\n",
    "#       0.2310541008583445,\n",
    "#       0.23100981536558268,\n",
    "#       0.23094446606500219,\n",
    "#       0.23087536757864044,\n",
    "#       0.2308108888258257,\n",
    "#       0.2307436483790938,\n",
    "#       0.23068893835629267,\n",
    "#       0.23063861067734046,\n",
    "#       0.2305614823729162,\n",
    "#       0.23049361764439066,\n",
    "#       0.23044525408602587,\n",
    "#       0.2303831703311649,\n",
    "#       0.2303067596079919,\n",
    "#       0.23024678440395596,\n",
    "#       0.23019047050546063,\n",
    "#       0.23014392375305107,\n",
    "#       0.2300691195193289,\n",
    "#       0.22998022262975865,\n",
    "#       0.2299135343753695,\n",
    "#       0.2298556285388993,\n",
    "#       0.2298092811334569,\n",
    "#       0.22974019289477154,\n",
    "#       0.22966863626302284,\n",
    "#       0.22961232938483542,\n",
    "#       0.22953254774151183,\n",
    "#       0.22948664565717078,\n",
    "#       0.2294202264763289,\n",
    "#       0.22934303507961545,\n",
    "#       0.2293038383322948,\n",
    "#       0.22923720666733266,\n",
    "#       0.22918386202386035,\n",
    "#       0.2291477006333988,\n",
    "#       0.22907677311075741,\n",
    "#       0.2290201910077463,\n",
    "#       0.22895061704114056,\n",
    "#       0.228885572860494,\n",
    "#       0.2288359879216011,\n",
    "#       0.22877951556953785,\n",
    "#       0.22873175319503747,\n",
    "#       0.22868188350889115,\n",
    "#       0.228650099510077,\n",
    "#       0.2286054180553471,\n",
    "#       0.2285577082774327,\n",
    "#       0.22851136373167819,\n",
    "#       0.22845298205453896,\n",
    "#       0.2284220643647395,\n",
    "#       0.22837375408636013,\n",
    "#       0.22834784674665998,\n",
    "#       0.2282844491998059,\n",
    "#       0.2282413886726189,\n",
    "#       0.22820584450217898,\n",
    "#       0.22817198004904937,\n",
    "#       0.2281317534571321,\n",
    "#       0.22810729392340123,\n",
    "#       0.2280703836925741,\n",
    "#       0.22804666177539992,\n",
    "#       0.22801936843387893,\n",
    "#       0.2279907185281552,\n",
    "#       0.22796230192747657,\n",
    "#       0.22794742884656188,\n",
    "#       0.227912288693605,\n",
    "#       0.22788142393374367,\n",
    "#       0.2278513629100471,\n",
    "#       0.22781932805443966,\n",
    "#       0.22778641552016804\n",
    "#     ],\n",
    "#     \"train-Logloss-std\": [\n",
    "#       0.0004883453104895832,\n",
    "#       0.0007693167139056263,\n",
    "#       0.0007304965943699246,\n",
    "#       0.0009565237851187171,\n",
    "#       0.0009540618526433818,\n",
    "#       0.000996546349903537,\n",
    "#       0.0012882267626683776,\n",
    "#       0.0010146974092498644,\n",
    "#       0.0009205031176535161,\n",
    "#       0.0011144690819682621,\n",
    "#       0.0010659313828008036,\n",
    "#       0.0013454758314689307,\n",
    "#       0.0016069725779400035,\n",
    "#       0.00163940793406872,\n",
    "#       0.0013417455364092843,\n",
    "#       0.001353927488140434,\n",
    "#       0.0015919832183990611,\n",
    "#       0.001778275890825219,\n",
    "#       0.001744316455547446,\n",
    "#       0.0016739078861758277,\n",
    "#       0.0015212132494127394,\n",
    "#       0.0016066557479942267,\n",
    "#       0.0015396977826752753,\n",
    "#       0.0014758991717211338,\n",
    "#       0.0014244034179726043,\n",
    "#       0.001325108057474121,\n",
    "#       0.0016060386012091937,\n",
    "#       0.0016839262720580387,\n",
    "#       0.0019617538837946656,\n",
    "#       0.001969969050857795,\n",
    "#       0.0021827561034036378,\n",
    "#       0.0022473209463729016,\n",
    "#       0.002342371092572878,\n",
    "#       0.002641134062222222,\n",
    "#       0.0024242151334971235,\n",
    "#       0.0024312187671732956,\n",
    "#       0.0024741040479543223,\n",
    "#       0.002441308772583056,\n",
    "#       0.0025265980184684426,\n",
    "#       0.0025145249279106556,\n",
    "#       0.0024712184164099988,\n",
    "#       0.0026399823976974743,\n",
    "#       0.0027335075254287604,\n",
    "#       0.002684367968575521,\n",
    "#       0.0030125839496095014,\n",
    "#       0.0031137544548635932,\n",
    "#       0.003014197530668978,\n",
    "#       0.002870084259451351,\n",
    "#       0.002882771490246289,\n",
    "#       0.0031046298773610872,\n",
    "#       0.003463256765982263,\n",
    "#       0.003716909627794587,\n",
    "#       0.003737486482206845,\n",
    "#       0.0036250232105187684,\n",
    "#       0.0035626797693440937,\n",
    "#       0.003712772441746049,\n",
    "#       0.0038654393335838637,\n",
    "#       0.00409254960297054,\n",
    "#       0.003848787504349543,\n",
    "#       0.0037339904095756438,\n",
    "#       0.0038838308814454206,\n",
    "#       0.00402361991134482,\n",
    "#       0.003934310145734403,\n",
    "#       0.0038310940666530856,\n",
    "#       0.0036900833113643412,\n",
    "#       0.0037574074950372993,\n",
    "#       0.003720870490849822,\n",
    "#       0.0036918050379237876,\n",
    "#       0.0037306146453601496,\n",
    "#       0.00366291800917382,\n",
    "#       0.0036849813270605058,\n",
    "#       0.003819562568607346,\n",
    "#       0.00398664187898257,\n",
    "#       0.004143557553018322,\n",
    "#       0.0043431326816247015,\n",
    "#       0.004439498144850598,\n",
    "#       0.004517110812712233,\n",
    "#       0.004603698107337898,\n",
    "#       0.004672570889459181,\n",
    "#       0.004494954371427566,\n",
    "#       0.004671496824330111,\n",
    "#       0.004647551248022107,\n",
    "#       0.004680340182278443,\n",
    "#       0.004630548546297981,\n",
    "#       0.004666123651939151,\n",
    "#       0.004568899035183864,\n",
    "#       0.004700134415566136,\n",
    "#       0.004797243239606059,\n",
    "#       0.0048952874214774865,\n",
    "#       0.004893590033437091,\n",
    "#       0.004909090192849753,\n",
    "#       0.00497256885374757,\n",
    "#       0.005138530024361652,\n",
    "#       0.0053850193408259564,\n",
    "#       0.005629121048881568,\n",
    "#       0.005702630686131392,\n",
    "#       0.00564778318726519,\n",
    "#       0.005602061627553522,\n",
    "#       0.005508139342580278,\n",
    "#       0.005662434191160718,\n",
    "#       0.005653656932794178,\n",
    "#       0.005533321848077467,\n",
    "#       0.0055256545269915126,\n",
    "#       0.005355826973101359,\n",
    "#       0.005401956969211081,\n",
    "#       0.005363336256480288,\n",
    "#       0.005220401326513724,\n",
    "#       0.0050871158324254,\n",
    "#       0.004882011632409956,\n",
    "#       0.0049901286202288595,\n",
    "#       0.004927025936005199,\n",
    "#       0.005000024479991006,\n",
    "#       0.004861627200734921,\n",
    "#       0.004661249524847617,\n",
    "#       0.004689513954444935,\n",
    "#       0.004712895394464775,\n",
    "#       0.004726914902563847,\n",
    "#       0.004804116648770702,\n",
    "#       0.004848013745911275,\n",
    "#       0.004864253701716293,\n",
    "#       0.004868394969226944,\n",
    "#       0.004793780492857014,\n",
    "#       0.0048167141683337225,\n",
    "#       0.004745746157036125,\n",
    "#       0.004790204508454073,\n",
    "#       0.0047712866227787604,\n",
    "#       0.0046930253131251065,\n",
    "#       0.0047455465215998065,\n",
    "#       0.0047585316278833485,\n",
    "#       0.004736444399404218,\n",
    "#       0.004735941460906173,\n",
    "#       0.0047382377705994985,\n",
    "#       0.004678252828775719,\n",
    "#       0.004861277149822298,\n",
    "#       0.004804203132986529,\n",
    "#       0.004816438496911998,\n",
    "#       0.004811973288747067,\n",
    "#       0.004804231479488684,\n",
    "#       0.0048159574759196275,\n",
    "#       0.004845028690991361,\n",
    "#       0.0048593896358154285,\n",
    "#       0.004680746046608471,\n",
    "#       0.004623753328504983,\n",
    "#       0.004687489249235912,\n",
    "#       0.004771046385752931,\n",
    "#       0.0048965704249114015,\n",
    "#       0.004870624707700916,\n",
    "#       0.004744949282935303,\n",
    "#       0.004694917852121898,\n",
    "#       0.004617708439363949,\n",
    "#       0.004606093083007909,\n",
    "#       0.004681958716899866,\n",
    "#       0.004741273405826933,\n",
    "#       0.004748611517732207,\n",
    "#       0.004804099301902724,\n",
    "#       0.0048179005734205055,\n",
    "#       0.004854516900068298,\n",
    "#       0.004876345440403339,\n",
    "#       0.004864041566421128,\n",
    "#       0.004792312240087443,\n",
    "#       0.00465722588954174,\n",
    "#       0.0045953014015926935,\n",
    "#       0.004437951591512641,\n",
    "#       0.00431245954976231,\n",
    "#       0.004249539763445343,\n",
    "#       0.004332796933641194,\n",
    "#       0.004157008233668677,\n",
    "#       0.004187705505015383,\n",
    "#       0.004181123942289236,\n",
    "#       0.004220794749705315,\n",
    "#       0.004297383431618883,\n",
    "#       0.004279212755472178,\n",
    "#       0.004271100174625761,\n",
    "#       0.004250414085312138,\n",
    "#       0.004236462091752206,\n",
    "#       0.0043162227643771395,\n",
    "#       0.004193418084748973,\n",
    "#       0.004256032322855071,\n",
    "#       0.004299397395392902,\n",
    "#       0.004402425631522244,\n",
    "#       0.004519232238066087,\n",
    "#       0.004563107062234215,\n",
    "#       0.004421884224041924,\n",
    "#       0.004457991977846935,\n",
    "#       0.004292136521040571,\n",
    "#       0.004313023250874597,\n",
    "#       0.004234070009697931,\n",
    "#       0.004132557661664892,\n",
    "#       0.004097789198377956,\n",
    "#       0.004082793903403567,\n",
    "#       0.00397945782995328,\n",
    "#       0.0038762883338883504,\n",
    "#       0.003880789428829122,\n",
    "#       0.0038345999737453202,\n",
    "#       0.0038948295987701023,\n",
    "#       0.0038722579169077624,\n",
    "#       0.0037877209904968764,\n",
    "#       0.0038257641834860156,\n",
    "#       0.0037148727270765462,\n",
    "#       0.003625063181998942,\n",
    "#       0.0036093854120540347,\n",
    "#       0.003550101128188196,\n",
    "#       0.0035004645089897323,\n",
    "#       0.003545683466823631,\n",
    "#       0.0034314780744664005,\n",
    "#       0.003450832226990056,\n",
    "#       0.0035091100096358214,\n",
    "#       0.0035867246535024578,\n",
    "#       0.0034649955162625364,\n",
    "#       0.003422070632653723,\n",
    "#       0.0033917050361097017,\n",
    "#       0.0032806140007510106,\n",
    "#       0.003324453363908626,\n",
    "#       0.003275418195279042,\n",
    "#       0.0033355089101740716,\n",
    "#       0.0033944912026118296,\n",
    "#       0.003364566022791795,\n",
    "#       0.0033306665206370672,\n",
    "#       0.0033299847653952986,\n",
    "#       0.003280958422430769,\n",
    "#       0.003298172761799122,\n",
    "#       0.003261884682433159,\n",
    "#       0.003322295083871248,\n",
    "#       0.0032894094881772084,\n",
    "#       0.0033589162527377222,\n",
    "#       0.0032856520338519365,\n",
    "#       0.0032328028189929463,\n",
    "#       0.0032067229702473134,\n",
    "#       0.003140625340703871,\n",
    "#       0.0031389213355729768,\n",
    "#       0.0031381126572401987,\n",
    "#       0.003171577619705946,\n",
    "#       0.003157594539596667,\n",
    "#       0.0031667067533166383,\n",
    "#       0.003190796629022097,\n",
    "#       0.003235221860885041,\n",
    "#       0.003258927428152928,\n",
    "#       0.0032433751509088234,\n",
    "#       0.0033032951730937294,\n",
    "#       0.003285726651095135,\n",
    "#       0.0032132276777689928,\n",
    "#       0.0031377787296324165,\n",
    "#       0.0031523261764877324,\n",
    "#       0.0031772477564102042,\n",
    "#       0.003116487131132706,\n",
    "#       0.0031855329423017198,\n",
    "#       0.0031546687290133686,\n",
    "#       0.0031974873155238767,\n",
    "#       0.003227360518650757,\n",
    "#       0.003240720250939544,\n",
    "#       0.0031913918631711175,\n",
    "#       0.0031995026722319315,\n",
    "#       0.0032018030398319876,\n",
    "#       0.0031642774022485705,\n",
    "#       0.0030821120981781224,\n",
    "#       0.0030583935153954164,\n",
    "#       0.0030555385425602125,\n",
    "#       0.003030952548909212,\n",
    "#       0.0030664193368451376,\n",
    "#       0.0030916814026419872,\n",
    "#       0.0031239325964445143,\n",
    "#       0.0030855668775854853,\n",
    "#       0.0030575685139643018,\n",
    "#       0.0030452851132903094,\n",
    "#       0.002940646106028631,\n",
    "#       0.0029665649003131076,\n",
    "#       0.0029993796425893705,\n",
    "#       0.0029852168656448784,\n",
    "#       0.00290501975790979,\n",
    "#       0.002912226563130322,\n",
    "#       0.002855207251633297,\n",
    "#       0.002899148083760405,\n",
    "#       0.0028328108679946404,\n",
    "#       0.0027911026509706512,\n",
    "#       0.002842644226195943,\n",
    "#       0.0028120836170846333,\n",
    "#       0.002802127468160317,\n",
    "#       0.0027965813815470516,\n",
    "#       0.0027857262252348612,\n",
    "#       0.0027589382644845097,\n",
    "#       0.0027699448122485365,\n",
    "#       0.0027479065272149384,\n",
    "#       0.002792845728065126,\n",
    "#       0.0027426588953789253,\n",
    "#       0.002738783435967007,\n",
    "#       0.0027046937937327555,\n",
    "#       0.002703161299513918,\n",
    "#       0.002751979608557119,\n",
    "#       0.002709590148151373,\n",
    "#       0.0027012092261681627,\n",
    "#       0.0027092936449885086,\n",
    "#       0.002704704363355325,\n",
    "#       0.002688447113515275,\n",
    "#       0.002729937519871661,\n",
    "#       0.002690142519528373,\n",
    "#       0.002677169346569861,\n",
    "#       0.002697651612877089,\n",
    "#       0.0026633945791125547,\n",
    "#       0.002625636457236869,\n",
    "#       0.00261931715005845,\n",
    "#       0.002595944184808876,\n",
    "#       0.0025727733156874796,\n",
    "#       0.002587799830496776,\n",
    "#       0.002550606001806612,\n",
    "#       0.0025568568977066886,\n",
    "#       0.002559786091917233,\n",
    "#       0.0025134152305728505,\n",
    "#       0.002500550419781438,\n",
    "#       0.0024335052510139252,\n",
    "#       0.0024567394322734886,\n",
    "#       0.0024440177638467213,\n",
    "#       0.0024644387492200677,\n",
    "#       0.002446224404114844,\n",
    "#       0.002448847775951188,\n",
    "#       0.0024220910767572814,\n",
    "#       0.0024292080279432783,\n",
    "#       0.002424997669658061,\n",
    "#       0.0023587679319375904,\n",
    "#       0.002414542189739185,\n",
    "#       0.0024158993841763418,\n",
    "#       0.0024145628000104232,\n",
    "#       0.0023903276690041505,\n",
    "#       0.0023633740548945135,\n",
    "#       0.002341716574765295,\n",
    "#       0.0023749976988962245,\n",
    "#       0.002339146261668682,\n",
    "#       0.0023811855032688694,\n",
    "#       0.002394283617788589,\n",
    "#       0.002424676616133499,\n",
    "#       0.0024229409346352357,\n",
    "#       0.002375441517160558,\n",
    "#       0.002339146441598515,\n",
    "#       0.0023204941533381223,\n",
    "#       0.0023646457833091743,\n",
    "#       0.0023527099878618415,\n",
    "#       0.002303604986727268,\n",
    "#       0.0022774227166375417,\n",
    "#       0.0022846447282939323,\n",
    "#       0.0023146238410958254,\n",
    "#       0.0023208329947685406,\n",
    "#       0.0023315138576297744,\n",
    "#       0.002345759997660567,\n",
    "#       0.002367478495986776,\n",
    "#       0.0023880447829171827,\n",
    "#       0.0023664295838924025,\n",
    "#       0.0023481141128568754,\n",
    "#       0.0023567246925611223,\n",
    "#       0.0023188029407856113,\n",
    "#       0.0023026743968687035,\n",
    "#       0.00231547459561033,\n",
    "#       0.0022980885539909458,\n",
    "#       0.002297372453891252,\n",
    "#       0.0023022331967050566,\n",
    "#       0.0022899853288314695,\n",
    "#       0.002305787331970289,\n",
    "#       0.002320884000962144,\n",
    "#       0.002343838216059143,\n",
    "#       0.0023268951176975785,\n",
    "#       0.002326893445941241,\n",
    "#       0.002312627910673601,\n",
    "#       0.0023213903109791105,\n",
    "#       0.002317234974077094,\n",
    "#       0.0023197051646720694,\n",
    "#       0.0022900474334468885,\n",
    "#       0.002332738586782032,\n",
    "#       0.0023491525881280955,\n",
    "#       0.0023602566245477657,\n",
    "#       0.002344658150289062,\n",
    "#       0.002361503018606967,\n",
    "#       0.0023398896134318265,\n",
    "#       0.002339833543159497,\n",
    "#       0.0023456925976940877,\n",
    "#       0.0023394341423345452,\n",
    "#       0.002363480562391442,\n",
    "#       0.0023463609166105024,\n",
    "#       0.0023099962618961236,\n",
    "#       0.002320246579415577,\n",
    "#       0.002331628745544967,\n",
    "#       0.002327599870285434,\n",
    "#       0.0023713329031609847,\n",
    "#       0.0023584717449282736,\n",
    "#       0.002360521920041475,\n",
    "#       0.002337981544941982,\n",
    "#       0.0022881596450403307,\n",
    "#       0.00228215803971562,\n",
    "#       0.0022600810437853436,\n",
    "#       0.002259650713791832,\n",
    "#       0.0022497989056611645,\n",
    "#       0.0022619259467340723,\n",
    "#       0.0022663346138590795,\n",
    "#       0.0022425037206486964,\n",
    "#       0.0022537770260053812,\n",
    "#       0.0022702896043013377,\n",
    "#       0.002265240857947706,\n",
    "#       0.0022508262806048966,\n",
    "#       0.0022254415278452236,\n",
    "#       0.0022398848230060662,\n",
    "#       0.002203060986531641,\n",
    "#       0.0021935504734040203,\n",
    "#       0.0021602651969419044,\n",
    "#       0.0021823515890913637,\n",
    "#       0.0021745162228869516,\n",
    "#       0.002154591978306848,\n",
    "#       0.002139303812550624,\n",
    "#       0.00215722444394448,\n",
    "#       0.0021504771115124703,\n",
    "#       0.0021778912913434163,\n",
    "#       0.0021775009414405798,\n",
    "#       0.0021892163556301325,\n",
    "#       0.0021960882278940297,\n",
    "#       0.0021911210745269903,\n",
    "#       0.002180632499138633,\n",
    "#       0.002183649636803341,\n",
    "#       0.0021700844120378852,\n",
    "#       0.002170442545465955,\n",
    "#       0.0021858700671492258,\n",
    "#       0.002205013987018423,\n",
    "#       0.0022283312833280312,\n",
    "#       0.0022046773435767083,\n",
    "#       0.0022017261760583157,\n",
    "#       0.0022002555969607577,\n",
    "#       0.0021806537417054113,\n",
    "#       0.0021806525406375074,\n",
    "#       0.002185374630719557,\n",
    "#       0.0021739752334103656,\n",
    "#       0.002183864535615737,\n",
    "#       0.0021835658724921737,\n",
    "#       0.002165002407776566,\n",
    "#       0.002150540710541321,\n",
    "#       0.0021319737330016213,\n",
    "#       0.0021112888435244883,\n",
    "#       0.0020992639005091985,\n",
    "#       0.002107511614266912,\n",
    "#       0.0021245721749356635,\n",
    "#       0.0021140864586307686,\n",
    "#       0.0021140020537887662,\n",
    "#       0.0021312513624253304,\n",
    "#       0.0021659531320681086,\n",
    "#       0.002159898722849434,\n",
    "#       0.0021544037722914626,\n",
    "#       0.0021432257865109,\n",
    "#       0.0021518090642393653,\n",
    "#       0.0021631419288655755,\n",
    "#       0.002181614003802716,\n",
    "#       0.0021625615599400964,\n",
    "#       0.0021590278305803263,\n",
    "#       0.0021472119056573625,\n",
    "#       0.002132958706219077,\n",
    "#       0.002129352445612402,\n",
    "#       0.00213440460678383,\n",
    "#       0.0021383566431434514,\n",
    "#       0.002142748318959072,\n",
    "#       0.002126983546672562,\n",
    "#       0.0021479773575079487,\n",
    "#       0.0021308963573109714,\n",
    "#       0.002119409888905393,\n",
    "#       0.002092067489609657,\n",
    "#       0.0020859422509025513,\n",
    "#       0.0020868519280000343,\n",
    "#       0.002083370159421231,\n",
    "#       0.0021008503744975393,\n",
    "#       0.0021117726228727734,\n",
    "#       0.002115878978996544,\n",
    "#       0.0021280314075281742,\n",
    "#       0.0021058036959738075,\n",
    "#       0.0021205148712148177,\n",
    "#       0.002133771793770507,\n",
    "#       0.002122591626264651,\n",
    "#       0.002103112654994186,\n",
    "#       0.0020823508769950874,\n",
    "#       0.002048491698351187,\n",
    "#       0.0020381323785148733,\n",
    "#       0.0020458759778573784,\n",
    "#       0.0020275284388238547,\n",
    "#       0.002011200904007852,\n",
    "#       0.0020044534693025016,\n",
    "#       0.0019856848148604805,\n",
    "#       0.001966697962856146,\n",
    "#       0.0019662397734575335,\n",
    "#       0.0019626378237478583,\n",
    "#       0.0019558416806943263,\n",
    "#       0.0019323364859317532,\n",
    "#       0.0019309240213660572,\n",
    "#       0.0018984603237586768,\n",
    "#       0.0018765360139024758,\n",
    "#       0.0018751620626461846,\n",
    "#       0.0018918123074864617,\n",
    "#       0.0018933255332011167,\n",
    "#       0.0019006535752861505,\n",
    "#       0.0019008086122634814,\n",
    "#       0.0019151012624010202,\n",
    "#       0.0019231013215017354,\n",
    "#       0.0019294059057521275,\n",
    "#       0.001940308897834611,\n",
    "#       0.0019523010387724284,\n",
    "#       0.001964081708020946,\n",
    "#       0.0019853003546512174,\n",
    "#       0.001998293034930298,\n",
    "#       0.002005942363217677,\n",
    "#       0.002015585078402022\n",
    "#     ]\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in res['params'].items():\n",
    "#     res['params'][key] = [value]\n",
    "# res['params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# läs in data\n",
    "def läs_in_data_och_förbered():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    \n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # läs in FEATIRES.txt\n",
    "    with open('../FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        features = f.read().splitlines()\n",
    "\n",
    "    assert len(features) == len(X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'   \n",
    "    assert set(features) == set(X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#              namn, ant_hästar, proba, Kelly,  motst_ant, motst_diff, ant_favoriter,   only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,       False,          0,            False,    True,   pref='../')\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,       True,           2,            True,     False,   pref='../')\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,       True,           2,            True,     True,   pref='../')\n",
    "# typ16= tp.Typ('typ16',True,       True, True,      2,       True,           2,            False,    True,   pref='../')\n",
    "\n",
    "typer = [typ6, typ1, typ9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "             'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV av typ6, typ1, typ9, typ16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "    # Läs in data\n",
    "    # remove_features\n",
    "\n",
    "# modeller konfigurering, CatBoostClassifier  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a GridSearch for CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def create_grid_search(X_, y_, typ, verbose=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    # remove features\n",
    "    X = typ.prepare_for_model(X)\n",
    "    if not typ.streck:\n",
    "        X.drop('streck', axis=1, inplace=True)\n",
    "    \n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features\\n',cat_features)\n",
    "    X = remove_features(X, remove_mer=['datum','avd'])\n",
    "    # get numerical features and cat_features\n",
    "    num_features = list(X.select_dtypes(include=[np.number]).columns)\n",
    "    cat_features = list(X.select_dtypes(include=['object']).columns)\n",
    "    # print('cat_features:\\n',cat_features,'\\n')\n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "    # create a GridSearch\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "        'iterations': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        'bagging_temperature': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'rsm': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'depth_per_tree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'border_count': [32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "        'fold_permutation_block_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'od_pval': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9],\n",
    "        'od_wait': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3,  1.5, 1.7, 1.9],\n",
    "        'od_type': ['IncToDec', 'Iter'],\n",
    "    }\n",
    "\n",
    "    # X.datum = pd.to_datetime(X.datum)\n",
    "    # X.datum = X.datum.dt.date\n",
    "    # alla_datum = X.datum.unique()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # for train_index, test_index in tscv.split(X):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        # X_train, X_test = X[train_index], X[test_index]\n",
    "        # y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = CatBoostClassifier(iterations=500, loss_function='Logloss', eval_metric='AUC',\n",
    "                               use_best_model=False, early_stopping_rounds=100, verbose=verbose,)\n",
    "\n",
    "    grid = {'learning_rate': [0.002,0.005],\n",
    "            'depth': [2,4,6,8],\n",
    "            'l2_leaf_reg': [8, 9, 10,11]}\n",
    "    \n",
    "    # print(X[cat_features].info())\n",
    "    assert X[cat_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "\n",
    "    grid_search_result = model.grid_search(grid,\n",
    "                                        X=Pool(X, y, cat_features=cat_features),\n",
    "                                        cv=tscv.split(X),\n",
    "                                        shuffle=False,\n",
    "                                        search_by_train_test_split=False,\n",
    "                                        verbose=verbose,\n",
    "                                        plot=True)\n",
    "    return grid_search_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testa grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN in cat before: 246\n",
      "Number of NaN in num before: 65637\n",
      "Number of NaN in cat after: 0\n",
      "Number of NaN in num after: 0\n"
     ]
    }
   ],
   "source": [
    "# Total GridSearchCV där jag använder egen TimeseriesSplit\n",
    "if False:   # modellerna typ1, typ9, typ16\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results6 = create_grid_search(X, y, typ6)\n",
    "\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    results1 = create_grid_search(X, y, typ1)\n",
    "\n",
    "    X,y = läs_in_data_och_förbered()\n",
    "    results9 = create_grid_search(X, y, typ9)\n",
    "\n",
    "    # X, y = läs_in_data_och_förbered()\n",
    "    # results16 = create_grid_search(X, y, typ16)\n",
    "\n",
    "if True: # prova KNN\n",
    "    # KNN GridSearch\n",
    "    # import KNN\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    X,y = läs_in_data_och_förbered()\n",
    "    X = remove_features(X, remove_mer=['avd', 'datum', 'streck'])\n",
    "    \n",
    "# get numerical features and cat_features\n",
    "num_features = list(X.select_dtypes(include=[np.number]).columns)\n",
    "cat_features = list(X.select_dtypes(include=['object']).columns)\n",
    "\n",
    "print('Number of NaN in cat before:', X[cat_features].isna().sum()[\n",
    "    X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "print('Number of NaN in num before:', X[num_features].isna().sum()[\n",
    "    X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "\n",
    "# impute 'missing' for all NaN in cat_features\n",
    "X[cat_features] = X[cat_features].fillna('missing')\n",
    "X[num_features] = X[num_features].fillna(0)\n",
    "\n",
    "print('Number of NaN in cat after:', X[cat_features].isna().sum()[\n",
    "    X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "print('Number of NaN in num after:', X[num_features].isna().sum()[\n",
    "    X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "10657\n",
      "1115\n",
      "3\n",
      "1679\n",
      "627\n",
      "1756\n",
      "678\n",
      "1826\n",
      "710\n",
      "1930\n",
      "714\n",
      "2005\n",
      "758\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bana</th>\n",
       "      <th>häst</th>\n",
       "      <th>kusk</th>\n",
       "      <th>kr</th>\n",
       "      <th>spår</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>ålder</th>\n",
       "      <th>kön</th>\n",
       "      <th>pris</th>\n",
       "      <th>h1_kusk</th>\n",
       "      <th>h1_bana</th>\n",
       "      <th>h1_spår</th>\n",
       "      <th>h1_plac</th>\n",
       "      <th>h1_pris</th>\n",
       "      <th>h1_odds</th>\n",
       "      <th>h1_kmtid</th>\n",
       "      <th>h2_kusk</th>\n",
       "      <th>h2_bana</th>\n",
       "      <th>h2_spår</th>\n",
       "      <th>h2_plac</th>\n",
       "      <th>h2_pris</th>\n",
       "      <th>h2_odds</th>\n",
       "      <th>h2_kmtid</th>\n",
       "      <th>h3_kusk</th>\n",
       "      <th>h3_bana</th>\n",
       "      <th>h3_spår</th>\n",
       "      <th>h3_plac</th>\n",
       "      <th>h3_pris</th>\n",
       "      <th>h3_odds</th>\n",
       "      <th>h3_kmtid</th>\n",
       "      <th>h4_kusk</th>\n",
       "      <th>h4_bana</th>\n",
       "      <th>h4_spår</th>\n",
       "      <th>h4_plac</th>\n",
       "      <th>h4_pris</th>\n",
       "      <th>h4_odds</th>\n",
       "      <th>h4_kmtid</th>\n",
       "      <th>h5_kusk</th>\n",
       "      <th>h5_bana</th>\n",
       "      <th>h5_spår</th>\n",
       "      <th>h5_plac</th>\n",
       "      <th>h5_pris</th>\n",
       "      <th>h5_odds</th>\n",
       "      <th>h5_kmtid</th>\n",
       "      <th>h1_dist</th>\n",
       "      <th>h2_dist</th>\n",
       "      <th>h3_dist</th>\n",
       "      <th>h4_dist</th>\n",
       "      <th>h5_dist</th>\n",
       "      <th>h1_auto</th>\n",
       "      <th>h2_auto</th>\n",
       "      <th>h3_auto</th>\n",
       "      <th>h4_auto</th>\n",
       "      <th>h5_auto</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45224</th>\n",
       "      <td>3</td>\n",
       "      <td>9783</td>\n",
       "      <td>1</td>\n",
       "      <td>51978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>562</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4</td>\n",
       "      <td>275</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3.06</td>\n",
       "      <td>12.7</td>\n",
       "      <td>4</td>\n",
       "      <td>508</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7</td>\n",
       "      <td>650</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>6.02</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7</td>\n",
       "      <td>305</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1282.555939</td>\n",
       "      <td>13430.958366</td>\n",
       "      <td>17.320508</td>\n",
       "      <td>12.247449</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45225</th>\n",
       "      <td>3</td>\n",
       "      <td>1764</td>\n",
       "      <td>642</td>\n",
       "      <td>94231.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>980</td>\n",
       "      <td>564</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>9.01</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1018</td>\n",
       "      <td>424</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.78</td>\n",
       "      <td>16.2</td>\n",
       "      <td>1068</td>\n",
       "      <td>284</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1132</td>\n",
       "      <td>275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1165</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>14.6</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10516.812635</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>8351.709315</td>\n",
       "      <td>9305.240914</td>\n",
       "      <td>10744.766693</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45226</th>\n",
       "      <td>3</td>\n",
       "      <td>912</td>\n",
       "      <td>6</td>\n",
       "      <td>68230.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12</td>\n",
       "      <td>393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13</td>\n",
       "      <td>604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.4</td>\n",
       "      <td>15</td>\n",
       "      <td>266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>13.6</td>\n",
       "      <td>16</td>\n",
       "      <td>470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45227</th>\n",
       "      <td>3</td>\n",
       "      <td>8337</td>\n",
       "      <td>749</td>\n",
       "      <td>36506.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1152</td>\n",
       "      <td>108</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>11.91</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1212</td>\n",
       "      <td>473</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.1</td>\n",
       "      <td>178</td>\n",
       "      <td>126</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1191</td>\n",
       "      <td>581</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1228</td>\n",
       "      <td>619</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12835.762032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45228</th>\n",
       "      <td>3</td>\n",
       "      <td>8831</td>\n",
       "      <td>250</td>\n",
       "      <td>84418.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>151</td>\n",
       "      <td>102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.65</td>\n",
       "      <td>11.7</td>\n",
       "      <td>158</td>\n",
       "      <td>568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.35</td>\n",
       "      <td>10.1</td>\n",
       "      <td>919</td>\n",
       "      <td>500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>10.5</td>\n",
       "      <td>171</td>\n",
       "      <td>511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>11.7</td>\n",
       "      <td>174</td>\n",
       "      <td>693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2446.919323</td>\n",
       "      <td>4238.188589</td>\n",
       "      <td>2996.851891</td>\n",
       "      <td>28.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45229</th>\n",
       "      <td>3</td>\n",
       "      <td>8194</td>\n",
       "      <td>109</td>\n",
       "      <td>305941.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>151</td>\n",
       "      <td>564</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>11.9</td>\n",
       "      <td>158</td>\n",
       "      <td>191</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>3.23</td>\n",
       "      <td>9.4</td>\n",
       "      <td>163</td>\n",
       "      <td>446</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>13.2</td>\n",
       "      <td>171</td>\n",
       "      <td>502</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.26</td>\n",
       "      <td>11.8</td>\n",
       "      <td>174</td>\n",
       "      <td>684</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17339.292691</td>\n",
       "      <td>17339.292691</td>\n",
       "      <td>12260.731443</td>\n",
       "      <td>15508.734856</td>\n",
       "      <td>49042.925773</td>\n",
       "      <td>36.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45230</th>\n",
       "      <td>3</td>\n",
       "      <td>937</td>\n",
       "      <td>554</td>\n",
       "      <td>24350.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>827</td>\n",
       "      <td>397</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>11.5</td>\n",
       "      <td>875</td>\n",
       "      <td>609</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>18.08</td>\n",
       "      <td>30.0</td>\n",
       "      <td>919</td>\n",
       "      <td>376</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>30.0</td>\n",
       "      <td>976</td>\n",
       "      <td>205</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>9.79</td>\n",
       "      <td>9.6</td>\n",
       "      <td>990</td>\n",
       "      <td>535</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7754.367428</td>\n",
       "      <td>3.179138</td>\n",
       "      <td>1.005332</td>\n",
       "      <td>1.297878</td>\n",
       "      <td>1.217517</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45231</th>\n",
       "      <td>3</td>\n",
       "      <td>7309</td>\n",
       "      <td>159</td>\n",
       "      <td>53286.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>251</td>\n",
       "      <td>450</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>39.95</td>\n",
       "      <td>9.7</td>\n",
       "      <td>259</td>\n",
       "      <td>300</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.7</td>\n",
       "      <td>278</td>\n",
       "      <td>317</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>11.6</td>\n",
       "      <td>286</td>\n",
       "      <td>311</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.9</td>\n",
       "      <td>292</td>\n",
       "      <td>332</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2346.618088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45232</th>\n",
       "      <td>3</td>\n",
       "      <td>1855</td>\n",
       "      <td>145</td>\n",
       "      <td>173582.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>232</td>\n",
       "      <td>450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4.24</td>\n",
       "      <td>9.8</td>\n",
       "      <td>236</td>\n",
       "      <td>568</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>7.83</td>\n",
       "      <td>9.6</td>\n",
       "      <td>460</td>\n",
       "      <td>551</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>11.6</td>\n",
       "      <td>487</td>\n",
       "      <td>110</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.80</td>\n",
       "      <td>10.2</td>\n",
       "      <td>501</td>\n",
       "      <td>651</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1423.295817</td>\n",
       "      <td>36716.745533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45233</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>1110</td>\n",
       "      <td>70281.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1249</td>\n",
       "      <td>451</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>9.08</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1749</td>\n",
       "      <td>491</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1820</td>\n",
       "      <td>500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1922</td>\n",
       "      <td>513</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.47</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1999</td>\n",
       "      <td>542</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>12.9</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.297878</td>\n",
       "      <td>0.820850</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>17339.292691</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>14.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bana  häst  kusk        kr  spår    dist  lopp_dist  start  ålder  kön       pris  h1_kusk  h1_bana  h1_spår  h1_plac  h1_pris  h1_odds  h1_kmtid  h2_kusk  h2_bana  h2_spår  h2_plac  h2_pris  \\\n",
       "45224     3  9783     1   51978.0   1.0  2100.0     2100.0      0      6    2  1200000.0        4      562      2.0      8.0   1500.0     6.36      12.4        4      275      2.0      1.0    150.0   \n",
       "45225     3  1764   642   94231.0   2.0  2100.0     2100.0      0      5    0  1200000.0      980      564      5.0      2.0    250.0     9.01      12.0     1018      424      5.0     15.0    125.0   \n",
       "45226     3   912     6   68230.0   3.0  2100.0     2100.0      0      5    0  1200000.0       11      360      0.0     20.0      0.0     3.30      30.0       12      393      0.0     20.0      0.0   \n",
       "45227     3  8337   749   36506.0   4.0  2100.0     2100.0      0      4    0  1200000.0     1152      108      7.0      1.0    137.0    11.91      12.2     1212      473      7.0      1.0      0.0   \n",
       "45228     3  8831   250   84418.0   5.0  2100.0     2100.0      0      7    0  1200000.0      151      102      0.0      3.0      0.0     4.65      11.7      158      568      0.0      2.0      0.0   \n",
       "45229     3  8194   109  305941.0   6.0  2100.0     2100.0      0      5    0  1200000.0      151      564      6.0      1.0    250.0     1.13      11.9      158      191      6.0      1.0    250.0   \n",
       "45230     3   937   554   24350.0   7.0  2100.0     2100.0      0      5    0  1200000.0      827      397      3.0      1.0     50.0     1.79      11.5      875      609      3.0     20.0   1500.0   \n",
       "45231     3  7309   159   53286.0   8.0  2100.0     2100.0      0      7    2  1200000.0      251      450      4.0      5.0    250.0    39.95       9.7      259      300      4.0      3.0      0.0   \n",
       "45232     3  1855   145  173582.0   9.0  2100.0     2100.0      0      9    0  1200000.0      232      450      3.0      6.0    250.0     4.24       9.8      236      568      3.0      1.0   1121.0   \n",
       "45233     3    70  1110   70281.0  10.0  2100.0     2100.0      0      6    2  1200000.0     1249      451      2.0     20.0    250.0     9.08      30.0     1749      491      2.0     20.0    100.0   \n",
       "\n",
       "       h2_odds  h2_kmtid  h3_kusk  h3_bana  h3_spår  h3_plac  h3_pris  h3_odds  h3_kmtid  h4_kusk  h4_bana  h4_spår  h4_plac  h4_pris  h4_odds  h4_kmtid  h5_kusk  h5_bana  h5_spår  h5_plac  h5_pris  \\\n",
       "45224     3.06      12.7        4      508      2.0     15.0    300.0     6.06      12.4        7      650      2.0     15.0    150.0     6.02      10.1        7      305      2.0     15.0    400.0   \n",
       "45225     5.78      16.2     1068      284      5.0      1.0     58.0     1.87      12.0     1132      275      5.0      1.0     72.0     1.30      12.8     1165       25      5.0      1.0     96.0   \n",
       "45226     2.00      30.0       13      604      0.0     15.0      0.0    17.20      11.4       15      266      0.0     15.0      0.0     1.40      13.6       16      470      0.0     15.0      0.0   \n",
       "45227     2.45      14.1      178      126      7.0      3.0      0.0     3.40      13.5     1191      581      7.0      3.0      0.0     2.60      13.0     1228      619      7.0      3.0      0.0   \n",
       "45228     9.35      10.1      919      500      0.0      4.0    100.0     9.20      10.5      171      511      0.0      4.0    300.0     7.07      11.7      174      693      0.0      4.0    150.0   \n",
       "45229     3.23       9.4      163      446      6.0      1.0    125.0     1.77      13.2      171      502      6.0      1.0    200.0     1.26      11.8      174      684      6.0      1.0   2000.0   \n",
       "45230    18.08      30.0      919      376      3.0     20.0    150.0     1.63      30.0      976      205      3.0     20.0    250.0     9.79       9.6      990      535      3.0     20.0    220.0   \n",
       "45231     9.50       9.7      278      317      4.0      2.0      0.0     4.40      11.6      286      311      4.0      2.0      0.0     3.40      10.9      292      332      4.0      2.0      0.0   \n",
       "45232     7.83       9.6      460      551      3.0     15.0      0.0     2.60      11.6      487      110      3.0     15.0      0.0     8.80      10.2      501      651      3.0     15.0      0.0   \n",
       "45233     1.30      30.0     1820      500      2.0      1.0    100.0     1.92      12.8     1922      513      2.0      1.0    250.0     1.47      11.5     1999      542      2.0      1.0    100.0   \n",
       "\n",
       "       h5_odds  h5_kmtid  h1_dist  h2_dist  h3_dist  h4_dist  h5_dist  h1_auto  h2_auto  h3_auto  h4_auto  h5_auto       h1_perf       h2_perf       h3_perf       h4_perf       h5_perf  senast  \\\n",
       "45224     3.68      13.2   2140.0   2140.0   2140.0   1640.0   2640.0        1        1        1        1        1   1282.555939  13430.958366     17.320508     12.247449     20.000000    36.0   \n",
       "45225     4.30      14.6   2640.0   2140.0   2100.0   2100.0   2100.0        1        1        1        1        1  10516.812635     11.180340   8351.709315   9305.240914  10744.766693    36.0   \n",
       "45226     1.30      11.0   1600.0   1600.0   2100.0   2080.0   1640.0        1        1        1        1        1      0.000000      0.000000      0.000000      0.000000      0.000000    35.0   \n",
       "45227     6.50      12.8   2000.0   2060.0   2000.0   1609.0   1609.0        1        1        1        1        1  12835.762032      0.000000      0.000000      0.000000      0.000000    28.0   \n",
       "45228     2.52      10.2   2011.0   1620.0   1640.0   2140.0   1640.0        1        1        1        1        1      0.000000      0.000000   2446.919323   4238.188589   2996.851891    28.0   \n",
       "45229     4.73      30.0   2640.0   1640.0   2140.0   2140.0   2140.0        1        1        1        1        1  17339.292691  17339.292691  12260.731443  15508.734856  49042.925773    36.0   \n",
       "45230     3.28      12.0   2140.0   2140.0   2140.0   1640.0   2140.0        1        1        1        1        1   7754.367428      3.179138      1.005332      1.297878      1.217517    23.0   \n",
       "45231     1.20      10.6   1609.0   1609.0   1609.0   1609.0   1609.0        1        1        1        1        1   2346.618088      0.000000      0.000000      0.000000      0.000000    14.0   \n",
       "45232    10.10      30.0   1609.0   1620.0   1600.0   1609.0   2100.0        1        1        1        1        1   1423.295817  36716.745533      0.000000      0.000000      0.000000    14.0   \n",
       "45233     1.11      12.9   1609.0   2140.0   2140.0   2140.0   2140.0        1        1        1        1        1      1.297878      0.820850  10966.331584  17339.292691  10966.331584    14.0   \n",
       "\n",
       "       delta1  delta2  delta3  delta4  \n",
       "45224    28.0    14.0    14.0   133.0  \n",
       "45225    20.0    22.0   147.0    14.0  \n",
       "45226    16.0   117.0    35.0    22.0  \n",
       "45227    14.0    14.0    93.0     7.0  \n",
       "45228    14.0    11.0    25.0    14.0  \n",
       "45229    22.0    14.0   159.0    15.0  \n",
       "45230    13.0    12.0    10.0    20.0  \n",
       "45231    13.0    21.0    21.0    14.0  \n",
       "45232    28.0    21.0    28.0    28.0  \n",
       "45233    53.0    21.0   109.0    17.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN in cat after: 0\n",
      "Number of NaN in num after: 0\n"
     ]
    }
   ],
   "source": [
    "X[cat_features] = X[cat_features].astype('category')\n",
    "# make categorical features numeric\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].cat.codes\n",
    "    print(len(X[col].unique()))\n",
    "\n",
    "display(X.tail(10))\n",
    "\n",
    "print('Number of NaN in cat after:', X[cat_features].isna().sum()[\n",
    "    X[cat_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "print('Number of NaN in num after:', X[num_features].isna().sum()[\n",
    "    X[num_features].isna().sum() > 0].sort_values(ascending=False).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 35}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.00550757782721253"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.00550757782721253"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if True:\n",
    "    # create a gridsearch for KNN\n",
    "    param_grid = {  'n_neighbors': [30,35],\n",
    "                    # 'weights': ['uniform','distance'],\n",
    "                    # 'algorithm': ['auto','ball_tree', 'kd_tree', 'brute'],\n",
    "                    # 'leaf_size': [4,5,6,7,10,20,30],\n",
    "                    # 'p': [1,2,3,5,10],\n",
    "                    # 'metric': ['euclidean','manhattan', 'chebyshev']\n",
    "                 }\n",
    "\n",
    "    grid_search = RandomizedSearchCV(KNeighborsClassifier(n_jobs=-1), param_grid, n_jobs=6, cv=tscv.split(X), scoring='f1', return_train_score=True, verbose=5)\n",
    "\n",
    "    \n",
    "    resultat = grid_search.fit(X, y)\n",
    "    display(resultat.best_params_, resultat.best_score_, max(resultat.cv_results_['mean_test_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 35}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultat.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "params ={'depth':6, 'learning_rate':0.005, 'l2_leaf_reg':10}\n",
    "# convert params to parameters\n",
    "# params_str = ','.join([str(k)+'='+str(v) for k, v in params.items()])\n",
    "cbc=CatBoostClassifier(**params, iterations=120)\n",
    "print(cbc.get_param('iterations'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-AUC-mean</th>\n",
       "      <th>test-AUC-std</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>test-Logloss-std</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "      <th>train-Logloss-std</th>\n",
       "      <th>typ</th>\n",
       "      <th>best_params</th>\n",
       "      <th>comm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>0.797722</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.263087</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.260534</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>1</td>\n",
       "      <td>'d': 4, 'l2': 9, 'LR': 0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283</td>\n",
       "      <td>0.813249</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.259381</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.255515</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>6</td>\n",
       "      <td>'d': 6, 'l2': 9, 'LR': 0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>297</td>\n",
       "      <td>0.813179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.254232</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>9</td>\n",
       "      <td>'d': 4, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>297</td>\n",
       "      <td>0.813179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.254232</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>16</td>\n",
       "      <td>'d': 4, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>298</td>\n",
       "      <td>0.798138</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.267621</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.265052</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>1</td>\n",
       "      <td>'d': 4, 'l2': 11, 'LR': 0.005</td>\n",
       "      <td>fler val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>299</td>\n",
       "      <td>0.813913</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>0.258810</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.256468</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>6</td>\n",
       "      <td>'d': 2, 'l2': 9, 'LR': 0.005</td>\n",
       "      <td>fler val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>297</td>\n",
       "      <td>0.813179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.254232</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>9</td>\n",
       "      <td>'d': 4, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>fler val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>297</td>\n",
       "      <td>0.813179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.254232</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>16</td>\n",
       "      <td>'d': 4, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>fler val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>432</td>\n",
       "      <td>0.799307</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.260090</td>\n",
       "      <td>0.021130</td>\n",
       "      <td>0.256483</td>\n",
       "      <td>0.024002</td>\n",
       "      <td>1</td>\n",
       "      <td>'d': 4, 'l2': 8, 'LR': 0.005</td>\n",
       "      <td>ökade iter=500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>460</td>\n",
       "      <td>0.815003</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.244052</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.240895</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>6</td>\n",
       "      <td>'d': 2, 'l2': 9, 'LR': 0.005</td>\n",
       "      <td>ökade iter=500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>496</td>\n",
       "      <td>0.814913</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.242677</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.239123</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>9</td>\n",
       "      <td>'d': 2, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>ökade iter=500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>496</td>\n",
       "      <td>0.814913</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.242677</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.239123</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>16</td>\n",
       "      <td>'d': 2, 'l2': 10, 'LR': 0.005</td>\n",
       "      <td>ökade iter=500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  test-Logloss-std  train-Logloss-mean  train-Logloss-std  typ                    best_params            comm\n",
       "0          296       0.797722      0.003251           0.263087          0.002299            0.260534           0.000938    1   'd': 4, 'l2': 9, 'LR': 0.005             NaN\n",
       "1          283       0.813249      0.003424           0.259381          0.001801            0.255515           0.001394    6   'd': 6, 'l2': 9, 'LR': 0.005             NaN\n",
       "2          297       0.813179      0.004967           0.257471          0.001976            0.254232           0.000886    9  'd': 4, 'l2': 10, 'LR': 0.005             NaN\n",
       "3          297       0.813179      0.004967           0.257471          0.001976            0.254232           0.000886   16  'd': 4, 'l2': 10, 'LR': 0.005             NaN\n",
       "4          298       0.798138      0.004514           0.267621          0.011353            0.265052           0.011595    1  'd': 4, 'l2': 11, 'LR': 0.005        fler val\n",
       "5          299       0.813913      0.004760           0.258810          0.001824            0.256468           0.000602    6   'd': 2, 'l2': 9, 'LR': 0.005        fler val\n",
       "6          297       0.813179      0.004967           0.257471          0.001976            0.254232           0.000886    9  'd': 4, 'l2': 10, 'LR': 0.005        fler val\n",
       "7          297       0.813179      0.004967           0.257471          0.001976            0.254232           0.000886   16  'd': 4, 'l2': 10, 'LR': 0.005        fler val\n",
       "8          432       0.799307      0.003276           0.260090          0.021130            0.256483           0.024002    1   'd': 4, 'l2': 8, 'LR': 0.005  ökade iter=500\n",
       "9          460       0.815003      0.004842           0.244052          0.002184            0.240895           0.001442    6   'd': 2, 'l2': 9, 'LR': 0.005  ökade iter=500\n",
       "10         496       0.814913      0.004943           0.242677          0.002470            0.239123           0.001752    9  'd': 2, 'l2': 10, 'LR': 0.005  ökade iter=500\n",
       "11         496       0.814913      0.004943           0.242677          0.002470            0.239123           0.001752   16  'd': 2, 'l2': 10, 'LR': 0.005  ökade iter=500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfResults = pd.read_csv('results.csv')\n",
    "def results(typ, dfResults, comm=[]):\n",
    "    s = \"results\"+str(typ)\n",
    "    temp1 =globals()[s]\n",
    "    temp = pd.DataFrame(temp1['cv_results'])\n",
    "    temp['typ'] = typ\n",
    "    max=temp['test-AUC-mean'].max()\n",
    "    temp=temp[temp['test-AUC-mean']==max]\n",
    "    if len(comm)>0:\n",
    "        temp['comm'] = comm\n",
    "    str(temp1['params'])\n",
    "    temp['best_params'] = str(temp1['params']).replace('{','').replace('}','').replace('depth','d').replace('l2_leaf_reg','l2').replace('learning_rate','LR')\n",
    "    dfResults = pd.concat([dfResults,temp])\n",
    "    return dfResults\n",
    "\n",
    "#### Slår ihop alla resultaten från GridSearch i funktionen ovan\n",
    "if False:\n",
    "    dfResults = results(1, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(6, dfResults,  'ökade iter=500')\n",
    "    dfResults = results(9, dfResults,  'ökade iter=500')\n",
    "    # dfResults = results(16, dfResults, 'ökade iter=500')\n",
    "    dfResults.to_csv('results.csv', index=False)\n",
    "dfResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spara modeller och optimala parms (kanske skall läras på del 1-4?)\n",
    "- Predict på den sista 5:e delen och skapa input till meta-model\n",
    "- gridsearchcv meta_model på detta. Spara optimala parms\n",
    "- avsluta med en learn på hela 5:- delen med optimala parms och spara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_temp(typ, X_, y_, iterations, depth, learning_rate, l2_leaf_reg,save=False):\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    \n",
    "    typ.learn(X, y, learning_rate=learning_rate, iterations=iterations, depth=depth, l2_leaf_reg=l2_leaf_reg, save=save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_until 37694 test_from 37695\n",
      "[6 460 \"'d': 2, 'l2': 9, 'LR': 0.005\"]\n",
      "[1 432 \"'d': 4, 'l2': 8, 'LR': 0.005\"]\n",
      "[9 496 \"'d': 2, 'l2': 10, 'LR': 0.005\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def learn_all_models(totals = False, save=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_until = train_index[-1:][0]\n",
    "        test_from = train_until+1\n",
    "\n",
    "    if totals == False:\n",
    "        ### sista 5:e delen av data\n",
    "        print('train_until',train_until, 'test_from',test_from)\n",
    "\n",
    "        X_train, X_test = X.iloc[:test_from], X.iloc[test_from:]\n",
    "        y_train, y_test = y[:test_from], y[test_from:]\n",
    "    else:\n",
    "        ### all data\n",
    "        print('train all')\n",
    "\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "    results = pd.read_csv('results.csv')\n",
    "\n",
    "    # typ6: 'd': 2, 'l2': 9, 'LR': 0.005\n",
    "    print(results[results.typ==6][['typ','iterations','best_params']].iloc[-1].values)\n",
    "    iter=460\n",
    "    d=2\n",
    "    LR=0.005\n",
    "    L2=9\n",
    "    learn_temp(typ6, X_train, y_train, iterations=iter, depth=d, learning_rate=LR, l2_leaf_reg=L2,save=save)\n",
    "\n",
    "    # typ1: 'd': 4, 'l2': 8, 'LR': 0.005\n",
    "    print(results[results.typ == 1][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 432\n",
    "    d = 4\n",
    "    LR = 0.005\n",
    "    L2 = 9\n",
    "    learn_temp(typ1, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "    # typ9: 'd': 2, 'l2': 10, 'LR': 0.005\n",
    "    print(results[results.typ == 9][['typ', 'iterations', 'best_params']].iloc[-1].values)\n",
    "    iter = 496\n",
    "    d = 2\n",
    "    LR = 0.005\n",
    "    L2 = 10\n",
    "    learn_temp(typ9, X_train, y_train, iterations=iter, depth=d,\n",
    "               learning_rate=LR, l2_leaf_reg=L2, save=save)\n",
    "\n",
    "if True:\n",
    "    learn_all_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapa ett Kelly-värde baserat på streck omvandlat till odds\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open('rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_from for meta 37695\n",
      "Index(['proba6', 'kelly6', 'proba1', 'kelly1', 'proba9', 'kelly9'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba6</th>\n",
       "      <th>kelly6</th>\n",
       "      <th>proba1</th>\n",
       "      <th>kelly1</th>\n",
       "      <th>proba9</th>\n",
       "      <th>kelly9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027114</td>\n",
       "      <td>-0.008182</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>-0.019691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033072</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>-0.007582</td>\n",
       "      <td>0.021116</td>\n",
       "      <td>-0.014397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457815</td>\n",
       "      <td>0.277788</td>\n",
       "      <td>0.241297</td>\n",
       "      <td>-0.010625</td>\n",
       "      <td>0.364020</td>\n",
       "      <td>0.152848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013895</td>\n",
       "      <td>-0.016828</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>0.011530</td>\n",
       "      <td>-0.019267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257797</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.142181</td>\n",
       "      <td>-0.142650</td>\n",
       "      <td>0.309993</td>\n",
       "      <td>0.080882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>0.273233</td>\n",
       "      <td>-0.080592</td>\n",
       "      <td>0.530361</td>\n",
       "      <td>0.301719</td>\n",
       "      <td>0.340373</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7535</th>\n",
       "      <td>0.015803</td>\n",
       "      <td>-0.019903</td>\n",
       "      <td>0.077053</td>\n",
       "      <td>0.043569</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>-0.027647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>0.014275</td>\n",
       "      <td>-0.016436</td>\n",
       "      <td>0.023404</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>0.013279</td>\n",
       "      <td>-0.017464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>0.062726</td>\n",
       "      <td>-0.011537</td>\n",
       "      <td>0.148711</td>\n",
       "      <td>0.081261</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>-0.044973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>0.037598</td>\n",
       "      <td>-0.089287</td>\n",
       "      <td>0.146579</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>0.109207</td>\n",
       "      <td>-0.008237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7539 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        proba6    kelly6    proba1    kelly1    proba9    kelly9\n",
       "0     0.027114 -0.008182  0.021605 -0.013890  0.016008 -0.019691\n",
       "1     0.033072 -0.002008  0.027692 -0.007582  0.021116 -0.014397\n",
       "2     0.457815  0.277788  0.241297 -0.010625  0.364020  0.152848\n",
       "3     0.013895 -0.016828  0.017940 -0.012657  0.011530 -0.019267\n",
       "4     0.257797  0.011355  0.142181 -0.142650  0.309993  0.080882\n",
       "...        ...       ...       ...       ...       ...       ...\n",
       "7534  0.273233 -0.080592  0.530361  0.301719  0.340373  0.019235\n",
       "7535  0.015803 -0.019903  0.077053  0.043569  0.008330 -0.027647\n",
       "7536  0.014275 -0.016436  0.023404 -0.007023  0.013279 -0.017464\n",
       "7537  0.062726 -0.011537  0.148711  0.081261  0.031744 -0.044973\n",
       "7538  0.037598 -0.089287  0.146579  0.034062  0.109207 -0.008237\n",
       "\n",
       "[7539 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stack for learning meta_model\n",
    "def skapa_stack_learning(X_, y, save=True):\n",
    "    X=X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    for typ in typer:\n",
    "            nr = typ.name[3:]\n",
    "            stacked_data['proba'+nr] = typ.predict(X)\n",
    "            stacked_data['kelly' + nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "\n",
    "    print(stacked_data.columns)\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    return stacked_data,y   # enbart stack-info\n",
    "\n",
    "if True: # förbered data och kör stacking\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    X, y = läs_in_data_och_förbered()\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index[-1:], \"TEST:\", test_index[0])\n",
    "        train_from = test_index[0]   # sic!\n",
    "\n",
    "    ### sista 5:e delen av data\n",
    "    print('train_from for meta', train_from)\n",
    "\n",
    "    X_train, y_train = X.iloc[train_from:], y[train_from:]\n",
    "    X_stack,y_stack = skapa_stack_learning(X_train, y_train, False)\n",
    "    display(X_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konf 1 n_estimators 10 max_depth None min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.9169651147367025\n",
      "konf 2 n_estimators 10 max_depth None min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.9148428173497811\n",
      "konf 3 n_estimators 10 max_depth None min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.9044966175885396\n",
      "konf 4 n_estimators 10 max_depth 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.772383605252686\n",
      "konf 5 n_estimators 10 max_depth 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.772383605252686\n",
      "konf 6 n_estimators 10 max_depth 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.772383605252686\n",
      "konf 7 n_estimators 10 max_depth 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.797585886722377\n",
      "konf 8 n_estimators 10 max_depth 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.797585886722377\n",
      "konf 9 n_estimators 10 max_depth 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.797585886722377\n",
      "konf 10 n_estimators 10 max_depth 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.8152274837511606\n",
      "konf 11 n_estimators 10 max_depth 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.8181456426581775\n",
      "konf 12 n_estimators 10 max_depth 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8157580580978909\n",
      "konf 13 n_estimators 10 min_samples_split 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.772383605252686\n",
      "konf 14 n_estimators 10 min_samples_split 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.772383605252686\n",
      "konf 15 n_estimators 10 min_samples_split 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.772383605252686\n",
      "konf 16 n_estimators 10 min_samples_split 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.797585886722377\n",
      "konf 17 n_estimators 10 min_samples_split 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.797585886722377\n",
      "konf 18 n_estimators 10 min_samples_split 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.797585886722377\n",
      "konf 19 n_estimators 10 min_samples_split 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.8152274837511606\n",
      "konf 20 n_estimators 10 min_samples_split 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.8181456426581775\n",
      "konf 21 n_estimators 10 min_samples_split 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n",
      "OOB_score 0.8157580580978909\n",
      "konf 22 n_estimators 100 max_depth None min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9293009682981828\n",
      "konf 23 n_estimators 100 max_depth None min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9267807401512137\n",
      "konf 24 n_estimators 100 max_depth None min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9262501658044834\n",
      "konf 25 n_estimators 100 max_depth 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 26 n_estimators 100 max_depth 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 27 n_estimators 100 max_depth 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 28 n_estimators 100 max_depth 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8091258787637617\n",
      "konf 29 n_estimators 100 max_depth 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8091258787637617\n",
      "konf 30 n_estimators 100 max_depth 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8087279480037141\n",
      "konf 31 n_estimators 100 max_depth 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8401644780474864\n",
      "konf 32 n_estimators 100 max_depth 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8416235575009948\n",
      "konf 33 n_estimators 100 max_depth 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8364504576203741\n",
      "konf 34 n_estimators 100 min_samples_split 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 35 n_estimators 100 min_samples_split 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 36 n_estimators 100 min_samples_split 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7760976256797983\n",
      "konf 37 n_estimators 100 min_samples_split 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8091258787637617\n",
      "konf 38 n_estimators 100 min_samples_split 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8091258787637617\n",
      "konf 39 n_estimators 100 min_samples_split 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8087279480037141\n",
      "konf 40 n_estimators 100 min_samples_split 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8401644780474864\n",
      "konf 41 n_estimators 100 min_samples_split 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8416235575009948\n",
      "konf 42 n_estimators 100 min_samples_split 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8364504576203741\n",
      "konf 43 n_estimators 200 max_depth None min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9286377503647698\n",
      "konf 44 n_estimators 200 max_depth None min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9282398196047221\n",
      "konf 45 n_estimators 200 max_depth None min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9270460273245789\n",
      "konf 46 n_estimators 200 max_depth 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 47 n_estimators 200 max_depth 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 48 n_estimators 200 max_depth 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 49 n_estimators 200 max_depth 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8107176018039528\n",
      "konf 50 n_estimators 200 max_depth 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8105849582172702\n",
      "konf 51 n_estimators 200 max_depth 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8097890966971747\n",
      "konf 52 n_estimators 200 max_depth 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8430826369545033\n",
      "konf 53 n_estimators 200 max_depth 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.841225626740947\n",
      "konf 54 n_estimators 200 max_depth 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8400318344608039\n",
      "konf 55 n_estimators 200 min_samples_split 2 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 56 n_estimators 200 min_samples_split 2 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 57 n_estimators 200 min_samples_split 2 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.7795463589335455\n",
      "konf 58 n_estimators 200 min_samples_split 4 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8107176018039528\n",
      "konf 59 n_estimators 200 min_samples_split 4 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8105849582172702\n",
      "konf 60 n_estimators 200 min_samples_split 4 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8097890966971747\n",
      "konf 61 n_estimators 200 min_samples_split 6 min_samples_split 2\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8430826369545033\n",
      "konf 62 n_estimators 200 min_samples_split 6 min_samples_split 4\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.841225626740947\n",
      "konf 63 n_estimators 200 min_samples_split 6 min_samples_split 6\n",
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.8400318344608039\n",
      "{1: 0.9169651147367025, 2: 0.9148428173497811, 3: 0.9044966175885396, 4: 0.772383605252686, 5: 0.772383605252686, 6: 0.772383605252686, 7: 0.797585886722377, 8: 0.797585886722377, 9: 0.797585886722377, 10: 0.8152274837511606, 11: 0.8181456426581775, 12: 0.8157580580978909, 13: 0.772383605252686, 14: 0.772383605252686, 15: 0.772383605252686, 16: 0.797585886722377, 17: 0.797585886722377, 18: 0.797585886722377, 19: 0.8152274837511606, 20: 0.8181456426581775, 21: 0.8157580580978909, 22: 0.9293009682981828, 23: 0.9267807401512137, 24: 0.9262501658044834, 25: 0.7760976256797983, 26: 0.7760976256797983, 27: 0.7760976256797983, 28: 0.8091258787637617, 29: 0.8091258787637617, 30: 0.8087279480037141, 31: 0.8401644780474864, 32: 0.8416235575009948, 33: 0.8364504576203741, 34: 0.7760976256797983, 35: 0.7760976256797983, 36: 0.7760976256797983, 37: 0.8091258787637617, 38: 0.8091258787637617, 39: 0.8087279480037141, 40: 0.8401644780474864, 41: 0.8416235575009948, 42: 0.8364504576203741, 43: 0.9286377503647698, 44: 0.9282398196047221, 45: 0.9270460273245789, 46: 0.7795463589335455, 47: 0.7795463589335455, 48: 0.7795463589335455, 49: 0.8107176018039528, 50: 0.8105849582172702, 51: 0.8097890966971747, 52: 0.8430826369545033, 53: 0.841225626740947, 54: 0.8400318344608039, 55: 0.7795463589335455, 56: 0.7795463589335455, 57: 0.7795463589335455, 58: 0.8107176018039528, 59: 0.8105849582172702, 60: 0.8097890966971747, 61: 0.8430826369545033, 62: 0.841225626740947, 63: 0.8400318344608039}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# fit meta_model\n",
    "def learn_meta_model(X, y,  save=True,\n",
    "                     n_estimators=100, \n",
    "                     max_depth=None,\n",
    "                     min_samples_split=2, \n",
    "                     min_samples_leaf=1, \n",
    "                     max_features='auto', \n",
    "                     max_leaf_nodes=None,\n",
    "                     max_samples=None,):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "\n",
    "    meta_model = RandomForestClassifier(\n",
    "       class_weight='balanced',    #{0:1,1:10},\n",
    "       oob_score=True, verbose=1, n_jobs=8, random_state=2022,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_samples=max_samples,)\n",
    "    \n",
    "    meta_model.fit(X, y)\n",
    "\n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9430916552667579\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open('../modeller/meta_model.model', 'wb') as f:\n",
    "            pickle.dump(meta_model, f)\n",
    "\n",
    "    return meta_model\n",
    "\n",
    "if True:\n",
    "    grid ={\n",
    "        'n_estimators': [10,100,200],\n",
    "        'max_depth': [None,2,4,6],\n",
    "        'min_samples_split': [2,4,6],\n",
    "        'min_samples_leaf':[1],  # 2,3],\n",
    "        'max_features': ['auto'], # 'log2', None],\n",
    "        'max_leaf_nodes': [None], # 20, 10, 5],\n",
    "        'max_samples':[None], # 0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    }\n",
    "    \n",
    "    oob_dict = {}\n",
    "    n=1\n",
    "    keys = list(grid.keys())\n",
    "    for i, key in enumerate(keys[:3]):\n",
    "        # loop over all values in key\n",
    "        for value in grid[key]:\n",
    "            for key2 in keys[i+1:3]:\n",
    "                for value2 in grid[key2]:\n",
    "                    for key3 in keys[i+2:3]:\n",
    "                        for value3 in grid[key3]:\n",
    "                            \n",
    "                            print('konf', n, key, value, key2, value2, key3, value3)\n",
    "                \n",
    "                            meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                                  n_estimators = value,\n",
    "                                                  max_depth = value2,\n",
    "                                                  min_samples_split = value3,\n",
    "                                                  min_samples_leaf = 1,  # 2,3], \n",
    "                                                  max_features = 'auto', # 'log2', None], \n",
    "                                                  max_leaf_nodes = None, # 20, 10, 5],  \n",
    "                                                  max_samples    = None,)\n",
    "                            # keep n:oob_score in dict\n",
    "                            oob_dict[n] = meta_model.oob_score_\n",
    "        \n",
    "                            n+=1\n",
    "    print(oob_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print key where max value in oob_dict\n",
    "# print(max(oob_dict, key=oob_dict.get))  \n",
    "print('Manuellt hittade jag: n_estimators=100, max_depth=None,min_samples_split=6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final: learn meta_model med optimala parms och learn allt för modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting meta_model on X with all models predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_score 0.9262501658044834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    #####    X_stack och y_stack måste vara definierade ovan   #####\n",
    "    meta_model = learn_meta_model(X_stack, y_stack, save=True,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=None,\n",
    "                                  min_samples_split=6,\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ### alla värden enligt ovan körningar för modellerna ###\n",
    "    learn_all_models(totals=True, save=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktioner för att prioritera mellan hästar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se även kelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för en omgång (ett datum) ta ut största diff för streck per avd \n",
    "# om only_clear=True, enbart för diff >= 25\n",
    "def lista_med_favoriter(df_, ant, only_clear):\n",
    "    df = df_.copy()\n",
    "    min_diff = 25 if only_clear else 0\n",
    "    # sortera på avd,streck\n",
    "    df = df.sort_values(['avd', 'streck'], ascending=[False, False])\n",
    "    diff_list = []\n",
    "    for avd in range(1, 8):\n",
    "        diff = df.loc[df.avd == avd].streck.iloc[0] - \\\n",
    "            df.loc[df.avd == avd].streck.iloc[1]\n",
    "        if diff >= min_diff:\n",
    "            diff_list.append((avd, diff))\n",
    "\n",
    "     # sortera på diff\n",
    "    diff_list = sorted(diff_list, key=lambda x: x[1], reverse=True)\n",
    "    return diff_list[:ant]\n",
    "\n",
    "# temp is a list of tuples (avd, diff). check if avd is in the list\n",
    "def check_avd(avd, temp):\n",
    "    for t in temp:\n",
    "        if t[0] == avd:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_insats(df):\n",
    "    insats = 0\n",
    "    # group by avd\n",
    "    summa = df.groupby('avd').avd.count().prod() / 2\n",
    "    return summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning-fasen\n",
    "Bygg en separat v75_learning_ny.ipynb   \n",
    "Bygg en separat v75_validate_ny.ipynb  \n",
    "Bygg en separat v75_hyperparms.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gör en scrape på senaste veckan (behövs inte i denna test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum < test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "# _, rf = model_streck_to_odds(X)   # used in next cell\n",
    "\n",
    "# # spara rf\n",
    "# import pickle\n",
    "# with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "#     pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all_data.csv \n",
    "Baka ihop senaste vekan med all_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skapa_stack_learning(X_, y, features, iterations=1000, random_state=2022, verbose=False, save=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack med proba och kelly\n",
    "    X måste ha datum och avd\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    \n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        model = typ.learn(X, y, features, iterations=iterations, save=save, verbose=verbose)\n",
    "        stacked_data['proba'+nr] = typ.predict(X) \n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "    \n",
    "    # print(stacked_data.columns)\n",
    "    return stacked_data   # enbart stack-info\n",
    "\n",
    "# fit meta_model\n",
    "def learn_meta_model(X,y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print('\\nFitting meta_model on X with all models predictions')\n",
    "    \n",
    "    meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "    meta_model.fit(X, y)\n",
    "    \n",
    "    print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "    # pickle save stacking\n",
    "    pickle.dump(meta_model, open('..\\\\modeller\\\\meta.model', 'wb'))\n",
    "    \n",
    "    return meta_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature list from a file (ej plac)\n",
    "def read_feature_list(file='../FEATURES.txt'):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liten_cv_timeseries_demo(x_,y_):\n",
    "    X = x_.copy()\n",
    "    y = y_.copy()\n",
    "    print('Holdout validation data from X = X[~validation]')\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        print(f\"TRAIN each model on this: {train_index[0]}-{train_index[-1]}, Predict on: {test_index[0]}-{test_index[-1]}\")\n",
    "        print('save the predictions of each model')\n",
    "        X_train = X.loc[train_index]\n",
    "        X_test = X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    print('Train meta_model on all models predictions')\n",
    "    print('validate meta_model on the validation data')\n",
    "    \n",
    "    print('\\nHandle stratified and unbalanced data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör learning-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = read_feature_list(\"../FEATURES.txt\")\n",
    "\n",
    "print(\"hur mycket skall sparas för input till learning meta-model?\")\n",
    "print('kör vs.v75_sraping')\n",
    "print('tvätta data')\n",
    "X, y = läs_in_data_och_förbered()\n",
    "print(' concat med all_data')\n",
    "\n",
    "assert X.shape[1] == len(FEATURES), f'X_train.shape[1] {X.shape[1]} != len(FEATURES) {len(FEATURES)}'\n",
    "assert set(X.columns) == set(FEATURES), f'set(X_train.columns) {set(X.columns)} != set(FEATURES) {set(FEATURES)}'\n",
    "X = X[FEATURES]  # för att få kolumner i rätt ordning\n",
    "andel_train = 0.60\n",
    "andel_meta_train = 0.75\n",
    "alla_datum = X.datum.unique()\n",
    "train_datum=alla_datum[:int(len(alla_datum)*andel_train)]\n",
    "test_datum=alla_datum[int(len(alla_datum)*andel_train):]\n",
    "meta_train = test_datum[:int(len(test_datum)*andel_meta_train)]\n",
    "meta_test = test_datum[int(len(test_datum)*andel_meta_train):]\n",
    "print(f'alla_datum {len(alla_datum)} varav train_datum {len(train_datum)} och test_datum {len(test_datum)}')\n",
    "print(f'meta_train {len(meta_train)} och meta_test {len(meta_test)}')\n",
    "\n",
    "if True:\n",
    "    liten_cv_timeseries_demo(X, y)\n",
    "else:    \n",
    "    X_stacked = skapa_stack_learning(X_train, y_train, FEATURES, iterations=100,random_state=2022, verbose=False, save=True)\n",
    "    # display(X_stacked)\n",
    "    meta_model = learn_meta_model(X_stacked, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spela-fasen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "import V75_scraping as vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape-funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v75_scrape():\n",
    "    df, strukna = vs.v75_scraping(history=True, resultat=False)\n",
    "    \n",
    "    # df = pd.read_csv('../sparad_scrape.csv')\n",
    "    for f in ['häst','bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ metod\n",
    "# Ta fram rader för varje typ enligt test-resultaten innan\n",
    "# låt meta_model välja mellan typerna - hur? Hur maximer insatsen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som bygger stack-data från modellerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# för stacking ta med alla hästar per typ och proba plus kelly\n",
    "def build_stack_df(X_):\n",
    "    X = X_.copy()\n",
    "    stacked_data = X[['datum','avd', 'startnr','häst']].copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly'+nr] = kelly(stacked_data['proba'+nr], X[['streck']], None)\n",
    "    return stacked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion där meta_model gör predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_predict(X_):\n",
    "    # X_ innehåller även datum,startnr och avd\n",
    "    extra = ['datum', 'avd', 'startnr', 'häst']\n",
    "    assert list(X_.columns[:4]) == extra, 'meta_model måste ha datum, avd och startnr, häst för att kunna välja'\n",
    "    X = X_.copy()\n",
    "    with open('../modeller\\\\meta.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "        \n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict_proba(X.iloc[:,-8:])[:,1]\n",
    "    my_columns = extra + list(X.columns)[-9:] \n",
    "    \n",
    "    return X[my_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion som väljer rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_cost(antal_rader):\n",
    "    cost = (antal_rader**2)/2\n",
    "    return antal_rader,cost\n",
    "\n",
    "def välj_rad(X_):\n",
    "    \n",
    "    max_insats=320\n",
    "    veckans_rad = X_.copy()\n",
    "    veckans_rad['välj'] = False\n",
    "\n",
    "    for avd in veckans_rad.avd.unique():\n",
    "        max_pred = veckans_rad[veckans_rad.avd == avd]['meta_predict'].max()\n",
    "        veckans_rad.loc[(veckans_rad.avd == avd) & (veckans_rad.meta_predict == max_pred), 'välj'] = True\n",
    "    antal_rader=1    \n",
    "    veckans_rad = veckans_rad.sort_values(by=['meta_predict'], ascending=False)\n",
    "    \n",
    "    # 3. Använda ensam favorit för ett par avd? Kolla test-resultat\n",
    "    # for each row in rad, välj=True if select_func(cost,avd) == True\n",
    "    cost = antal_rader*0.5\n",
    "    for i, row in veckans_rad.iterrows():\n",
    "        new_antal,new_cost = comp_cost(antal_rader+1)\n",
    "        # print(the_cost)\n",
    "        if new_cost > max_insats:\n",
    "            break\n",
    "        \n",
    "        antal_rader = new_antal\n",
    "        cost = new_cost\n",
    "        veckans_rad.loc[i, 'välj'] = True\n",
    "        # print(cost)\n",
    "    veckans_rad.sort_values(by=['välj', 'avd'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    return veckans_rad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kör hela välj-rad-skiten här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = v75_scrape()\n",
    "print(X.datum.unique())\n",
    "df_stack = build_stack_df(X,FEATURES)\n",
    "df_meta = meta_predict(df_stack)\n",
    "df_meta.reset_index(drop=True, inplace=True)\n",
    "veckans_rad = välj_rad(df_meta)\n",
    "# rename columns \n",
    "veckans_rad.rename(columns={'startnr':'nr', 'meta_predict':'Meta', 'välj':'Välj'}, inplace=True)\n",
    "\n",
    "display(veckans_rad[veckans_rad.välj])\n",
    "print('kostnad', veckans_rad.välj.sum()**2/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En massa gammal - kanske reusable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for Learn!\n",
    "def init_learn():\n",
    "    df = pd.read_csv('..\\\\all_data.csv')\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    # X = ordinal_enc(X, 'häst')\n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X, cat_features = tp.prepare_for_catboost(X)\n",
    "    print('cat_features:', cat_features)\n",
    "    X.head()\n",
    "    return X, y, cat_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modell för streck_to_odds - skall vara fix och inte ändras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_streck_to_odds(X_):\n",
    "    X = X_.copy()\n",
    "    # import modules for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    # import random forest module\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    X_odds = X.loc[X.vodds <= 40]  # remove outliers\n",
    "    ix_break = int(len(X_odds.datum.unique())*0.75)\n",
    "    test_start = X_odds.datum.unique()[ix_break]\n",
    "\n",
    "    X_train, X_test = X_odds[X_odds.datum <\n",
    "                             test_start], X_odds[X_odds.datum >= test_start]\n",
    "    y_train, y_test = X_train['vodds'], X_test['vodds']\n",
    "    X_train = X_train[['streck']].astype(float)\n",
    "    X_test = X_test[['streck']].astype(float)\n",
    "\n",
    "    # make a model of RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predrf = rf.predict(X_test)\n",
    "    # make a model and fit it\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_predlr = linreg.predict(X_test)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients:', linreg.coef_)\n",
    "    # print the mean absolute error\n",
    "    print(\"LR Mean absolute error: %.2f\" % mae(y_test, y_predlr))\n",
    "    print(\"RF Mean absolute error: %.2f\" % mae(y_test, y_predrf))\n",
    "\n",
    "    return linreg, rf\n",
    "\n",
    "\n",
    "linreg, rf = model_streck_to_odds(X)   # used in next cell\n",
    "# spara rf\n",
    "import pickle\n",
    "with open('rf_streck_odds.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engångsgrej för att initiera typ-instanserna med learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bara första gången. Initierar Typ-klassen\n",
    "def learn(X_train, y_train, X_test=None, y_test=None, cat_features=[],  iterations=1000,  verbose=False):\n",
    "    cbc = CatBoostClassifier(iterations=iterations, loss_function='Logloss', eval_metric='AUC', verbose=verbose)\n",
    "    X_train = remove_features(X_train, remove_mer=['avd','datum'])\n",
    "    cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "    if X_test is not None:\n",
    "        X_test = remove_features(X_test, remove_mer=['avd', 'datum'])\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "        cbc.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100, use_best_model=True, verbose=verbose)\n",
    "    else:\n",
    "        cbc.fit(train_pool, use_best_model=True, verbose=verbose)\n",
    "    return cbc\n",
    "\n",
    "def beräkna_datum(X,fract=0.75):\n",
    "    ix_break = int(len(X.datum.unique())*fract)\n",
    "    test_start = X.datum.unique()[ix_break]\n",
    "    return test_start\n",
    "\n",
    "if False: # Kör en initiering av typerna \n",
    "    Xlearn, cat_features= prepare_for_catboost(X)  \n",
    "    # print(Xlearn.columns)\n",
    "    for typ in [typ6, typ1, typ9, typ16]:\n",
    "        print(typ.name)\n",
    "        Xtyp = typ.prepare_for_model(Xlearn)                                 ###########\n",
    "\n",
    "        if not typ.streck:                                                ################\n",
    "            Xtyp.drop('streck', axis=1, inplace=True)\n",
    "            \n",
    "        if True: # använda X_test    \n",
    "            test_start = beräkna_datum(Xtyp)    \n",
    "            X_train, X_test = Xtyp[Xtyp.datum < test_start], Xtyp[Xtyp.datum >= test_start]\n",
    "            y_train, y_test = y[X_train.index], y[X_test.index]\n",
    "            # print('innan learn',X_train.columns)\n",
    "            typ_model = learn(X_train, y_train, X_test, y_test)  ##########\n",
    "            print('best iteration',typ_model.best_iteration_)                             ##########\n",
    "            print('best score',    typ_model.best_score_)                                 ##########\n",
    "        # save model\n",
    "        typ.save_model(typ_model)                                                       ##########                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1e00af82eb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = läs_in_data_och_förbered()\n",
    "X,cat_features = tp.prepare_for_catboost(X)\n",
    "typ6.learn(X,y, iterations=33) # best iter = 25 {'Logloss': 0.23245952928761984, 'AUC': 0.8262112132692319}\n",
    "typ1.learn(X,y, iterations=39) # best iter = 39 {'Logloss': 0.23278308932319106, 'AUC': 0.826883367187688}\n",
    "typ9.learn(X,y, iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}\n",
    "# typ16.learn(X,y,iterations=37) # best iter = 37 {'Logloss': 0.23312091900160384, 'AUC': 0.8257515762557716}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapa stack predict med alla typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack predict for all models\n",
    "def stack_predict(X_, models):\n",
    "    X = X_.copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        X['proba'+nr] = typ.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "        \n",
    "    # cols=X.columns[-8]    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete learning process with all steps in stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()  # The meta model\n",
    "    \n",
    "# fit my models on split date for timeseries   \n",
    "print('START fitting and predicting TimeseriesSplit') \n",
    "\n",
    "\n",
    "cross_val_predict=pd.DataFrame()\n",
    "for id_train, id_test in TimeSeriesSplit(n_splits=5).split(df_stack):  \n",
    "    for typ in typer:\n",
    "        typ.learn(df_stack.loc[id_train],y.loc[id_train], iterations=25)\n",
    "    df_pred = stack_predict(df_stack.loc[id_test], [typ6, typ1, typ9])\n",
    "    df_pred['y']=y.loc[id_test]\n",
    "    cross_val_predict = pd.concat([cross_val_predict, df_pred.iloc[:,-9:]])\n",
    "       \n",
    "print('\\nFitting my models with df_stack')\n",
    "# final fit with all the available data\n",
    "for typ in [typ6, typ1, typ9]:\n",
    "    typ.learn(df_stack, y, iterations=20)\n",
    "\n",
    "print('\\nFitting meta_model on predicted above')\n",
    "# fit a rf meta_model on cross_val_predict\n",
    "meta_model = RandomForestClassifier(max_depth=None, n_estimators=100, oob_score=True, verbose=1, n_jobs=10, random_state=2022)\n",
    "meta_model.fit(cross_val_predict.iloc[:, :-1], cross_val_predict.iloc[:, -1])\n",
    "print('OOB_score', meta_model.oob_score_)   # 0.9305314451043094\n",
    "# pickle save stacking\n",
    "pickle.dump(meta_model, open('..\\\\modeller\\\\meta_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on unseen data\n",
    "def unseen_predictions(X_, models, meta_model):\n",
    "    X = X_.copy()\n",
    "    for model in models:\n",
    "        nr = model.name[3:]\n",
    "        X['proba'+nr] = model.predict(X)\n",
    "        X['kelly'+nr] = kelly(X['proba'+nr], X[['streck']], None)\n",
    "        \n",
    "    return(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "\n",
    "# a small test:\n",
    "unseen_predictions(df_stack.iloc[-80:,:], typer, meta_model)[:,1],y.iloc[-80:].values\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
