{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# Detta √§r en kopia av 1_üêé_v75.py\n",
    "##########################################################################################\n",
    "\n",
    "#%%\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "import logging\n",
    "import typ as tp\n",
    "import V75_scraping as vs\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import sys\n",
    "# import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "import sklearn\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\modeller\\\\')\n",
    "pref = ''\n",
    "logging.basicConfig(filename='app.log', filemode='w',\n",
    "                    format='%(name)s - %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√§r f√∂ljer streamlit grejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "#  st.set_page_config(page_title=\"v75 Spel\", page_icon=\"üêé\")\n",
    "#  st.sidebar.header(\"üêé V75 Spel\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ tas bort ############\n",
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def v75_scraping(full=True):\n",
    "    if not full:\n",
    "        # st.write(\"ANV√ÑNDER SPARAT\")\n",
    "        df = pd.read_csv('sparad_scrape_spela.csv')\n",
    "        try:\n",
    "            df.drop(['plac'], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        print('start vs.v75_scraping')\n",
    "        df = vs.v75_scraping(resultat=False, history=True, headless=True)\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n",
    "\n",
    "    # Alternativ metod\n",
    "    # Ta fram rader f√∂r varje typ enligt test-resultaten innan\n",
    "    # l√•t meta_model v√§lja mellan typerna tills max insats, sorterat p√• meta_proba\n",
    "\n",
    "# Funktioner f√∂r att prioritera mellan h√§star\n",
    "# Skapa ett Kelly-v√§rde baserat p√• streck omvandlat till odds\n",
    "\n",
    "\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open('rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n",
    "\n",
    "# f√∂r en omg√•ng (ett datum) ta ut st√∂rsta diff f√∂r streck per avd\n",
    "# om only_clear=True, enbart f√∂r diff >= 25\n",
    "\n",
    "\n",
    "def lista_med_favoriter(df_, ant, only_clear):\n",
    "    df = df_.copy()\n",
    "    min_diff = 25 if only_clear else 0\n",
    "    # sortera p√• avd,streck\n",
    "    df = df.sort_values(['avd', 'streck'], ascending=[False, False])\n",
    "    diff_list = []\n",
    "    for avd in range(1, 8):\n",
    "        diff = df.loc[df.avd == avd].streck.iloc[0] - \\\n",
    "            df.loc[df.avd == avd].streck.iloc[1]\n",
    "        if diff >= min_diff:\n",
    "            diff_list.append((avd, diff))\n",
    "\n",
    "     # sortera p√• diff\n",
    "    diff_list = sorted(diff_list, key=lambda x: x[1], reverse=True)\n",
    "    return diff_list[:ant]\n",
    "\n",
    "# temp is a list of tuples (avd, diff). check if avd is in the list\n",
    "\n",
    "\n",
    "def check_avd(avd, temp):\n",
    "    for t in temp:\n",
    "        if t[0] == avd:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def compute_total_insats(df):\n",
    "    summa = df.groupby('avd').avd.count().prod() / 2\n",
    "    return summa\n",
    "\n",
    "# feature med antal h√§star per avdeling\n",
    "\n",
    "\n",
    "def l√§gg_in_antal_h√§star(df_):\n",
    "    df = df_.copy()\n",
    "    df['ant_per_lopp'] = None\n",
    "    df['ant_per_lopp'] = df.groupby(['datum', 'avd'])['avd'].transform('count')\n",
    "    return df\n",
    "\n",
    "# r√§kna ut mest streck per avdeling\n",
    "\n",
    "\n",
    "def mest_streck(X_, i, datum, avd):\n",
    "    X = X_.copy()\n",
    "    X.sort_values(by=['datum', 'avd', 'streck'], ascending=[\n",
    "                  True, True, False], inplace=True)\n",
    "    return X.loc[(X.datum == datum) & (X.avd == avd), 'streck'].iloc[i]\n",
    "\n",
    "# n flest streck per avd som features\n",
    "\n",
    "\n",
    "def l√§gg_in_motst√•ndare(X_, ant_motst√•ndare):\n",
    "    X = X_.copy()\n",
    "\n",
    "    # set X['motst√•ndare1'] to largest streck in every avd\n",
    "    grouped = X.groupby(['datum', 'avd'])['streck']\n",
    "    X['motst√•ndare1'] = grouped.transform(max)\n",
    "\n",
    "    for i in range(2, ant_motst√•ndare+1):\n",
    "        # set X['motst√•ndare'+str(i)] to ith largest streck in every avd\n",
    "        X['motst√•ndare' +\n",
    "            str(i)] = grouped.transform(lambda x: x.nlargest(i).min())\n",
    "\n",
    "    return X\n",
    "\n",
    "# som f√∂reg√•ende men med diff istf faktiska v√§rden\n",
    "\n",
    "\n",
    "def l√§gg_in_diff_motst√•ndare(X_, motst√•ndare):\n",
    "    X = X_.copy()\n",
    "\n",
    "    # set X['motst√•ndare1'] to largest streck in every avd\n",
    "    grouped = X.groupby(['datum', 'avd'])['streck']\n",
    "    X['diff1'] = grouped.transform(max) - X.streck\n",
    "\n",
    "    for i in range(2, motst√•ndare+1):\n",
    "        # set X['motst√•ndare'+str(i)] to ith largest streck in every avd\n",
    "        X['diff' +\n",
    "            str(i)] = grouped.transform(lambda x: x.nlargest(i).min()) - X.streck\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "#%%\n",
    "# skapa modeller\n",
    "#             name,  ant_√§star,  proba, kelly, motst_ant,   motst_diff,  ant_favoriter,  only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,\n",
    "              False,          0,            False,    True)\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,\n",
    "              True,           2,            True,     False)\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,\n",
    "              True,           2,            True,     True)\n",
    "# typ16= tp.Typ('typ16', True,      True, True,      2,          True,          2,            False,    True)\n",
    "\n",
    "typer = [typ6, typ1, typ9]  # load a file with pickl\n",
    "\n",
    "with open('modeller\\\\meta_rf_model.model', 'rb') as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "# with open('modeller\\\\meta_ridge_model.model', 'rb') as f:\n",
    "#    meta_model = pickle.load(f)\n",
    "\n",
    "# with open('modeller\\\\meta_lasso_model.model', 'rb') as f:\n",
    "#     meta_model = pickle.load(f)\n",
    "\n",
    "\n",
    "#%%\n",
    "# f√∂r stacking ta med alla h√§star per typ och proba plus kelly\n",
    "def build_stack_df(X_, typer):\n",
    "    X = X_.copy()\n",
    "    first_features = ['datum', 'avd', 'startnr', 'h√§st']\n",
    "    stacked_data = X[first_features].copy()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        # print('stack '+typ.name)\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly' +\n",
    "                     nr] = kelly(stacked_data['proba'+nr], X[['streck']], None)\n",
    "\n",
    "    with open(pref+'META_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        meta_features = f.read().splitlines()\n",
    "    stacked_data = stacked_data[first_features + meta_features]\n",
    "\n",
    "    return stacked_data\n",
    "\n",
    "\n",
    "def meta_knn_predict(X_):\n",
    "    # X_ inneh√•ller √§ven datum,startnr och avd\n",
    "    first_features = ['datum', 'avd', 'startnr', 'h√§st']\n",
    "    pred_columns = ['proba'+str(i) for i in [6, 1, 9]] + \\\n",
    "        ['kelly'+str(i) for i in [6, 1, 9]]\n",
    "\n",
    "    X = X_.copy()\n",
    "    assert list(\n",
    "        X.columns[:4]) == first_features, 'meta_model m√•ste ha datum, avd och startnr, h√§st f√∂r att kunna v√§lja'\n",
    "    with open('modeller\\\\meta_knn_model.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "\n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict_proba(X[pred_columns])[:, 1]\n",
    "    my_columns = first_features + pred_columns + ['meta_predict']\n",
    "\n",
    "    return X[my_columns]\n",
    "\n",
    "\n",
    "def meta_rf_predict(X_):\n",
    "    # X_ inneh√•ller √§ven datum,startnr och avd\n",
    "    first_features = ['datum', 'avd', 'startnr', 'h√§st']\n",
    "    pred_columns = ['proba'+str(i) for i in [6, 1, 9]] + \\\n",
    "        ['kelly'+str(i) for i in [6, 1, 9]]\n",
    "\n",
    "    X = X_.copy()\n",
    "    assert list(\n",
    "        X.columns[:4]) == first_features, 'meta_model m√•ste ha datum, avd och startnr, h√§st f√∂r att kunna v√§lja'\n",
    "    with open('modeller\\\\meta_rf_model.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "\n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict_proba(X[pred_columns])[:, 1]\n",
    "    my_columns = first_features + pred_columns + ['meta_predict']\n",
    "\n",
    "    return X[my_columns]\n",
    "\n",
    "\n",
    "def meta_ridge_predict(X_):\n",
    "    # X_ inneh√•ller √§ven datum,startnr och avd\n",
    "    first_features = ['datum', 'avd', 'startnr', 'h√§st']\n",
    "    pred_columns = ['proba'+str(i) for i in [6, 1, 9]] + \\\n",
    "        ['kelly'+str(i) for i in [6, 1, 9]]\n",
    "\n",
    "    assert list(\n",
    "        X_.columns[:4]) == first_features, 'meta_model m√•ste ha datum, avd och startnr, h√§st f√∂r att kunna v√§lja'\n",
    "    X = X_.copy()\n",
    "    with open('modeller\\\\meta_ridge_model.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "\n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model._predict_proba_lr(X[pred_columns])[:, 1]\n",
    "    my_columns = first_features + pred_columns + ['meta_predict']\n",
    "\n",
    "    return X[my_columns]\n",
    "\n",
    "\n",
    "def meta_lasso_predict(X_):\n",
    "    # X_ inneh√•ller √§ven datum,startnr och avd\n",
    "    first_features = ['datum', 'avd', 'startnr', 'h√§st']\n",
    "    pred_columns = ['proba'+str(i) for i in [6, 1, 9]] + \\\n",
    "        ['kelly'+str(i) for i in [6, 1, 9]]\n",
    "\n",
    "    assert list(\n",
    "        X_.columns[:4]) == first_features, 'meta_model m√•ste ha datum, avd och startnr, h√§st f√∂r att kunna v√§lja'\n",
    "    X = X_.copy()\n",
    "    with open('modeller\\\\meta_lasso_model.model', 'rb') as f:\n",
    "        meta_model = pickle.load(f)\n",
    "\n",
    "    # print(meta_model.predict_proba(X.iloc[:, -8:]))\n",
    "    X['meta_predict'] = meta_model.predict(X[pred_columns])\n",
    "    my_columns = first_features + pred_columns + ['meta_predict']\n",
    "\n",
    "    return X[my_columns]\n",
    "\n",
    "\n",
    "def mesta_diff_per_avd(X_):\n",
    "    sm = X_.copy()\n",
    "    # select the highest meta_predict per avd\n",
    "    sm['first'] = sm.groupby('avd')['meta_predict'].transform(\n",
    "        lambda x: x.nlargest(2).reset_index(drop=True)[0])\n",
    "    sm['second'] = sm.groupby('avd')['meta_predict'].transform(\n",
    "        lambda x: x.nlargest(2).reset_index(drop=True)[1])\n",
    "\n",
    "    sm = sm.query(\"(first==meta_predict or second==meta_predict)\").copy()\n",
    "    sm['diff'] = sm['first'] - sm['second']\n",
    "\n",
    "    # drop duplicates per avd\n",
    "    sm = sm.drop_duplicates(subset='avd', keep='first')\n",
    "\n",
    "    sm.sort_values(by='diff', ascending=False, inplace=True)\n",
    "    # sm.to_csv('mesta_diff_per_avd.csv')\n",
    "    return sm\n",
    "\n",
    "\n",
    "def v√§lj_rad(df_meta_predict, max_insats=300):\n",
    "    veckans_rad = df_meta_predict.copy()\n",
    "    veckans_rad['v√§lj'] = False   # inga rader valda √§nnu\n",
    "\n",
    "    # first of all: select one horse per avd\n",
    "    for avd in veckans_rad.avd.unique():\n",
    "        max_pred = veckans_rad[veckans_rad.avd == avd]['meta_predict'].max()\n",
    "        veckans_rad.loc[(veckans_rad.avd == avd) & (\n",
    "            veckans_rad.meta_predict == max_pred), 'v√§lj'] = True\n",
    "    # veckans_rad.query(\"v√§lj==True\").to_csv('veckans_basrad.csv')\n",
    "    veckans_rad = veckans_rad.sort_values(by=['meta_predict'], ascending=False)\n",
    "    veckans_rad = veckans_rad.reset_index(drop=True)\n",
    "\n",
    "    mest_diff = mesta_diff_per_avd(veckans_rad)\n",
    "\n",
    "    cost = 0.5  # 1 rad\n",
    "\n",
    "    # now select the rest of the horses one by one sorted by meta_predict\n",
    "    for i, row in veckans_rad.iterrows():\n",
    "        if row.avd == mest_diff.avd.iloc[0]:\n",
    "            continue\n",
    "        if row.avd == mest_diff.avd.iloc[1]:\n",
    "            continue\n",
    "        # print('i',i)\n",
    "        veckans_rad.loc[i, 'v√§lj'] = True\n",
    "        cost = compute_total_insats(veckans_rad[veckans_rad.v√§lj])\n",
    "        # print('cost',cost)\n",
    "        if cost > max_insats:\n",
    "            # veckans_rad.loc[i, 'v√§lj'] = False\n",
    "            break\n",
    "\n",
    "    # print('cost', cost_before)\n",
    "    veckans_rad.sort_values(by=['v√§lj', 'avd'], ascending=[\n",
    "                            False, True], inplace=True)\n",
    "    # display(veckans_rad[veckans_rad.v√§lj])\n",
    "    return veckans_rad\n",
    "\n",
    "\n",
    "#%%\n",
    "#############################\n",
    "#### l√§s in meta_scores  ####\n",
    "#############################\n",
    "try:\n",
    "    with open(pref+'modeller/meta_scores.pkl', 'rb') as f:\n",
    "        meta_scores = pickle.load(f)\n",
    "except:\n",
    "    st.write('No meta_scores.pkl found')\n",
    "    print('No meta_scores.pkl found')\n",
    "    meta_scores = {'knn': 0.6, 'rf': 0.4, 'ridge': 0.7, 'lasso': 0.8}\n",
    "# print('meta_scores:', meta_scores)\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "def sort_list_of_meta(m):\n",
    "    try:\n",
    "        if meta_scores[m] == None:\n",
    "            print(f'No score for {m} found')\n",
    "            return 0\n",
    "        return meta_scores[m]\n",
    "    except:\n",
    "        st.write(f'{m} not found')\n",
    "        print(f'{m} not found')\n",
    "        return -1\n",
    "\n",
    "\n",
    "#%%\n",
    "## Streamlit kod startar h√§r\n",
    "v75 = st.container()\n",
    "scraping = st.container()\n",
    "avd = st.container()\n",
    "sortera = st.container()\n",
    "\n",
    "if 'datum' in st.session_state:\n",
    "    datum = st.session_state['datum']\n",
    "    year = int(datum[:4])\n",
    "    month = int(datum[5:7])\n",
    "    day = int(datum[8:])\n",
    "    datum = st.sidebar.date_input(\n",
    "        'V√§lj datum', datetime.date(year, month, day))\n",
    "    datum = datum.strftime('%Y-%m-%d')\n",
    "\n",
    "    if datum != st.session_state['datum']:\n",
    "        st.session_state['datum'] = datum\n",
    "        datum = \"https://www.atg.se/spel/\"+datum+\"/V75/\"\n",
    "        omg_df = pd.DataFrame([datum], columns=['Link'])\n",
    "        omg_df.to_csv('omg_att_spela_link.csv', index=False)\n",
    "\n",
    "\n",
    "# typ16 och typ9 √§r samma f√∂rutom hur man v√§ljer rader\n",
    "models = [typ6, typ1, typ9]\n",
    "\n",
    "\n",
    "def use_meta(df_stack, meta):\n",
    "    if meta == 'knn':\n",
    "        df_meta = meta_knn_predict(df_stack)\n",
    "    elif meta == 'rf':\n",
    "        df_meta = meta_rf_predict(df_stack)\n",
    "    elif meta == 'lasso':\n",
    "        df_meta = meta_lasso_predict(df_stack)\n",
    "    elif meta == 'ridge':\n",
    "        df_meta = meta_ridge_predict(df_stack)\n",
    "    else:\n",
    "        st.error(f'meta={meta} finns inte - av√§nder RandomForestClassifier')\n",
    "        df_meta = meta_rf_predict(df_stack)\n",
    "\n",
    "    df_meta.reset_index(drop=True, inplace=True)\n",
    "    df = v√§lj_rad(df_meta)\n",
    "    st.session_state.df = df\n",
    "    st.experimental_rerun()\n",
    "\n",
    "\n",
    "# define st.state\n",
    "if 'df' not in st.session_state:\n",
    "    st.session_state['df'] = None\n",
    "    print(\"sklearn version\", sklearn.__version__)\n",
    "\n",
    "if 'meta' not in st.session_state:\n",
    "    st.session_state['meta'] = 'rf'\n",
    "\n",
    "with scraping:\n",
    "    def scrape(full=True, meta='rf'):\n",
    "        scraping.write('web-scraping f√∂r ny data')\n",
    "        with st.spinner('Ta det lugnt!'):\n",
    "            # st.image('winning_horse.png')  # ,use_column_width=True)\n",
    "\n",
    "            #####################\n",
    "            # start v75_scraping as a thread\n",
    "            #####################\n",
    "\n",
    "            i = 0.0\n",
    "            seconds = 0\n",
    "            placeholder = st.empty()\n",
    "\n",
    "            my_bar = st.progress(i)\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(v75_scraping, full)\n",
    "                while future.running():\n",
    "                    time.sleep(1)\n",
    "                    seconds += 1\n",
    "                    placeholder.write(f\"‚è≥ {seconds} sekunder\")\n",
    "                    i += 1/65\n",
    "                    if i < 0.99:\n",
    "                        my_bar.progress(i)\n",
    "                my_bar.progress(1.0)\n",
    "                df_scraped = future.result()\n",
    "\n",
    "                df_scraped.to_csv('sparad_scrape_spela.csv', index=False)\n",
    "\n",
    "            st.balloons()\n",
    "            my_bar.empty()\n",
    "            placeholder.empty()\n",
    "            # print(df_scraped.datum.unique())\n",
    "            df_stack = build_stack_df(df_scraped, typer)\n",
    "            df_stack.to_csv('sparad_stack.csv', index=False)\n",
    "            use_meta(df_stack, meta)\n",
    "\n",
    "    col1, col2 = st.columns([1, 4])\n",
    "\n",
    "    do_scraping = False\n",
    "    with col1:\n",
    "        if st.button('scrape'):\n",
    "            do_scraping = True\n",
    "    with col2:\n",
    "        if st.button('reuse scrape'):\n",
    "            try:\n",
    "                df = pd.read_csv('sparad_scrape_spela.csv')\n",
    "\n",
    "                if df.datum.iloc[0] != st.session_state.datum:\n",
    "                    st.error(\n",
    "                        f'Datum i data = {df.datum.iloc[0]} \\n\\n √§r inte samma som i omg√•ng')\n",
    "                else:\n",
    "                    # st.success(f'inl√§st data med datum = {temp_df.datum.iloc[0]}')\n",
    "                    st.info(\n",
    "                        f'inl√§st data med datum = {df.datum.iloc[0]} k√∂r nu scrape med full=False')\n",
    "                    try:\n",
    "                        df.drop(['plac'], axis=1, inplace=True)\n",
    "                    except:\n",
    "                        pass\n",
    "                    # scrape(False, meta=st.session_state['meta'])\n",
    "                    # st.info('scrape klar')\n",
    "                    del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "            except:\n",
    "                # write error message\n",
    "                st.error('<Dey finns ingen sparad data')\n",
    "\n",
    "    if do_scraping:\n",
    "        scrape(meta=st.session_state['meta'])\n",
    "        del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "\n",
    "    scraping.empty()\n",
    "\n",
    "with v75:\n",
    "    if 'datum' not in st.session_state:\n",
    "        omg_df = pd.read_csv('omg_att_spela_link.csv')\n",
    "        urlen = omg_df.Link.values[0]\n",
    "        datum = urlen.split('spel/')[1][0:10]\n",
    "        st.session_state.datum = datum\n",
    "\n",
    "    st.title('üêé v75 -  ' + st.session_state.datum)\n",
    "\n",
    "\n",
    "with avd:\n",
    "    if st.session_state.df is not None:\n",
    "        use = avd.radio('V√§lj avdelning', ('Avd 1 och 2',\n",
    "                        'Avd 3 och 4', 'Avd 5 och 6', 'Avd 7', 'clear'))\n",
    "        avd.subheader(use)\n",
    "        st.write('TA BORT OUTLIERS')\n",
    "        col1, col2 = st.columns(2)\n",
    "        # print(df.iloc[0].h√§st)\n",
    "        dfi = st.session_state.df\n",
    "        dfi.rename(columns={'startnr': 'nr',\n",
    "                   'meta_predict': 'Meta'}, inplace=True)\n",
    "        try:\n",
    "            dfi['kelly'] = (dfi[['kelly1', 'kelly6', 'kelly9']]).max(axis=1)\n",
    "        except:\n",
    "            # om 'kelly-kolumnen saknas' s√• skapas den\n",
    "            scrape(False, meta=st.session_state['meta'])\n",
    "            del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "\n",
    "        # print(dfi[dfi.v√§lj][['avd','nr','h√§st','kelly1','kelly6','kelly9','kelly16','Meta']])\n",
    "        # CSS to inject contained in a string\n",
    "        hide_dataframe_row_index = \"\"\"\n",
    "            <style>\n",
    "            .row_heading.level0 {display:none}\n",
    "            .blank {display:none}\n",
    "            </style>\n",
    "            \"\"\"\n",
    "\n",
    "        # Inject CSS with Markdown\n",
    "        st.markdown(hide_dataframe_row_index, unsafe_allow_html=True)\n",
    "\n",
    "        if use == 'Avd 1 och 2':\n",
    "            col1.table(dfi[(dfi.avd == 1) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "            col2.table(dfi[(dfi.avd == 2) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "        elif use == 'Avd 3 och 4':\n",
    "            col1.table(dfi[(dfi.avd == 3) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "            col2.table(dfi[(dfi.avd == 4) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "        elif use == 'Avd 5 och 6':\n",
    "            col1.table(dfi[(dfi.avd == 5) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "            col2.table(dfi[(dfi.avd == 6) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "        elif use == 'Avd 7':\n",
    "            col1.table(dfi[(dfi.avd == 7) & dfi.v√§lj].sort_values(by=['Meta'], ascending=False)[\n",
    "                       ['nr', 'h√§st', 'Meta', 'kelly']])\n",
    "        elif use == 'clear':\n",
    "            st.stop()\n",
    "        else:\n",
    "            st.write('ej klart')\n",
    "\n",
    "        st.write(compute_total_insats(dfi[dfi.v√§lj]))\n",
    "\n",
    "with sortera:\n",
    "    if st.sidebar.checkbox('se data'):\n",
    "        dfr = st.session_state.df\n",
    "        sort = st.sidebar.radio('sortera p√•', ['Meta', 'kelly', 'avd'])\n",
    "        if sort:\n",
    "            if sort == 'kelly':\n",
    "                st.write(dfr[['avd', 'nr', 'h√§st', 'Meta', 'kelly']].sort_values(\n",
    "                    by=['kelly', 'avd'], ascending=[False, False]))\n",
    "            elif sort == 'Meta':\n",
    "                st.write(dfr[['avd', 'nr', 'h√§st', 'Meta', 'kelly']].sort_values(\n",
    "                    by=['Meta', 'avd', 'nr'], ascending=[False, False, False]))\n",
    "            else:\n",
    "                dfra = dfr[['avd', 'nr', 'h√§st', 'proba6', 'proba9', 'proba1',\n",
    "                            'kelly6', 'kelly9', 'kelly1', 'Meta', 'v√§lj', 'kelly']]\n",
    "                st.write(dfra.sort_values(\n",
    "                    by=['avd', 'nr'], ascending=[True, True]))\n",
    "\n",
    "meta_list = ['rf', 'knn', 'ridge', 'lasso']\n",
    "meta_list.sort(reverse=True, key=lambda x: sort_list_of_meta(x))\n",
    "meta = st.sidebar.radio('v√§lj meta_model', meta_list)\n",
    "\n",
    "if meta != st.session_state.meta:\n",
    "    st.session_state.meta = meta\n",
    "    st.write('meta_model:', meta)\n",
    "    df_scraped = pd.read_csv('sparad_scrape_spela.csv')\n",
    "    try:\n",
    "        df_scraped.drop(['plac'], axis=1, inplace=True)\n",
    "        st.info('this file is not up to date - a scrape is needed')\n",
    "    except:\n",
    "        pass\n",
    "    df_stack = build_stack_df(df_scraped, typer)\n",
    "    df_stack.to_csv('sparad_stack.csv', index=False)\n",
    "    use_meta(df_stack, meta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
