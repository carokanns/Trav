{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testar olika metoder för metamodell learn och gör en gridsearch på metamodeller\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.width', 260)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 80)\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "# import concurrent.futures\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "# sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\modeller\\\\')\n",
    "import V75_scraping as vs\n",
    "\n",
    "import typ as tp\n",
    "\n",
    "pref='../'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skapa modeller\n",
    "#           name, ant_hästar, proba, kelly, motst_ant, motst_diff,  ant_favoriter, only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,     False,          0,            False,    True,  pref)\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,     True,           2,            True,     False, pref)\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,     True,           2,            True,     True,  pref)\n",
    "typ16 = tp.Typ('typ16', True,      True, True,      2,    True,           2,            False,    True,  pref)\n",
    "\n",
    "typer = [typ6, typ1, typ9, typ16]  # load a file with pickl\n",
    "\n",
    "\n",
    "# with open(pref+'modeller\\\\meta_ridge_model.model', 'rb') as f:\n",
    "#     meta_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#              LEARNING                       #\n",
    "###############################################\n",
    "\n",
    "def skapa_stack_learning(X_, y, save=True):\n",
    "    X = X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly' + nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "\n",
    "    # print(stacked_data.columns)\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    \n",
    "    return stacked_data, y   # enbart stack-info\n",
    "    \n",
    "##### RidgeClassifier (meta model) #####\n",
    "def learn_meta_ridge_model(X, y, alpha=1, class_weight='balanced', save=True):\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "    ridge_model = RidgeClassifier(alpha=alpha,class_weight=class_weight, random_state=2022)\n",
    "    ridge_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_ridge_model.model', 'wb') as f:\n",
    "            pickle.dump(ridge_model, f)\n",
    "\n",
    "    return ridge_model\n",
    "\n",
    "##### RandomForestClassifier (meta model) #####\n",
    "def learn_meta_rf_model(X, y, n_estimators=100, max_depth=None, class_weight='balanced', save=True):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth, n_jobs=6, class_weight=class_weight, random_state=2022)\n",
    "    rf_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_rf_model.model', 'wb') as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "##### LassoClassifier (meta model) #####\n",
    "def learn_meta_lasso_model(X, y, alpha=1, max_iter=1000,  save=True):\n",
    "    from sklearn.linear_model import Lasso\n",
    "\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=max_iter, random_state=2022)\n",
    "    lasso_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_lasso_model.model', 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "\n",
    "    return lasso_model\n",
    "\n",
    "\n",
    "def learn_meta_model(X, y,  meta='ridge', alpha=1,n_estimators=100, max_iter=1000, max_depth=None, class_weight='balanced', save=True):\n",
    "    if meta == 'ridge':\n",
    "        return learn_meta_ridge_model(X, y, alpha=alpha, class_weight=class_weight, save=save)\n",
    "    elif meta == 'rf':\n",
    "        return learn_meta_rf_model(X, y, n_estimators=n_estimators, max_depth=max_depth, class_weight=class_weight, save=save)\n",
    "    elif meta == 'lasso':\n",
    "        return learn_meta_lasso_model(X, y, alpha=alpha, max_iter=max_iter, save=save)\n",
    "    else:\n",
    "        assert False, f'{meta} is not a valid meta model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def förbered(df,meta_fraction):\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    \n",
    "    # läs in FEATURES.txt\n",
    "    with open(pref+'FEATURES.txt', 'r',encoding='utf-8') as f:    \n",
    "        features = f.read().splitlines()\n",
    "     \n",
    "    X=X[features]\n",
    "    \n",
    "    assert len(features) == len(X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'   \n",
    "    assert set(features) == set(X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "    \n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    if meta_fraction==0: \n",
    "        # no meta data created\n",
    "        return X,y,None,None\n",
    "    \n",
    "    # use a fraction for meta data\n",
    "    meta_antal = int(len(X.datum.unique())*meta_fraction)\n",
    "    meta_datum = X.datum.unique()[-meta_antal:]\n",
    "\n",
    "    X_val = X.loc[X.datum.isin(meta_datum)]\n",
    "    y_val = y[X_val.index]\n",
    "    X=X.loc[~X.datum.isin(meta_datum)]\n",
    "    y=y.loc[X.index]\n",
    "    return X, y, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kelly-värde baserat på streck omvandlat till odds\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open(pref+'rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TimeSeries_learning( typer, n_splits=5,meta_fraction=0.2, meta='ridge', save=True, learn_models=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack av X_test och y_test från alla typer. Används som input till meta_model.\n",
    "        - learn_models=True betyder att vi både gör en learning och skapar en stack\n",
    "        - learn_models=False betyder att vi bara skapar en stack och då har save ingen funktion\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_all = pd.read_csv(pref+'all_data.csv')\n",
    "             \n",
    "    # print('sista datum',df_all.datum.iloc[-1])\n",
    "    \n",
    "    X, y, _, _ = förbered(df_all, meta_fraction=meta_fraction)\n",
    "    print('shape of X', X.shape)\n",
    "    print('sista datum', X.datum.iloc[-1], 'resp', df_all.datum.iloc[-1])\n",
    "    \n",
    "    ts = TimeSeriesSplit(n_splits=n_splits)\n",
    "    stacked_data=pd.DataFrame(columns=['proba6', 'proba1', 'proba9', 'proba16', 'kelly6', 'kelly1', 'kelly9', 'kelly16','y'])\n",
    "        \n",
    "    for enum,(train_index, test_index) in enumerate(ts.split(X,y)):\n",
    "        print('\\nshape of X_train', X.iloc[train_index].shape, 'shape of X_test', X.iloc[test_index].shape)\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['y'] =  y_test\n",
    "        print('enum',enum)\n",
    "        for typ in typer:\n",
    "            print('  ',typ.name)\n",
    "            if learn_models:\n",
    "                # eftersom predict(X_test) gör en load-model så gör vi save här\n",
    "                cbc = typ.learn(X_train, y_train, X_test, y_test, save=save)\n",
    "                print('  ','best_iteration', cbc.get_best_iteration())\n",
    "                \n",
    "            this_proba=typ.predict(X_test)\n",
    "            \n",
    "            nr = typ.name[3:]\n",
    "            \n",
    "            temp_df['proba'+nr] = this_proba\n",
    "\n",
    "            this_kelly=kelly(this_proba, X_test[['streck']], None)\n",
    "            temp_df['kelly' + nr] = this_kelly\n",
    "        \n",
    "        \n",
    "        stacked_data = pd.concat([stacked_data, temp_df],ignore_index=True)\n",
    "        stacked_data.y = stacked_data.y.astype(int)\n",
    "        \n",
    "    # step 2: learn models on all of X - what iteration to use?\n",
    "    print('\\nFull X learning')\n",
    "    for typ in typer:\n",
    "        print(typ.name)\n",
    "        if learn_models:\n",
    "            cbc = typ.learn(X, y, None, None, iterations=100, save=save)\n",
    "\n",
    "    # step 3: learn meta model\n",
    "    meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=0.001,\n",
    "                              max_iter=55, n_estimators=360, max_depth=5, meta='rf', class_weight=None)\n",
    "\n",
    "    print('✔️ Learning done')\n",
    "\n",
    "    return stacked_data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skall inte köras efter final learning - sparar modeller baserade på mindra data\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#                     VALIDATE                               #\n",
    "##############################################################\n",
    "\n",
    "print('skall inte köras efter final learning - sparar modeller baserade på mindra data')\n",
    "\n",
    "def predict_meta_ridge_model(X, ridge_model=None):\n",
    "    if ridge_model is None:\n",
    "        with open(pref+'modeller/meta_ridge_model.model', 'rb') as f:\n",
    "            ridge_model = pickle.load(f)\n",
    "\n",
    "    return ridge_model._predict_proba_lr(X)\n",
    "\n",
    "def predict_meta_rf_model(X, rf_model=None):\n",
    "    if rf_model is None:\n",
    "        with open(pref+'modeller/meta_rf_model.model', 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "\n",
    "    return rf_model.predict_proba(X)\n",
    "\n",
    "\n",
    "def predict_meta_lasso_model(X, lasso_model=None):\n",
    "    if lasso_model is None:\n",
    "        with open(pref+'modeller/meta_lasso_model.model', 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "\n",
    "    return lasso_model.predict(X)\n",
    "\n",
    "def predict_meta_model(X, meta_model=None):\n",
    "    if meta_model == 'ridge':\n",
    "        return predict_meta_ridge_model(X, meta_model)[:, 1]\n",
    "    elif meta_model == 'rf':\n",
    "        return predict_meta_rf_model(X, meta_model)[:, 1]\n",
    "    elif meta_model == 'lasso':\n",
    "        return predict_meta_lasso_model(X, meta_model)\n",
    "    elif meta_model == None:\n",
    "        assert False, 'ingen meta_model angiven'\n",
    "    else:             \n",
    "        return meta_model.predict(X)\n",
    "\n",
    "def display_scores(y_true, y_pred):\n",
    "    print('AUC', roc_auc_score(y_true, y_pred), '  ')\n",
    "    # and the F1 score\n",
    "    print('F1', f1_score(y_true, y_pred), '  ')\n",
    "    #accuracy\n",
    "    print('Acc', accuracy_score(y_true, y_pred), '  ')\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, typ, fr=0.1, to=0.5, step = 0.001):\n",
    "     \n",
    "    #### Först:  hitta ett treshold som tippar ca 2.5 hästar per avd ####\n",
    "    for tresh in np.arange(fr, to, step):\n",
    "        cost = 12*sum(y_pred > tresh)/len(y_pred)\n",
    "        if cost < 2.5:\n",
    "            break\n",
    "    tresh = round(tresh, 4)\n",
    "    # print(f'Treshold: {tresh}\\n')\n",
    "    y_pred = (y_pred > tresh).astype(int)\n",
    "    # confusion_matrix_graph(y_true, y_pred, f'{typ} treshold={tresh}')\n",
    "    \n",
    "    #### Sedan: confusion matrix graph ####\n",
    "    title = f'{typ} treshold={tresh}'\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(font_scale=2.0)\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "\n",
    "    # increase font size\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    # plot fig\n",
    "    plt.show()\n",
    "    \n",
    "    #### print scores ####\n",
    "    display_scores(y_true, y_pred)\n",
    "    print('spelade per lopp:', 12 * sum(y_pred)/len(y_pred))\n",
    "    \n",
    "\n",
    "def validate(meta_model=None, drop=[]):\n",
    "    print('Only accurate directly after \"Learn TimeSeries\"')\n",
    "    df_all = pd.read_csv(pref+'all_data.csv')\n",
    "    print('sista datum', df_all.datum.iloc[-1])\n",
    "\n",
    "    _, _, X_val, y_val = förbered(df_all, meta_fraction=0.2)\n",
    "   \n",
    "    # create the stack from validation data\n",
    "    stacked_meta_val, y_val = skapa_stack_learning(X_val, y_val)\n",
    "    stacked_meta_val = stacked_meta_val.drop(drop, axis=1)\n",
    "    stacked_meta_val['meta'] = predict_meta_model(stacked_meta_val, meta_model=meta_model)\n",
    "    stacked_meta_val['y'] = y_val.values\n",
    "    stacked_meta_val['avd'] = X_val.avd.values\n",
    "    \n",
    "    ##############################################################\n",
    "    #                          Meta model                        #\n",
    "    ##############################################################\n",
    "    y_true = stacked_meta_val['y']\n",
    "    y_pred = stacked_meta_val['meta']\n",
    "    plot_confusion_matrix(y_true, y_pred, 'meta', fr=0.05, to=0.8, step = 0.0001)\n",
    "    \n",
    "    ################################################################\n",
    "    #                         proba 6, 1, 9, 16                    #\n",
    "    ################################################################\n",
    "    for typ in typer:\n",
    "        name = 'proba' + typ.name[3:]\n",
    "        y_pred = stacked_meta_val[name]\n",
    "        plot_confusion_matrix(y_true, y_pred, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#            FINAL LEARNING steps                            #\n",
    "##############################################################\n",
    "def final_learning(typer, n_splits=5, learn_models=True):\n",
    "    # step 1: learn models and produce the stacked data\n",
    "    print('Step 1: Final learn models and produce the stacked data')\n",
    "    stacked_data = TimeSeries_learning(typer, n_splits=n_splits, meta_fraction=0, save=True, learn_models=learn_models)\n",
    "\n",
    "    # step 2: learn meta model\n",
    "    print('Step 2: Final learn meta model')\n",
    "    meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=0.001, max_iter=55, n_estimators=360, max_depth=5, meta='rf', class_weight=None)\n",
    "\n",
    "    print(meta_model)\n",
    "    # print(meta_model.coef_)\n",
    "    # l = list(zip(stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "    l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "    l.sort(key=lambda x: x[1], reverse=True)\n",
    "    print('feature imp:', l)\n",
    "    print('✔️ Final learning done')\n",
    "    return stacked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parts of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries learning - skapar stacked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_data=TimeSeries_learning(typer, n_splits=3, meta_fraction=0.2, meta='rf', save=True, learn_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn meta model on stacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=47, max_iter=40, n_estimators=360, max_depth=5, meta='rf', class_weight=None)\n",
    "print(meta_model)\n",
    "# print(meta_model.coef_)\n",
    "# l = list(zip(stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "l.sort(key=lambda x: x[1], reverse=True)\n",
    "print('feature imp:', l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate meta model and typ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(meta='rf', meta_model=meta_model, drop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stacked_data = final_learning(typer, n_splits=5, learn_models=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_model = learn_meta_model(final_stacked_data.drop(['y'], \n",
    "                              axis=1), final_stacked_data['y'], alpha=0.001, max_iter=55, n_estimators=360, max_depth=5, meta='rf', class_weight=None)\n",
    "print(meta_model)\n",
    "# print(meta_model.coef_)\n",
    "# l = list(zip(final_stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "l.sort(key=lambda x: x[1], reverse=True)\n",
    "print('feature imp:', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  optimize rf                                                #\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  optimize Lasso                                             #\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  optimize Ridge                                             #\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_params = {\n",
    "    'lasso': {\n",
    "        'model': Lasso(random_state=2022),\n",
    "        'params': {\n",
    "            'alpha': [0.00001, 0.00002, 0.00003, 0.0001, 0.0002, 0.0005],\n",
    "            'max_iter': [55, 30, 40, 50, 55,70,100,500,1000],\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(n_jobs=4, random_state=2022),\n",
    "        'params': {\n",
    "            'n_estimators': [300, 366, 400],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'class_weight': [None],\n",
    "        }\n",
    "    },\n",
    "    'ridge': {  \n",
    "        'model': RidgeClassifier(random_state=2022),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10, 100, 200, 500, 1000],\n",
    "            'class_weight': [None],\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {'priors': [None]}\n",
    "    },\n",
    "    # 'naive_bayes_multinomial': {\n",
    "    #     'model': MultinomialNB(),\n",
    "    #     'params': {}\n",
    "    # },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 2, 3, 4, 5],\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# final_stacked_data = final_learning(typer, n_splits=5, learn_models=True)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    print(model_name,end=', ')\n",
    "    clf = GridSearchCV(mp['model'], mp['params'],n_jobs=4,\n",
    "                       cv=5, scoring=None,  return_train_score=False)\n",
    "    clf.fit(final_stacked_data.drop(['y'],axis=1), final_stacked_data['y'])\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    print('best_score =', clf.best_score_, clf.best_params_)\n",
    "df_grid = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "df_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
