{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END 13.43.23\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testar olika metoder för metamodell learn och gör en gridsearch på metamodeller\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from IPython.display import display \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.width', 260)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 80)\n",
    "from category_encoders import TargetEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "# import concurrent.futures\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    'C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "pref = '../'\n",
    "import travdata as td\n",
    "\n",
    "# sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\modeller\\\\')\n",
    "import V75_scraping as vs\n",
    "\n",
    "import typ as tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edward', 'ingvar']\n",
      "['abc', 'bdd', 'csn', 'david', 'edward', 'fridz', 'gustav', 'hans', 'ingvar']\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy list with strings\n",
    "l =      ['abc', 'bdd', 'csn', 'david', 'edward', 'fridz', 'gustav', 'hans', 'ingvar']\n",
    "print([item for item in l if 'ar' in item])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kollar XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lägg_in_antal_hästar(df_):\n",
    "    df = df_.copy()\n",
    "    df['ant_per_lopp'] = None\n",
    "    df['ant_per_lopp'] = df.groupby(['datum', 'avd'])['avd'].transform('count')\n",
    "    return df\n",
    "\n",
    "\n",
    "def lägg_in_diff_motståndare(X_, ant_motståndare):\n",
    "    X = X_.copy()\n",
    "\n",
    "    # set X['motståndare1'] to largest streck in every avd\n",
    "    grouped = X.groupby(['datum', 'avd'])['streck']\n",
    "    X['diff1'] = grouped.transform(max) - X.streck\n",
    "\n",
    "    for i in range(2, ant_motståndare+1):\n",
    "        # set X['motståndare'+str(i)] to ith largest streck in every avd\n",
    "        X['diff' +\n",
    "            str(i)] = grouped.transform(lambda x: x.nlargest(i).min()) - X.streck\n",
    "\n",
    "    return X\n",
    "\n",
    "def prepare_for_model(X_):\n",
    "    X = X_.copy()\n",
    "    X = lägg_in_antal_hästar(X)\n",
    "    X = lägg_in_diff_motståndare(X, 3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skapar feature-filerna (en gång för alla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:     # Kör bara vid behov av att skapa nya filer med features\n",
    "    # get travdata\n",
    "    v75 = td.v75(pref='../')\n",
    "    df, _ = v75.förbered_data(extra=True)\n",
    "\n",
    "    # extract categorical features\n",
    "    cat_features = df.select_dtypes(include=['object']).columns\n",
    "    # extract all other features\n",
    "    num_features = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "    L1_features = ['cat1_L1', 'cat2_L1', 'xgb1_L1', 'xgb1_L1']\n",
    "    L2_features = ['cat1_L2', 'cat2_L2', 'xgb1_L2', 'xgb1_L2']\n",
    "\n",
    "    # skriv ut cat_features till fil bevara å, ä, ö\n",
    "    with open(pref+'CAT_RAW.txt', 'w', encoding='utf-8') as f:   # Måste editeras för att ta bort ev onödiga features\n",
    "        for item in cat_features:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    # skriv ut num_features till fil\n",
    "    with open(pref+'NUM_RAW.txt', 'w',  encoding='utf-8') as f:  # Måste editeras för att ta bort onödiga features\n",
    "        for item in num_features:\n",
    "            f.write(\"%s\\n\" % item)   \n",
    "            \n",
    "    # skriv ut META_features till fil\n",
    "    with open(pref+'L1_OUTP_FEATURES.txt', 'w',  encoding='utf-8') as f:  # Adderas till den totala listan av features\n",
    "        for item in L1_features:\n",
    "            f.write(\"%s\\n\" % item)    \n",
    "    with open(pref+'L2_OUTP_FEATURES.txt', 'w',  encoding='utf-8') as f:  # Används till medelvärdet för den slutliga prediktionen\n",
    "        for item in L2_features:\n",
    "            f.write(\"%s\\n\" % item)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exempel på hur feature-filerna skall läsas in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# läs in CAT_FEATURES.txt till cat_features\n",
    "with open(pref+'CAT_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "    cat_features = f.read().split()        \n",
    "    \n",
    "# läs in NUM_FEATURES.txt till num_features\n",
    "with open(pref+'NUM_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "    num_features = f.read().split()    \n",
    "\n",
    "# läs in L1_OUTP_FEATURES.txt till L1_features\n",
    "with open(pref+'L1_OUTP_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "    L1_features = f.read().split()\n",
    "# läs in L2_OUTP_FEATURES.txt till L2_features\n",
    "with open(pref+'L2_OUTP_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "    L2_features = f.read().split()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat1_L2', 'cat2_L2', 'xgb1_L2', 'xgb1_L2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_data.csv\n",
      "Loading dataframe from the file: ../all_data.csv\n",
      "Alla cat_features ['bana', 'häst', 'kusk', 'kön', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana', 'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
      "plac finns i df\n",
      "Creating new catboost encoder\n",
      "CatBoost encoding done\n",
      "ENC.get_feature_names Ett ['bana', 'häst', 'kusk', 'kön', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana', 'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
      "ENC.get_feature_names ['bana', 'häst', 'kusk', 'kön', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana', 'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
      "ung antal lopp 4694.999999999999\n",
      "CatBoostEncoder Redan fixat i förbered_data()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>avd</th>\n",
       "      <th>bana</th>\n",
       "      <th>häst</th>\n",
       "      <th>kusk</th>\n",
       "      <th>streck</th>\n",
       "      <th>spår</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>ålder</th>\n",
       "      <th>kön</th>\n",
       "      <th>pris</th>\n",
       "      <th>h1_kusk</th>\n",
       "      <th>h1_bana</th>\n",
       "      <th>h1_spår</th>\n",
       "      <th>h1_plac</th>\n",
       "      <th>h1_pris</th>\n",
       "      <th>h1_odds</th>\n",
       "      <th>h1_kmtid</th>\n",
       "      <th>h2_kusk</th>\n",
       "      <th>h2_bana</th>\n",
       "      <th>h2_spår</th>\n",
       "      <th>h2_plac</th>\n",
       "      <th>h2_pris</th>\n",
       "      <th>h2_odds</th>\n",
       "      <th>h2_kmtid</th>\n",
       "      <th>h3_kusk</th>\n",
       "      <th>h3_bana</th>\n",
       "      <th>h3_spår</th>\n",
       "      <th>h3_plac</th>\n",
       "      <th>h3_pris</th>\n",
       "      <th>h3_odds</th>\n",
       "      <th>h3_kmtid</th>\n",
       "      <th>h4_kusk</th>\n",
       "      <th>h4_bana</th>\n",
       "      <th>h4_spår</th>\n",
       "      <th>h4_plac</th>\n",
       "      <th>h4_pris</th>\n",
       "      <th>h4_odds</th>\n",
       "      <th>h4_kmtid</th>\n",
       "      <th>h5_kusk</th>\n",
       "      <th>h5_bana</th>\n",
       "      <th>h5_spår</th>\n",
       "      <th>h5_plac</th>\n",
       "      <th>h5_pris</th>\n",
       "      <th>h5_odds</th>\n",
       "      <th>h5_kmtid</th>\n",
       "      <th>h1_dist</th>\n",
       "      <th>h2_dist</th>\n",
       "      <th>h3_dist</th>\n",
       "      <th>h4_dist</th>\n",
       "      <th>h5_dist</th>\n",
       "      <th>h1_auto</th>\n",
       "      <th>h2_auto</th>\n",
       "      <th>h3_auto</th>\n",
       "      <th>h4_auto</th>\n",
       "      <th>h5_auto</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>startnr</th>\n",
       "      <th>rel_kr</th>\n",
       "      <th>streck_avst</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>h1_samma_bana</th>\n",
       "      <th>h2_samma_bana</th>\n",
       "      <th>h3_samma_bana</th>\n",
       "      <th>h1_samma_kusk</th>\n",
       "      <th>h2_samma_kusk</th>\n",
       "      <th>h3_samma_kusk</th>\n",
       "      <th>häst_namn</th>\n",
       "      <th>kusk_namn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.086178</td>\n",
       "      <td>0.110221</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.080678</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>16.8</td>\n",
       "      <td>0.104309</td>\n",
       "      <td>0.092676</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.70</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.090703</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>52.42</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>0.103769</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.028726</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3935.030968</td>\n",
       "      <td>6006.507182</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>8.366600</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102926</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>allaballakaitoz</td>\n",
       "      <td>carl-erik lindblom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.086178</td>\n",
       "      <td>0.099342</td>\n",
       "      <td>0.098834</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.080678</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>0.149669</td>\n",
       "      <td>0.101346</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>12.45</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.145868</td>\n",
       "      <td>0.081085</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.55</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.106469</td>\n",
       "      <td>0.099475</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.58</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.101773</td>\n",
       "      <td>0.074843</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>0.103340</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2735.738970</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>753.137355</td>\n",
       "      <td>753.137355</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114914</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>aristocat boko</td>\n",
       "      <td>ulf ohlsson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        datum  avd      bana      häst      kusk  streck  spår    dist  lopp_dist  start  ålder       kön      pris   h1_kusk   h1_bana  h1_spår  h1_plac  h1_pris  h1_odds  h1_kmtid   h2_kusk   h2_bana  h2_spår  h2_plac  h2_pris  h2_odds  h2_kmtid   h3_kusk  \\\n",
       "0  2014-12-28  1.0  0.086178  0.110221  0.007834     5.0   6.0  2100.0     2100.0      0      6  0.080678  125000.0  0.083552  0.088300      3.0      2.0     35.0     3.92      16.8  0.104309  0.092676      3.0      1.0     30.0     3.70      14.9  0.090703   \n",
       "1  2014-12-28  1.0  0.086178  0.099342  0.098834     7.0  12.0  2100.0     2100.0      0      7  0.080678  125000.0  0.149669  0.101346      7.0      4.0    125.0    12.45      30.0  0.145868  0.081085      7.0      5.0    100.0     7.55      30.0  0.106469   \n",
       "\n",
       "    h3_bana  h3_spår  h3_plac  h3_pris  h3_odds  h3_kmtid   h4_kusk   h4_bana  h4_spår  h4_plac  h4_pris  h4_odds  h4_kmtid   h5_kusk   h5_bana  h5_spår  h5_plac  h5_pris  h5_odds  h5_kmtid  h1_dist  h2_dist  h3_dist  h4_dist  h5_dist  h1_auto  h2_auto  \\\n",
       "0  0.075912      3.0     15.0    125.0    52.42      14.3  0.035038  0.103769      3.0     15.0     70.0     5.20      13.9  0.028726  0.095196      3.0     15.0     25.0      2.2      12.3   2140.0   2140.0   2640.0   2140.0   1609.0        1        1   \n",
       "1  0.099475      7.0      6.0     70.0    15.58      13.6  0.101773  0.074843      7.0      6.0     70.0     2.55      15.1  0.095652  0.103340      7.0      6.0    100.0      7.2      15.6   1640.0   3180.0   2140.0   2640.0   3180.0        1        0   \n",
       "\n",
       "   h3_auto  h4_auto  h5_auto      h1_perf      h2_perf     h3_perf     h4_perf     h5_perf  senast  delta1  delta2  delta3  delta4  startnr    rel_kr  streck_avst  rel_rank  h1_samma_bana  h2_samma_bana  h3_samma_bana  h1_samma_kusk  h2_samma_kusk  \\\n",
       "0        1        1        1  3935.030968  6006.507182   11.180340    8.366600    5.000000    21.0    19.0    17.0    10.0    18.0      0.0  0.102926         43.0  0.416667          False          False          False           True           True   \n",
       "1        1        1        0  2735.738970  1484.131591  753.137355  753.137355  900.171313     8.0     7.0    24.0    14.0    11.0      0.0  0.114914         41.0  0.250000          False          False          False          False          False   \n",
       "\n",
       "   h3_samma_kusk        häst_namn           kusk_namn  \n",
       "0           True  allaballakaitoz  carl-erik lindblom  \n",
       "1           True   aristocat boko         ulf ohlsson  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "# TODO Testa andra encoders\n",
    "# TODO Ta med samtliga kolumner hela tiden och använd X[USE_KOLUMNS] för fit() och predict()\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "import json\n",
    "\n",
    "encoder_type = 'catboost'  # 'target', 'catboost' or 'catcodes'\n",
    "\n",
    "# get travdata\n",
    "v75 = td.v75(pref='../')\n",
    "if encoder_type == 'target':\n",
    "    df, _ = v75.förbered_data(extra=True, target_encode_list=cat_features)\n",
    "elif encoder_type == 'catboost':\n",
    "    print('Alla cat_features', cat_features)\n",
    "    df, ENC = v75.förbered_data(extra=True, catboost_encode_list=cat_features)\n",
    "    print('ENC.get_feature_names Ett',ENC.get_feature_names())\n",
    "elif encoder_type == 'catcodes':    \n",
    "    df,_ = v75.förbered_data(extra=True) \n",
    "else:\n",
    "    raise ValueError('encoder_type must be target, catboost or catcodes')\n",
    "name='xgb1L1'\n",
    "\n",
    "print('ENC.get_feature_names',ENC.get_feature_names())\n",
    "\n",
    "    # Läs in parametrar från fil\n",
    "with open(pref+'optimera/params_'+name+'.json', 'rb') as f:\n",
    "    params = json.load(f)\n",
    "    params = params['params']\n",
    "\n",
    "iterations = params['iterations'] if 'iterations' in params else 500\n",
    "params.pop('iterations')  # Ta bort iterations från params\n",
    "\n",
    "print('ung antal lopp',np.mean(df.avd.value_counts())*0.7)\n",
    "\n",
    "# df.drop(['datum', 'avd', 'startnr'], axis=1, inplace=True)\n",
    "\n",
    "# df = df[['bana','häst','streck','spår','dist','lopp_dist','start','ålder','kön','pris','y']].copy()\n",
    "df.y = df.y.astype('float')\n",
    "\n",
    "n_train = int(len(df) * 0.7)\n",
    "X_train = df.iloc[:n_train].drop('y', axis=1)\n",
    "y_train = df.iloc[:n_train]['y']\n",
    "X_test = df.iloc[n_train:].drop('y', axis=1)\n",
    "y_test = df.iloc[n_train:]['y']\n",
    "\n",
    "# bool_features = list(X_train.select_dtypes(include=['bool']).columns)\n",
    "# X_train[bool_features] = X_train[bool_features].astype('int')\n",
    "# X_test[bool_features] = X_test[bool_features].astype('int')\n",
    "\n",
    "# display(df.shape, len(cat_features)+len(num_features))\n",
    "# display(df.info() )\n",
    "\n",
    "if   encoder_type=='catcodes': # Använd category codes som värden \n",
    "    print('# Använd category codes')\n",
    "    X_train_use = X_train.copy()\n",
    "    X_test_use = X_test.copy()\n",
    "    X_train_use[cat_features] = X_train[cat_features].astype('category')\n",
    "    X_test_use[cat_features] = X_test[cat_features].astype('category')\n",
    "    X_train_use[cat_features] = X_train_use[cat_features].apply(\n",
    "        lambda x: x.cat.codes)\n",
    "    X_test_use[cat_features] = X_test_use[cat_features].apply(\n",
    "        lambda x: x.cat.codes)\n",
    "elif encoder_type=='target' :   # Använd Target encoding \n",
    "    print('# TargetEncoder Redan fixat i förbered_data()')\n",
    "    X_train_use = X_train.copy()\n",
    "    X_test_use = X_test.copy()\n",
    "elif encoder_type=='catboost' :   # Använd CatBoost encoding \n",
    "    print('CatBoostEncoder Redan fixat i förbered_data()')\n",
    "    X_train_use = X_train.copy()\n",
    "    X_test_use = X_test.copy()\n",
    "else:\n",
    "    raise ValueError('encoder_type måste vara \"target\" eller \"catcodes\"')    \n",
    "\n",
    "display(X_train_use.head(2))\n",
    "model = xgb.XGBClassifier(**params,\n",
    "                        scale_pos_weight=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Använd encoder.fit_transform() för att omvandla kategoriska variabler i df_train\n",
    "# if ohe:\n",
    "#     X_train_use = encoder.fit_transform(X_train)\n",
    "# else:\n",
    "#     X_train_use = X_train    \n",
    "    \n",
    "use_features = cat_features + num_features\n",
    "print(f'tränar på X_train_use med shape {X_train_use[use_features].shape}')    \n",
    "model.fit(X_train_use[use_features], y_train, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beräkna accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lim=0.5   #0.57\n",
    "\n",
    "use_features = cat_features + num_features    \n",
    "print(f'predict på X_test_use med shape {X_test_use[use_features].shape}')        \n",
    "y_pred = (model.predict_proba(X_test_use[use_features])[:, 1] > lim)\n",
    "y_pred = y_pred.astype(int)\n",
    "print('Antal tippade', np.sum(y_pred), 'av', len(y_pred),   '(', np.sum(y_pred)/len(y_pred)*12, ')')\n",
    "# Beräkna konfusionsmatris\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n",
    "\n",
    "# Rita upp värme kartan med den normaliserade matrisen\n",
    "# print(f'Confusion matrix: \\n{confusion_matrix}')\n",
    "\n",
    "import seaborn as sns\n",
    "# Skapa en värmevägskarta av konfusionsmatrisen\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d',\n",
    "            xticklabels=['predicted negative', 'predicted positive'],\n",
    "            yticklabels=['true negative', 'true positive'])\n",
    "\n",
    "# Beräkna precision och recall\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "# # Beräkna F1-score\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# print(f'F1-score: {f1:.2f}')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Mät AUC för y_pred\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Skriv ut AUC\n",
    "print(f'AUC: {auc:.2f}')\n",
    "\n",
    "\n",
    "# Skriv ut en rapport med AUC och andra prestandametriker\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Använder Category Codes\n",
    "tot=9014+3862+274+935  \n",
    "935/tot*100\n",
    "\n",
    "AUC: 0.74   \n",
    "_________________precision______recall______f1-score____support  \n",
    "\n",
    "           0       0.97      0.70      0.81     12876\n",
    "           1       0.19      0.77      0.31      1209 \n",
    "\n",
    "    accuracy                           0.71     14085  \n",
    "   macro avg_______0.58__________0.74_________0.56_________14085  \n",
    "weighted avg____0.90__________0.71_________0.77_________14085  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((753/(753+2194))*2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_counts = df.y.value_counts(normalize=True)\n",
    "\n",
    "# Skriv ut antalet 1:or och 0:or\n",
    "print(y_counts*11.61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kollar stacking i sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# import LightGBM classifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# import Ridge classifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "ts = TimeSeriesSplit(n_splits=3)\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=2022)),\n",
    "    ('lg', LGBMClassifier(n_estimators=100, random_state=2022))\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=RidgeClassifier(),\n",
    "    stack_method='predict_proba',\n",
    ")\n",
    "\n",
    "v75 = td.v75(pref='../')\n",
    "target_encode_list = ['häst', 'kön','bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk',\n",
    "                       'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']\n",
    "df,_ = v75.förbered_data(missing_num=True, missing_cat=True,\n",
    "                      target_encode_list=target_encode_list)  # num hanteras av catboost\n",
    "\n",
    "display(df)\n",
    "for train_index, test_index in ts.split(df):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1:]\n",
    "\n",
    "# add a list to the row with datum=d\n",
    "l=[None]*69\n",
    "l[0]=69\n",
    "d= \"2022-10-01\"\n",
    "\n",
    "lr = pd.DataFrame([[d]+l], columns=df.columns) \n",
    "display(lr)\n",
    "def add_row(df, d, l):\n",
    "    df.loc[(df.datum==d ) & (df.spår==9),:] = [d] + l\n",
    "    print([d]+l)\n",
    "    return df\n",
    "# df=add_row(df, d, l)\n",
    "pd.concat([df,lr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for train_index, test_index in ts.split(df):\n",
    "    train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "    print(clf.fit(train.drop(['datum','avd','y'], axis=1),train.y).score(test.drop(['datum','avd','y'], axis=1), test.y))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skapa modeller\n",
    "#           name, ant_hästar, proba, kelly, motst_ant, motst_diff,  ant_favoriter, only_clear, streck\n",
    "typ6 = tp.Typ('typ6', True,       True, False,     0,     False,          0,            False,    True,  pref)\n",
    "typ1 = tp.Typ('typ1', False,      True, False,     2,     True,           2,            True,     False, pref)\n",
    "typ9 = tp.Typ('typ9', True,       True, True,      2,     True,           2,            True,     True,  pref)\n",
    "# typ16 = tp.Typ('typ16', True,      True, True,      2,    True,           2,            False,    True,  pref)\n",
    "\n",
    "typer = [typ6, typ1, typ9]    # , typ16]  # load a file with pickl\n",
    "\n",
    "\n",
    "# with open(pref+'modeller\\\\meta_ridge_model.model', 'rb') as f:\n",
    "#     meta_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly-värde baserat på streck omvandlat till odds\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    with open(pref+'rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#              LEARNING                       #\n",
    "###############################################\n",
    "\n",
    "def skapa_stack_learning(X_, y, save=True):\n",
    "    X = X_.copy()\n",
    "    stacked_data = pd.DataFrame()\n",
    "    for typ in typer:\n",
    "        nr = typ.name[3:]\n",
    "        stacked_data['proba'+nr] = typ.predict(X)\n",
    "        stacked_data['kelly' + nr] = kelly(stacked_data['proba' + nr], X[['streck']], None)\n",
    "\n",
    "    # print(stacked_data.columns)\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    \n",
    "    return stacked_data, y   # enbart stack-info\n",
    "    \n",
    "##### KNN (meta model) #####  \n",
    "def learn_meta_knn_model(X, y, algorithm = 'auto', leaf_size = 1, metric = 'chebyshev', n_neighbors = 600, p = 1, weights = 'distance', save = True):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    knn_model = KNeighborsClassifier( p=p, n_neighbors=n_neighbors, metric=metric, leaf_size=leaf_size, algorithm=algorithm, weights=weights, n_jobs=6)\n",
    "    knn_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_knn_model.model', 'wb') as f:\n",
    "            pickle.dump(knn_model, f)\n",
    "\n",
    "    return knn_model\n",
    "\n",
    "##### RidgeClassifier (meta model) #####\n",
    "def learn_meta_ridge_model(X, y, alpha=1, class_weight='balanced', save=True):\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "    ridge_model = RidgeClassifier(alpha=alpha,class_weight=class_weight, random_state=2022)\n",
    "    ridge_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_ridge_model.model', 'wb') as f:\n",
    "            pickle.dump(ridge_model, f)\n",
    "\n",
    "    return ridge_model\n",
    "\n",
    "##### RandomForestClassifier (meta model) #####\n",
    "def learn_meta_rf_model(X, y, n_estimators=100, max_depth=None, class_weight='balanced', save=True):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth, n_jobs=6, class_weight=class_weight, random_state=2022)\n",
    "    rf_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_rf_model.model', 'wb') as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "##### LassoClassifier (meta model) #####\n",
    "def learn_meta_lasso_model(X, y, alpha=1, max_iter=1000,  save=True):\n",
    "    from sklearn.linear_model import Lasso\n",
    "\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=max_iter, random_state=2022)\n",
    "    lasso_model.fit(X, y)\n",
    "    # pickle save stacking\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_lasso_model.model', 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "\n",
    "    return lasso_model\n",
    "\n",
    "\n",
    "def learn_meta_model(X, y,  meta='ridge', alpha=1,n_estimators=100, max_iter=1000, max_depth=None, class_weight='balanced', save=True):\n",
    "    if meta == 'ridge':\n",
    "        return learn_meta_ridge_model(X, y, alpha=alpha, class_weight=class_weight, save=save)\n",
    "    elif meta == 'rf':\n",
    "        return learn_meta_rf_model(X, y, n_estimators=n_estimators, max_depth=max_depth, class_weight=class_weight, save=save)\n",
    "    elif meta == 'lasso':\n",
    "        return learn_meta_lasso_model(X, y, alpha=alpha, max_iter=max_iter, save=save)\n",
    "    elif meta == 'knn':\n",
    "        return learn_meta_knn_model(X, y, save=save)\n",
    "    else:\n",
    "        assert False, f'{meta} is not a valid meta model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def förbered(df,meta_fraction):\n",
    "    # Följande datum saknar avd==5 och kan inte användas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "    \n",
    "    # läs in FEATURES.txt\n",
    "    with open(pref+'FEATURES.txt', 'r',encoding='utf-8') as f:    \n",
    "        features = f.read().splitlines()\n",
    "     \n",
    "    X=X[features]\n",
    "    \n",
    "    assert len(features) == len(X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'   \n",
    "    assert set(features) == set(X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "    \n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['häst', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    if meta_fraction==0: \n",
    "        # no meta data created\n",
    "        return X,y,None,None\n",
    "    \n",
    "    # use a fraction for meta data\n",
    "    meta_antal = int(len(X.datum.unique())*meta_fraction)\n",
    "    meta_datum = X.datum.unique()[-meta_antal:]\n",
    "\n",
    "    X_val = X.loc[X.datum.isin(meta_datum)]\n",
    "    y_val = y[X_val.index]\n",
    "    X=X.loc[~X.datum.isin(meta_datum)]\n",
    "    y=y.loc[X.index]\n",
    "    return X, y, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TimeSeries_learning( typer, n_splits=5,meta_fraction=0.2, meta='rf', save=True, learn_models=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack av X_test och y_test från alla typer. Används som input till meta_model.\n",
    "        - learn_models=True betyder att vi både gör en learning och skapar en stack\n",
    "        - learn_models=False betyder att vi bara skapar en stack och då har save ingen funktion\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_all = pd.read_csv(pref+'all_data.csv')\n",
    "             \n",
    "    # print('sista datum',df_all.datum.iloc[-1])\n",
    "    \n",
    "    X, y, _, _ = förbered(df_all, meta_fraction=meta_fraction)\n",
    "    print('shape of X', X.shape)\n",
    "    print('sista datum', X.datum.iloc[-1], 'resp', df_all.datum.iloc[-1])\n",
    "    \n",
    "    ts = TimeSeriesSplit(n_splits=n_splits)\n",
    "    stacked_data=pd.DataFrame(columns=['proba6', 'proba1', 'proba9',  'kelly6', 'kelly1', 'kelly9','y'])\n",
    "        \n",
    "    for enum,(train_index, test_index) in enumerate(ts.split(X,y)):\n",
    "        print('\\nshape of X_train', X.iloc[train_index].shape, 'shape of X_test', X.iloc[test_index].shape)\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['y'] =  y_test\n",
    "        print('enum',enum)\n",
    "        for typ in typer:\n",
    "            print('  ',typ.name)\n",
    "            if learn_models:\n",
    "                # eftersom predict(X_test) gör en load-model så gör vi save här\n",
    "                cbc = typ.learn(X_train, y_train, X_test, y_test, save=save)\n",
    "                print('  ','best_iteration', cbc.get_best_iteration())\n",
    "                \n",
    "            this_proba=typ.predict(X_test)\n",
    "            \n",
    "            nr = typ.name[3:]\n",
    "            \n",
    "            temp_df['proba'+nr] = this_proba\n",
    "\n",
    "            this_kelly=kelly(this_proba, X_test[['streck']], None)\n",
    "            temp_df['kelly' + nr] = this_kelly\n",
    "        \n",
    "        \n",
    "        stacked_data = pd.concat([stacked_data, temp_df],ignore_index=True)\n",
    "        stacked_data.y = stacked_data.y.astype(int)\n",
    "        \n",
    "    # step 2: learn models on all of X - what iteration to use?\n",
    "    print('\\nFull X learning')\n",
    "    for typ in typer:\n",
    "        print(typ.name)\n",
    "        if learn_models:\n",
    "            cbc = typ.learn(X, y, None, None, iterations=100, save=save)\n",
    "\n",
    "    # step 3: learn meta model\n",
    "    print('Null values', stacked_data.isnull().sum())\n",
    "    meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=0.001,\n",
    "                              max_iter=55, n_estimators=360, max_depth=5, meta=meta, class_weight=None)\n",
    "\n",
    "    print('✔️ Learning done')\n",
    "\n",
    "    return stacked_data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                     VALIDATE                               #\n",
    "##############################################################\n",
    "\n",
    "print('skall inte köras efter final learning - sparar modeller baserade på mindra data')\n",
    "\n",
    "def predict_meta_ridge_model(X, ridge_model=None):\n",
    "    if ridge_model is None:\n",
    "        with open(pref+'modeller/meta_ridge_model.model', 'rb') as f:\n",
    "            ridge_model = pickle.load(f)\n",
    "\n",
    "    return ridge_model._predict_proba_lr(X)\n",
    "\n",
    "def predict_meta_rf_model(X, rf_model=None):\n",
    "    if rf_model is None:\n",
    "        with open(pref+'modeller/meta_rf_model.model', 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "\n",
    "    return rf_model.predict_proba(X)\n",
    "\n",
    "def predict_meta_lasso_model(X, lasso_model=None):\n",
    "    if lasso_model is None:\n",
    "        with open(pref+'modeller/meta_lasso_model.model', 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "\n",
    "    return lasso_model.predict(X)\n",
    "\n",
    "def predict_meta_knn_model(X, knn_model=None):\n",
    "    if knn_model is None:\n",
    "        print('ladda in knn_model')\n",
    "        with open(pref+'modeller/meta_knn_model.model', 'rb') as f:\n",
    "            knn_model = pickle.load(f)\n",
    "    print('predict knn')\n",
    "    return knn_model.predict_proba(X)\n",
    "\n",
    "def predict_meta_model(X, meta_model=None):\n",
    "    if meta_model == 'ridge':\n",
    "        return predict_meta_ridge_model(X, meta_model)[:, 1]\n",
    "    elif meta_model == 'rf':\n",
    "        return predict_meta_rf_model(X, meta_model)[:, 1]\n",
    "    elif meta_model == 'lasso':\n",
    "        return predict_meta_lasso_model(X, meta_model)\n",
    "    elif meta_model == 'knn':\n",
    "        return predict_meta_knn_model(X, meta_model)[:,1]\n",
    "    elif meta_model == None:\n",
    "        assert False, 'ingen meta_model angiven'\n",
    "    else:             \n",
    "        return meta_model.predict(X)\n",
    "\n",
    "def display_scores(y_true, y_pred):\n",
    "    print('AUC', roc_auc_score(y_true, y_pred), '  ')\n",
    "    # and the F1 score\n",
    "    print('F1', f1_score(y_true, y_pred), '  ')\n",
    "    #accuracy\n",
    "    print('Acc', accuracy_score(y_true, y_pred), '  ')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, typ, fr=0.1, to=0.5, step = 0.001):\n",
    "    #### Först:  hitta ett treshold som tippar ca 2.5 hästar per avd ####\n",
    "    for tresh in np.arange(fr, to, step):\n",
    "        cost = 12*sum(y_pred > tresh)/len(y_pred)\n",
    "        if cost < 2.5:\n",
    "            break\n",
    "    tresh = round(tresh, 4)\n",
    "    # print(f'Treshold: {tresh}\\n')\n",
    "    y_pred = (y_pred > tresh).astype(int)\n",
    "    # confusion_matrix_graph(y_true, y_pred, f'{typ} treshold={tresh}')\n",
    "    \n",
    "    #### Sedan: confusion matrix graph ####\n",
    "    title = f'{typ} treshold={tresh}'\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(font_scale=2.0)\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "\n",
    "    # increase font size\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    # plot fig\n",
    "    plt.show()\n",
    "    \n",
    "    #### print scores ####\n",
    "    display_scores(y_true, y_pred)\n",
    "    print('spelade per lopp:', 12 * sum(y_pred)/len(y_pred))\n",
    "    \n",
    "\n",
    "def validate(meta_model=None, drop=[]):\n",
    "    print('Only accurate directly after \"Learn TimeSeries\"')\n",
    "    df_all = pd.read_csv(pref+'all_data.csv')\n",
    "    print('sista datum', df_all.datum.iloc[-1])\n",
    "\n",
    "    _, _, X_val, y_val = förbered(df_all, meta_fraction=0.2)\n",
    "   \n",
    "    # create the stack from validation data\n",
    "    stacked_meta_val, y_val = skapa_stack_learning(X_val, y_val)\n",
    "    stacked_meta_val = stacked_meta_val.drop(drop, axis=1)\n",
    "    stacked_meta_val['meta'] = predict_meta_model(stacked_meta_val, meta_model=meta_model)\n",
    "    stacked_meta_val['y'] = y_val.values\n",
    "    stacked_meta_val['avd'] = X_val.avd.values\n",
    "    \n",
    "    ##############################################################\n",
    "    #                          Meta model                        #\n",
    "    ##############################################################\n",
    "    y_true = stacked_meta_val['y']\n",
    "    y_pred = stacked_meta_val['meta']\n",
    "    display(y_pred)\n",
    "    plot_confusion_matrix(y_true, y_pred, 'meta', fr=0.0, to=0.1, step = 0.0001)\n",
    "    \n",
    "    ################################################################\n",
    "    #                         proba 6, 1, 9                        #\n",
    "    ################################################################\n",
    "    for typ in typer:\n",
    "        name = 'proba' + typ.name[3:]\n",
    "        y_pred = stacked_meta_val[name]\n",
    "        plot_confusion_matrix(y_true, y_pred, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#            FINAL LEARNING steps                            #\n",
    "##############################################################\n",
    "def final_learning(typer, n_splits=5, learn_models=True):\n",
    "    # step 1: learn models and produce the stacked data\n",
    "    print('Step 1: Final learn models and produce the stacked data')\n",
    "    stacked_data = TimeSeries_learning(typer, n_splits=n_splits, meta_fraction=0, save=True, learn_models=learn_models)\n",
    "\n",
    "    # step 2: learn meta model\n",
    "    meta = 'knn'\n",
    "    print('Step 2: Final learn meta model', meta)\n",
    "    \n",
    "    meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=0.001, max_iter=55, n_estimators=360, max_depth=5, meta=meta, class_weight=None)\n",
    "\n",
    "    print(meta_model)\n",
    "    \n",
    "    if meta != 'knn':\n",
    "        # print(meta_model.coef_)\n",
    "        # l = list(zip(stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "        l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "        l.sort(key=lambda x: x[1], reverse=True)\n",
    "        print('feature imp:', l)\n",
    "        \n",
    "    print('✔️ Final learning done')\n",
    "    return stacked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parts of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries learning - skapar stacked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_data=TimeSeries_learning(typer, n_splits=3, meta_fraction=0.2, meta='knn', save=True, learn_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn meta model on stacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = learn_meta_model(stacked_data.drop(['y'], axis=1), stacked_data['y'], alpha=47, max_iter=40, n_estimators=360, max_depth=5, meta='knn', class_weight=None)\n",
    "print(meta_model)\n",
    "# print(meta_model.coef_)\n",
    "# l = list(zip(stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "# l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "# l.sort(key=lambda x: x[1], reverse=True)\n",
    "# print('feature imp:', l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate meta model and typ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate( meta_model=meta_model, drop=[])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stacked_data = final_learning(typer, n_splits=5, learn_models=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta='knn'\n",
    "meta_model = learn_meta_model(final_stacked_data.drop(['y'], \n",
    "                              axis=1), final_stacked_data['y'], alpha=0.001, max_iter=55, n_estimators=360, max_depth=5, meta='knn', class_weight=None)\n",
    "print(meta_model)\n",
    "\n",
    "if meta=='knn':\n",
    "    pass \n",
    "else:\n",
    "    # print(meta_model.coef_)\n",
    "    # l = list(zip(final_stacked_data.columns, np.round(meta_model.coef_, 22)))\n",
    "\n",
    "    l = list(zip(stacked_data.columns, np.round(meta_model.feature_importances_, 4)))\n",
    "    l.sort(key=lambda x: x[1], reverse=True)\n",
    "    print('feature imp:', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  optimize models                                            #\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 0.814077\t{'algorithm': 'auto', 'leaf_size': 1, 'metric': 'chebyshev', 'n_neighbors': 1000, 'p': 1, 'weights': 'distance'}\n",
    "# 0.814750\t{'algorithm': 'auto', 'leaf_size': 1, 'metric': 'chebyshev', 'n_neighbors': 2000, 'p': 1, 'weights': 'distance'}\n",
    "# uppdatera ovan och i app_learn\n",
    "\n",
    "model_params = {\n",
    "    'knn': {\n",
    "        'model': KNeighborsClassifier(n_jobs=6),\n",
    "        'params': {\n",
    "            'algorithm': ['auto'],\n",
    "            'leaf_size': [1, 5, 30],\n",
    "            'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "            'n_neighbors': [5, 75, 130, 500, 600, 700,1000, 1500, 2000, 4000],\n",
    "            'p': [1, 2],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        }\n",
    "    },\n",
    "    'lasso': {\n",
    "        'model': Lasso(random_state=2022),\n",
    "        'params': {\n",
    "            'alpha': [0.00001, 0.00002, 0.00003, 0.0001, 0.0002, 0.0005],\n",
    "            'max_iter': [55, 30, 40, 50, 55,70,100,500,1000],\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(n_jobs=4, random_state=2022),\n",
    "        'params': {\n",
    "            'n_estimators': [300, 366, 400, 500],\n",
    "            'max_depth': [ 5, 6, 8],\n",
    "            'class_weight': [None],\n",
    "        }\n",
    "    },\n",
    "    'ridge': {  \n",
    "        'model': RidgeClassifier(random_state=2022),\n",
    "        'params': {\n",
    "            'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100],\n",
    "            'class_weight': [None],\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {'priors': [None]}\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 3, 4, 5,6],\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# final_stacked_data = final_learning(typer, n_splits=5, learn_models=True)\n",
    "\n",
    "display(pd.read_csv('grid_search_meta_results.csv').sort_values(by='best_score', ascending=False))\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    print(model_name,end=', ')\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], n_jobs=4, \n",
    "                       cv=tscv.split(final_stacked_data.drop(['y'], axis=1)), \n",
    "                       scoring='roc_auc',  \n",
    "                       return_train_score=False\n",
    "                      )\n",
    "    \n",
    "    clf.fit(final_stacked_data.drop(['y'],axis=1), final_stacked_data['y'])\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    print('best_score =', clf.best_score_, clf.best_params_)\n",
    "df_grid = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params']).sort_values(by='best_score', ascending=False)\n",
    "df_grid.to_csv('grid_search_meta_results.csv', index=False)\n",
    "df_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.read_csv('grid_search_meta_results.csv').sort_values(by='best_score', ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
