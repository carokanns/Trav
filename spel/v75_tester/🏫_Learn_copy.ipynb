{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "### Kopia av 2_üè´_Learn.py\n",
    "### I ett f√∂rsta steg inf√∂r travdata\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END 09.19.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "import typ_copy as tp\n",
    "import travdata as td\n",
    "import V75_scraping as vs\n",
    "import concurrent.futures\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "# import streamlit as st\n",
    "from logging import PlaceHolder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, mean_absolute_error\n",
    "# import streamlit as st\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 260)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "\n",
    "\n",
    "# sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\modeller\\\\')\n",
    "\n",
    "\n",
    "pref =  '../'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "streamlit grejer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## streamlit grejer #############################################\n",
    "# st.set_page_config(page_title=\"V75 Learning\", page_icon=\"üè´\")\n",
    "# st.markdown(\"# üè´ V75 Learning\")\n",
    "print('st.sidebar.header(\"üè´ V75 Learning\")')\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "# -------------- skapa test-modeller\n",
    "    #               name,   #h√§st     proba,    kelly,  #motst,  motst_diff, #fav, only_cl, streck, test,  pref\n",
    "test1 = tp.Typ('test1',  True,    True,     False,       0,   False,      0,   False,    True,  True, pref=pref)\n",
    "test2 = tp.Typ('test2',  True,    True,     False,       0,   False,      0,   False,    False, True, pref=pref)\n",
    "test3 = tp.Typ('test3',  True,    True,     False,       0,   False,      0,   False,    False, True, pref=pref)\n",
    "test4 = tp.Typ('test4',  True,    True,     False,       0,   False,      0,   False,    True,  False, pref=pref)\n",
    "\n",
    "modeller = [test1, test2, test3, test4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "################################################\n",
    "#              Web scraping                    #\n",
    "################################################\n",
    "\n",
    "def v75_scraping():\n",
    "    df = vs.v75_scraping(history=True, resultat=True, headless=True)\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n",
    "\n",
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    \n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "################################################\n",
    "#              Web scraping                    #\n",
    "################################################\n",
    "\n",
    "def v75_scraping():\n",
    "    df = vs.v75_scraping(history=True, resultat=True, headless=True)\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n",
    "\n",
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    \n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "###############################################\n",
    "#              LEARNING                       #\n",
    "###############################################\n",
    "\n",
    "def f√∂rbered_old(df, meta_fraction=None):\n",
    "    # F√∂ljande datum saknar avd==5 och kan inte anv√§ndas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "\n",
    "    # l√§s in FEATURES.txt\n",
    "    with open(pref+'FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        features = f.read().splitlines()\n",
    "\n",
    "    X = X[features]\n",
    "\n",
    "    assert len(features) == len(\n",
    "        X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'\n",
    "    assert set(features) == set(\n",
    "        X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "\n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    if meta_fraction == 0:\n",
    "        # no meta data\n",
    "        return X, y, None, None\n",
    "\n",
    "    # use a fraction for meta data\n",
    "    meta_antal = int(len(X.datum.unique())*meta_fraction)\n",
    "    meta_datum = X.datum.unique()[-meta_antal:]\n",
    "\n",
    "    X_val = X.loc[X.datum.isin(meta_datum)]\n",
    "    y_val = y[X_val.index]\n",
    "    X = X.loc[~X.datum.isin(meta_datum)]\n",
    "    y = y.loc[X.index]\n",
    "    return X, y, X_val, y_val\n",
    "\n",
    "\n",
    "def concat_data_old(df_all, df_ny, save=True):\n",
    "    df_ny = df_ny[df_all.columns]\n",
    "    df_all = pd.concat([df_all, df_ny])\n",
    "    # remove duplicates\n",
    "    all_shape = df_all.shape\n",
    "\n",
    "    df_all = df_all.drop_duplicates(subset=['datum', 'avd', 'h√§st'])\n",
    "    assert df_all.shape[0] + \\\n",
    "        90 > all_shape[0], f'{df_all.shape[0]+90} should be more than {all_shape[0]}'\n",
    "    assert df_all.shape[1] == all_shape[1], f'{df_all.shape[1]} should be {all_shape[1]}'\n",
    "    if save == True:\n",
    "        df_all.to_csv(pref+'all_data.csv', index=False)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### RidgeClassifier (meta model) #####\n",
    "def learn_meta_ridge_model(X, y, save=True):\n",
    "\n",
    "    with open(pref+'optimera/params_meta_ridge.json', 'r') as f:\n",
    "        params = json.load(f)['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    ridge_model = RidgeClassifier(**params, random_state=2022)\n",
    "\n",
    "    ridge_model.fit(X, y)\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_ridge.model', 'wb') as f:\n",
    "            pickle.dump(ridge_model, f)\n",
    "\n",
    "    return ridge_model\n",
    "\n",
    "##### RandomForestClassifier (meta model) #####\n",
    "\n",
    "\n",
    "def learn_meta_rf_model(X, y, save=True):\n",
    "\n",
    "    with open(pref+'optimera/params_meta_rf.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    rf_model = RandomForestClassifier(**params, n_jobs=6, random_state=2022)\n",
    "    rf_model.fit(X, y)\n",
    "\n",
    "    ######################### for testing ###############################\n",
    "    rf_train = X.copy(deep=True)\n",
    "    rf_train['y'] = y\n",
    "    rf_train.to_csv(pref+'rf_train.csv', index=False)\n",
    "    #########################              ###############################\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_rf.model', 'wb') as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "\n",
    "##### KNeighborsClassifier (meta model) #####\n",
    "def learn_meta_knn_model(X, y, save=True):\n",
    "    with open(pref+'optimera/params_meta_knn.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    knn_model = KNeighborsClassifier(**params, n_jobs=6)\n",
    "    knn_model.fit(X, y)\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_knn.model', 'wb') as f:\n",
    "            pickle.dump(knn_model, f)\n",
    "\n",
    "    return knn_model\n",
    "\n",
    "def learn_meta_et_model(X, y, save=True):\n",
    "    with open(pref+'optimera/params_meta_et.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "\n",
    "    params = {'n_estimators':10, 'max_depth':None, 'min_samples_leaf':5}\n",
    "    et_model = ExtraTreesClassifier(**params, n_jobs=6, random_state=2022)\n",
    "    et_model.fit(X, y)\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_et.model', 'wb') as f:\n",
    "            pickle.dump(et_model, f)\n",
    "\n",
    "    return et_model\n",
    "\n",
    "def prepare_stack_data(stack_data_, ENC=None):\n",
    "    \"\"\"Hantera missing values, NaN, etc f√∂r meta-modellerna\"\"\"\n",
    "\n",
    "    assert 'y' in stack_data_.columns, 'y is missing in stack_data'\n",
    "    stack_data = stack_data_.copy()\n",
    "    stack_data.y = stack_data.y.astype(int)\n",
    "\n",
    "    \"\"\" rensa bort features som inte ska anv√§ndas \"\"\"\n",
    "    # stack_data.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "    #             'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "\n",
    "    \"\"\" Fyll i saknade numeriska v√§rden med 0 \"\"\"\n",
    "    numericals = stack_data.drop('y', axis=1).select_dtypes(exclude=['object']).columns\n",
    "    stack_data[numericals] = stack_data[numericals].fillna(0)\n",
    "\n",
    "    \"\"\" Fyll i saknade kategoriska v√§rden med 'missing' \"\"\"\n",
    "    categoricals = stack_data.drop('y',axis=1).select_dtypes(include=['object']).columns\n",
    "    stack_data[categoricals] = stack_data[categoricals].fillna('missing')\n",
    "\n",
    "    # \"\"\" Hantera high cardinality \"\"\"\n",
    "    # cardinality_list=['h√§st','kusk','h1_kusk','h2_kusk','h3_kusk','h4_kusk','h5_kusk']\n",
    "\n",
    "    \"\"\" Target encoding\"\"\"\n",
    "    target_encode_list = ['bana', 'h√§st', 'kusk', 'k√∂n', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana',\n",
    "                          'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
    "\n",
    "    y = stack_data['y']\n",
    "    if ENC is None:\n",
    "        ENC = TargetEncoder(cols=target_encode_list, min_samples_leaf=20, smoothing=10).fit(stack_data, y)\n",
    "       \n",
    "    stack_data = ENC.transform(stack_data)\n",
    "\n",
    "    return stack_data, ENC\n",
    "\n",
    "def learn_meta_models(stack_data, meta_features, save=True):\n",
    "    \"\"\" all meta models will be fitted on X and y \"\"\"  \n",
    "    \n",
    "    stack_data, ENC = prepare_stack_data(stack_data)\n",
    "     # save encoder\n",
    "    with open(pref+'modeller/encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(ENC, f)\n",
    "        \n",
    "    stack_data.to_csv(pref+'prepared_stack_data.csv', index=False)   # for testing purposes\n",
    "    \n",
    "    Ridge_Classifier = learn_meta_ridge_model(stack_data[meta_features], stack_data.y, save=save)\n",
    "    RandomForest_Classifier = learn_meta_rf_model(stack_data[meta_features], stack_data.y, save=save)\n",
    "    Knn_model = learn_meta_knn_model(stack_data[meta_features], stack_data.y, save=save)\n",
    "    ExtraTrees_Classifier = learn_meta_et_model(stack_data[meta_features], stack_data.y, save=save)\n",
    "\n",
    "    return Ridge_Classifier, RandomForest_Classifier, Knn_model, ExtraTrees_Classifier, ENC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nytt ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skapa_data_f√∂r_datum(df_, curr_datum_ix, frac=0.5):\n",
    "    df = df_.copy()\n",
    "    datumar = df.datum.unique()\n",
    "    curr_datum = datumar[curr_datum_ix]\n",
    "    base_datum_ix = int(len(datumar[:curr_datum_ix]) * frac)  # base models\n",
    "\n",
    "    base_datum = datumar[base_datum_ix]\n",
    "    X_train = df.query(f'datum < @base_datum')\n",
    "    y_train = X_train.y\n",
    "    X_train = X_train.drop('y', axis=1)\n",
    "\n",
    "    X_meta = df.query(f'datum >= @base_datum and datum < @curr_datum')\n",
    "    y_meta = X_meta.y\n",
    "    X_meta = X_meta.drop('y', axis=1)\n",
    "\n",
    "    X_curr = df.query(f'datum == @curr_datum')\n",
    "    y_curr = X_curr.y\n",
    "    X_curr = X_curr.drop(['y'], axis=1)\n",
    "\n",
    "    return X_train, y_train, X_meta, y_meta, X_curr, y_curr\n",
    "\n",
    "\n",
    "\n",
    "def skapa_stack_data(model, name, X_meta, stack_data):\n",
    "    \"\"\"Skapa stack_data\"\"\"\n",
    "    assert 'y' in stack_data.columns, 'y is missing in stack_data'\n",
    "    this_proba = model.predict(X_meta)\n",
    "    # print(f'X_meta.shape = {X_meta.shape} this_proba.shape={this_proba.shape}')\n",
    "\n",
    "    # Bygg up meta-kolumnerna (proba) f√∂r denns modell\n",
    "    nr = name[3:]\n",
    "    stack_data['proba'+nr] = this_proba\n",
    "    return stack_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_val_data(df_work, val_fraction):\n",
    "    y=df_work.y\n",
    "    X=df_work.drop('y', axis=1)\n",
    "    if val_fraction == 0:\n",
    "        # no validation data\n",
    "        X_val, y_val = None, None\n",
    "    else:    \n",
    "        # use a fraction for meta data\n",
    "        datumar=df_work.datum.unique()\n",
    "        val_antal = int(len(datumar)*val_fraction)\n",
    "        val_datum = datumar[-val_antal:]\n",
    "\n",
    "        X_val = X.loc[X.datum.isin(val_datum)]\n",
    "        y_val = y[X_val.index]\n",
    "        X = X.loc[~X.datum.isin(val_datum)]\n",
    "        y = y.loc[X.index]\n",
    "    return X, y, X_val, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_modeller(modeller, X_train, y_train, X_meta, y_meta):\n",
    "    ############################################################################################################\n",
    "    #                        H√§r g√∂rs en f√∂rsta learn av modeller och sedan skapas stack_data\n",
    "    #                        - Learn modeller p√• X,y\n",
    "    #                        - Ha en egen skapa_stack_funktion (som ocks√• anv√§nds l√§ngre ner)\n",
    "    #                           - Skapa stack_data med predict X_meta med nya modellerna\n",
    "    #                           - Spara √§ven X_meta, y_meta i stack_data\n",
    "    ############################################################################################################\n",
    "    stack_data = X_meta.copy()\n",
    "    stack_data['y'] = y_meta\n",
    "    assert 'y' in stack_data.columns, '1. y is missing in stack_data'\n",
    "    for model in modeller:\n",
    "        name = model.name\n",
    "        print(f'first Learn {name} {X_train.datum.min()} -{X_train.datum.max()}')\n",
    "\n",
    "        model.learn(X_train, y_train, params=None, save=True)\n",
    "\n",
    "        stack_data = skapa_stack_data(model, name, X_meta, stack_data)\n",
    "\n",
    "    assert 'y' in stack_data.columns, '3. y is missing in stack_data'\n",
    "    # stack_data, enc = prepare_stack_data(stack_data)\n",
    "\n",
    "    return stack_data\n",
    "\n",
    "def normal_learning(modeller, meta_modeller, X_train, y_train, X_meta, y_meta):\n",
    "    stack_data = learn_modeller(modeller, X_train, y_train, X_meta, y_meta)\n",
    "    assert 'y' in stack_data.columns, 'y is missing in stack_data'\n",
    "    stack_data.to_csv('first_stack_data.csv', index=False)\n",
    "\n",
    "    # \"\"\" Learn meta_modeller p√• stack_data \"\"\"\n",
    "    meta_features = stack_data.drop(['datum', 'avd','y'],axis=1).columns.to_list()\n",
    "    _, _, _, _, ENC = learn_meta_models(stack_data, meta_features)\n",
    "    \n",
    "    return stack_data[meta_features + ['y']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TimeSeriesLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit learning models\n",
    "def TimeSeries_learning(df_ny_, modeller, n_splits=5, val_fraction=0.25, save=True, learn_models=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack med {1 - meta_fraction} av X fr√•n alla modeller. Anv√§nds som input till meta_model.\n",
    "        - learn_models=True betyder att vi b√•de g√∂r en learning och skapar en stack\n",
    "        - learn_models=False betyder att vi bara skapar en stack och d√• har param save ingen funktion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Skapa v75-instans\n",
    "    v75 = td.v75(pref=pref)\n",
    "    \n",
    "    base_features = v75.get_df().columns.to_list()\n",
    "    \n",
    "    if df_ny_ is not None:  # Har vi en ny omg√•ng?\n",
    "        df_ny = df_ny_[base_features].copy()\n",
    "        v75.concat(df_ny, update_work=True, save=True)\n",
    "\n",
    "    # H√§mta data fr√•n v75\n",
    "    _ = v75.f√∂rbered_data(missing_num=False)  # num hanteras av catboost\n",
    "    df_work = v75.test_l√§gg_till_kolumner()\n",
    "    \n",
    "    # base_ix = 100  # antal omg√•ngar som vi startar bas-modellerna fr√•n i backtesting\n",
    "\n",
    "    # datumar = df_work.datum.unique()\n",
    "    # startdatum = datumar[base_ix]\n",
    "\n",
    "    # assert val_fraction==0, 'meta_fraction m√•ste vara 0 tills vidare'\n",
    "    X, y, X_val, y_val = hold_out_val_data(df_work, val_fraction)\n",
    "    \n",
    "    validation_text = \"\"\n",
    "\n",
    "    if X_val is not None:\n",
    "        validation_text = f', Validation: {X_val.datum.iloc[0]} - {X_val.datum.iloc[-1]}'\n",
    "\n",
    "    # st.info(f'Train: {X.datum.iloc[0]} - {X.datum.iloc[-1]} {validation_text}')\n",
    "    print(f\"Train: {X.datum.iloc[0]} - {X.datum.iloc[-1]} {validation_text}\")\n",
    "\n",
    "    ts = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    stacked_data = pd.DataFrame()\n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 1: Learn the models on ts.split X_train and predict on X_test  #\n",
    "    ###############################################################################\n",
    "    # st.write('Skapar stacked_data till meta')\n",
    "    # my_bar = st.progress(0)\n",
    "\n",
    "    step = 1/(n_splits*len(modeller))-0.0000001\n",
    "    steps = 0.0\n",
    "\n",
    "    for enum, (train_index, test_index) in enumerate(ts.split(X, y)):\n",
    "        print('shape of X', X.shape, 'shape of X_train', X.iloc[train_index].shape, 'shape of X_test', X.iloc[test_index].shape)\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        temp_stack = X_test.copy()\n",
    "        temp_stack['y'] = y_test\n",
    "        for model in modeller:\n",
    "            steps += step\n",
    "            # progress bar continues to complete from 0 to 100\n",
    "            \n",
    "            # my_bar.progress(steps)\n",
    "\n",
    "            if learn_models:\n",
    "                with open(pref+'optimera/params_'+model.name+'.json', 'r') as f:\n",
    "                    params = json.load(f)\n",
    "                    params = params['params']\n",
    "                # learn p√• X_train-delen\n",
    "                \n",
    "                cbc = model.learn(X_train, y_train, X_test,\n",
    "                                y_test, params=params, save=save)\n",
    "\n",
    "            # predict the new fitted model on X_test-delen\n",
    "            nr = model.name[3:]\n",
    "            this_proba = model.predict(X_test)\n",
    "\n",
    "            # Bygg up meta-kolumnen proba f√∂r denns modell\n",
    "            temp_stack['proba'+nr] = this_proba\n",
    "\n",
    "        if stacked_data.empty:\n",
    "            stacked_data = temp_stack.copy()\n",
    "        else:        \n",
    "            stacked_data = pd.concat([stacked_data, temp_stack], ignore_index=True)\n",
    "        stacked_data.y = stacked_data.y.astype(int)\n",
    "\n",
    "    \n",
    "    # stacked_data_y = stacked_data.pop('y')\n",
    "    meta_features = stacked_data.drop(['datum', 'avd', 'y'], axis=1).columns.to_list()\n",
    "\n",
    "    # my_bar.progress(1.0)\n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 2:       Learn the meta models                                 #\n",
    "    ###############################################################################\n",
    "    # st.write('Learning meta models')\n",
    "    print('Learning meta models')\n",
    "    _, _, _, _, ENC = learn_meta_models(stacked_data, meta_features)\n",
    "    # save ENC\n",
    "    with open(pref+'modeller/encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(ENC, f)    \n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 3: learn models on all of X - what iteration to use?           #\n",
    "    ###############################################################################\n",
    "    # st.write('Learn models on all of Train')\n",
    "    print('Learn models on all of Train')\n",
    "    \n",
    "    # my_bar2 = st.progress(0)\n",
    "    ant_meta_models = 4\n",
    "    step = 1/(ant_meta_models) - 0.0000001\n",
    "    steps = 0.0\n",
    "    # my_bar2.progress(steps)\n",
    "\n",
    "    for model in modeller:\n",
    "        steps += step\n",
    "        # my_bar2.progress(steps)\n",
    "        if learn_models:\n",
    "            with open(pref+'optimera/params_'+model.name+'.json', 'r') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "            params = params['params']\n",
    "            cbc = model.learn(X, y, None, None, \n",
    "                              iterations=500, \n",
    "                              params=params, \n",
    "                              save=save)\n",
    "\n",
    "    # my_bar2.progress(1.0)\n",
    "    # st.empty()\n",
    "    \n",
    "    return stacked_data[meta_features + ['y']]\n",
    "\n",
    "\n",
    "def skapa_stack_learning(X_, y,meta_features):\n",
    "    # F√∂r validate\n",
    "    X = X_.copy()\n",
    "    print(X.shape)\n",
    "    print(len(meta_features))\n",
    "    stacked_data = X[meta_features].copy()\n",
    "    for model in modeller:\n",
    "        part = model.name[3:]\n",
    "        stacked_data['proba'+part] = model.predict(X)\n",
    "        meta_features += ['proba'+part]\n",
    "\n",
    "    assert list(stacked_data.columns) == meta_features, f'columns in stacked_data is wrong {list(stacked_data.columns)} \\n {meta_features}'\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    return stacked_data[meta_features], meta_features, y  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "##############################################################\n",
    "#                     VALIDATE                               #\n",
    "##############################################################\n",
    "\n",
    "def predict_meta_ridge_model(X, ridge_model=None):\n",
    "    if ridge_model is None:\n",
    "        with open(pref+'modeller/meta_ridge.model', 'rb') as f:\n",
    "            ridge_model = pickle.load(f)\n",
    "\n",
    "    return ridge_model._predict_proba_lr(X)\n",
    "\n",
    "\n",
    "def predict_meta_rf_model(X, rf_model=None):\n",
    "    if rf_model is None:\n",
    "        with open(pref+'modeller/meta_rf.model', 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "\n",
    "    return rf_model.predict_proba(X)\n",
    "\n",
    "\n",
    "def predict_meta_et_model(X, et_model=None):\n",
    "    if et_model is None:\n",
    "        with open(pref+'modeller/meta_et.model', 'rb') as f:\n",
    "            et_model = pickle.load(f)\n",
    "\n",
    "    return et_model.predict_proba(X)\n",
    "\n",
    "def predict_meta_knn_model(X, knn_model=None):\n",
    "    if knn_model is None:\n",
    "        with open(pref+'modeller/meta_knn.model', 'rb') as f:\n",
    "            knn_model = pickle.load(f)\n",
    "\n",
    "    return knn_model.predict_proba(X)\n",
    "\n",
    "def predict_meta_mean(preds, type):   # type='geometric' or 'arithmetic'\n",
    "    if type=='arithmetic':\n",
    "        return (preds.rf + preds.et + preds.ridge + preds.knn)/4\n",
    "    \n",
    "    return (preds.rf * preds.et * preds.ridge * preds.knn)**(1/4)\n",
    "\n",
    "def predict_meta_models(stack_data, meta_features):\n",
    "    preds = pd.DataFrame(columns=['rf', 'ridge', 'knn', 'meta'])\n",
    "    \n",
    "    with open(pref+'modeller/meta_enc.pkl', 'rb') as f:\n",
    "        ENC = pickle.load(f)\n",
    "        \n",
    "    stack_data, _ = prepare_stack_data(stack_data, ENC)\n",
    "    preds['rf'] = predict_meta_rf_model(stack_data[meta_features])[:, 1]\n",
    "    preds['ridge'] = predict_meta_ridge_model(stack_data[meta_features])[:, 1]\n",
    "    preds['knn'] = predict_meta_knn_model(stack_data[meta_features])[:, 1]\n",
    "    preds['et']  = predict_meta_et_model(stack_data[meta_features])[:, 1]\n",
    "    preds['meta'] = predict_meta_mean(preds, type='geometric')\n",
    "    \n",
    "    return preds\n",
    "    \n",
    "\n",
    "# write the scores\n",
    "\n",
    "\n",
    "def display_scores(y_true, y_pred, spelade):\n",
    "    # st.write('AUC', round(roc_auc_score(y_true, y_pred), 5), 'F1', round(f1_score(y_true, y_pred), 5), 'Acc', round(\n",
    "    #     accuracy_score(y_true, y_pred), 5), 'MAE', round(mean_absolute_error(y_true, y_pred), 5), '\\n', spelade)\n",
    "    print('AUC', round(roc_auc_score(y_true, y_pred), 5), 'F1', round(f1_score(y_true, y_pred), 5), 'Acc', round(\n",
    "        accuracy_score(y_true, y_pred), 5), 'MAE', round(mean_absolute_error(y_true, y_pred), 5), '\\n', spelade)\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def find_threshold(y_pred, fr, to, margin):\n",
    "    \"\"\" hitta threshold som ger 2.5 spelade per avdelning \"\"\"\n",
    "    thresh = 0\n",
    "    cnt = 0\n",
    "    # make a binary search\n",
    "    while cnt < 1000:\n",
    "        thresh = (fr + to) / 2\n",
    "        antal_spelade_per_avd = 12 * sum(y_pred > thresh)/len(y_pred)\n",
    "        if (antal_spelade_per_avd > (2.5 - margin)) and (antal_spelade_per_avd < (2.5 + margin)):\n",
    "            break\n",
    "\n",
    "        if antal_spelade_per_avd > 2.5:\n",
    "            fr = thresh-0.00001\n",
    "        else:\n",
    "            to = thresh+0.00001\n",
    "        cnt += 1\n",
    "\n",
    "    # print('ant', cnt, 'thresh', round(thresh, 4))\n",
    "    if cnt >= 1000:\n",
    "        print('threshold not found', 'fr', round(fr, 6), 'to', round(to, 6))\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model, fr=0.0, to=0.9, margin=0.001):\n",
    "\n",
    "    #### F√∂rst:  hitta ett threshold som tippar ca 2.5 h√§star per avd ####\n",
    "    # thresh = 0\n",
    "    # for thresh in np.arange(fr, to, step):\n",
    "    #     cost = 12*sum(y_pred > thresh)/len(y_pred)\n",
    "    #     if cost < 2.5:\n",
    "    #         break\n",
    "    thresh = round(find_threshold(y_pred, fr, to, margin), 4)\n",
    "    print(f'Threshold: {thresh}\\n')\n",
    "    y_pred = (y_pred > thresh).astype(int)\n",
    "    # confusion_matrix_graph(y_true, y_pred, f'{model} threshold={thresh}')\n",
    "\n",
    "    #### Sedan: confusion matrix graph ####\n",
    "    title = f'{model} threshold={thresh}'\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred,)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(font_scale=2.0)\n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", linewidths=.5,\n",
    "                square=True, cmap='Blues_r')\n",
    "\n",
    "    # increase font size\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    # st.write(fig)\n",
    "\n",
    "    # read dict from disk\n",
    "    try:\n",
    "        with open(pref+'modeller/meta_scores.pkl', 'rb') as f:\n",
    "            meta_scores = pickle.load(f)\n",
    "    except:\n",
    "        # st.write('No meta_scores.pkl found')\n",
    "        print('No meta_scores.pkl found')\n",
    "        meta_scores = {'knn': 0, 'rf': 0, 'ridge': 0, 'et': 0}\n",
    "\n",
    "    #### print scores ####\n",
    "    typ_AUC = display_scores(\n",
    "        y_true, y_pred, f'spelade per lopp: {round(12 * sum(y_pred)/len(y_pred),4)}')\n",
    "    meta_scores[model] = float(typ_AUC)\n",
    "    #### save dict to disk ####\n",
    "    with open(pref+'modeller/meta_scores.pkl', 'wb') as f:\n",
    "        pickle.dump(meta_scores, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate(drop=[], fraction=None):\n",
    "    # Skapa v75-instans\n",
    "    v75 = td.v75(pref=pref)\n",
    "\n",
    "    base_features = v75.get_df().columns.to_list()\n",
    "\n",
    "    # H√§mta data fr√•n v75\n",
    "    _ = v75.f√∂rbered_data(missing_num=False)  # num hanteras av catboost\n",
    "    df_work = v75.test_l√§gg_till_kolumner()\n",
    "\n",
    "    # st.info('skall endast  k√∂ras efter \"Learn for Validation\"')\n",
    "    # df_all = pd.read_csv(pref+'all_data.csv')\n",
    "\n",
    "    \n",
    "    _, _, X_val, y_val = hold_out_val_data(df_work, fraction)\n",
    "    # st.info(f'Validerar p√•:  {X_val.datum.iloc[0]} - -{X_val.datum.iloc[-1]}')\n",
    "    print(f'Validerar p√•:  {X_val.datum.iloc[0]} - -{X_val.datum.iloc[-1]}')\n",
    "\n",
    "    # # create the stack from validation data\n",
    "    # meta_features = all columns except avd and datum\n",
    "    meta_features = [col for col in df_work.columns if col not in ['datum','avd', 'y']]\n",
    "\n",
    "    stacked_val, meta_features, y_val = skapa_stack_learning(X_val, y_val, meta_features)\n",
    "\n",
    "    stacked_val = stacked_val.drop(drop, axis=1)\n",
    "\n",
    "    # ##############################################################\n",
    "    # #                          Meta models                       #\n",
    "    # ##############################################################\n",
    "\n",
    "    stacked_val['y'] = y_val\n",
    "    y_true = y_val.values\n",
    "\n",
    "    y_preds = predict_meta_models(stacked_val, meta_features)\n",
    "\n",
    "    # ############## write y_true to file for testing ##############\n",
    "    # # first make y_true a dataframe\n",
    "    # rf_y_true = pd.DataFrame(y_true, columns=['y'])\n",
    "    # rf_y_true.to_csv(pref+'rf_y_true.csv', index=False)\n",
    "    # ##############################################################\n",
    "\n",
    "    print(\"st.info('f√∂rbereder meta plot')\")\n",
    "    plot_confusion_matrix(y_true, y_preds.meta, 'meta', fr=0.0, to=0.9)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    print(\"st.info('f√∂rbereder rf plot')\")\n",
    "    plot_confusion_matrix(y_true, y_preds.rf, 'rf',\n",
    "                        fr=0.0, to=1.0, margin=0.01)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    print(\"st.info('f√∂rbereder et plot')\")\n",
    "    plot_confusion_matrix(y_true, y_preds.et, 'et',\n",
    "                      fr=0.0, to=1.0, margin=0.01)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    print(\"st.info('f√∂rbereder knn plot')\")\n",
    "    plot_confusion_matrix(y_true, y_preds.knn, 'knn', fr=0.0, to=0.9)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    print(\"st.info('f√∂rbereder ridge plot')\")\n",
    "    plot_confusion_matrix(y_true, y_preds.ridge, 'ridge', fr=0.0, to=0.9)\n",
    "    # # placeholder.empty()\n",
    "\n",
    "    # st.write('\\n')\n",
    "    # st.info('f√∂rbereder lasso plot')\n",
    "    # plot_confusion_matrix(y_true, predict_meta_model(stacked_val, meta_model='lasso'),\n",
    "    #                       'lasso', fr=0.0, to=0.9)\n",
    "    # # placeholder.empty()\n",
    "\n",
    "    # stacked_val['y'] = y_true\n",
    "    # stacked_val['avd'] = X_val.avd.values\n",
    "\n",
    "    # ################################################################\n",
    "    # #                         proba 6, 1, 9, (16)                  #\n",
    "    # ################################################################\n",
    "    # st.write('\\n')\n",
    "    # for model in typer:\n",
    "    #     st.write('\\n')\n",
    "    #     name = 'proba' + model.name[3:]\n",
    "    #     y_pred = stacked_val[name]\n",
    "    #     plot_confusion_matrix(y_true, y_pred, name, fr=0.0, to=0.9)\n",
    "\n",
    "#%%\n",
    "##############################################################\n",
    "#            FINAL LEARNING                                  #\n",
    "##############################################################\n",
    "\n",
    "def final_learning(modeller, n_splits=5):\n",
    "    # st.info('Final learning on all the data')\n",
    "    print('Final learning on all the data')\n",
    "    _ = TimeSeries_learning( None, modeller, n_splits=n_splits, meta_fraction=0, save=True)\n",
    "\n",
    "    # st.info('Step 2: Final learn meta model')\n",
    "    # _, _, _, _, enc = learn_meta_models(stacked_data, meta_featues)\n",
    "\n",
    "    # st.success('‚úîÔ∏è Final learning done')\n",
    "    print('Final learning done')\n",
    "\n",
    "#%%\n",
    "def scrape(full=True):\n",
    "    # scraping.write('Starta web-scraping f√∂r ny data')\n",
    "    # with st.spinner('Ta det lugnt!'):\n",
    "        # st.image('winning_horse.png')  # ,use_column_width=True)\n",
    "        #####################\n",
    "        # start v75_scraping as a thread\n",
    "        #####################\n",
    "        i = 0.0\n",
    "        # placeholder = st.empty()\n",
    "        seconds = 0\n",
    "        # my_bar = st.progress(i)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(v75_scraping)\n",
    "            while future.running():\n",
    "                time.sleep(1)\n",
    "                seconds += 1\n",
    "                # placeholder.write(f\"‚è≥ {seconds} sekunder\")\n",
    "                i += 1/65\n",
    "                if i < 0.99:\n",
    "                    pass\n",
    "                    # my_bar.progress(i)\n",
    "            # my_bar.progress(1.0)\n",
    "            # scraping.write('‚úîÔ∏è Scraping done, pls wait')\n",
    "            print('‚úîÔ∏è Scraping done, pls wait')\n",
    "            time.sleep(2)\n",
    "            df = future.result()\n",
    "            # scraping.write(f'‚úîÔ∏è {len(df)} rader h√§mtade')\n",
    "            print(f'‚úîÔ∏è {len(df)} rader h√§mtade')\n",
    "            \n",
    "            df.to_csv('sparad_scrape_learn.csv', index=False)\n",
    "\n",
    "        # st.balloons()\n",
    "        # my_bar.empty()\n",
    "        # placeholder.empty()\n",
    "\n",
    "        # st.session_state.df = df\n",
    "\n",
    "\n",
    "# models = [typ6, typ1, typ9]\n",
    "\n",
    "#%%\n",
    "# top = st.container()\n",
    "# buttons = st.container()\n",
    "# scraping = st.container()\n",
    "\n",
    "############################################\n",
    "#   Init session_state                     #\n",
    "############################################\n",
    "# with top:\n",
    "#     if 'fraction' not in st.session_state:\n",
    "#         st.session_state['fraction'] = 0.25\n",
    "fraction = 0.25\n",
    "#     if 'df' not in st.session_state:\n",
    "#         st.session_state['df'] = None\n",
    "omg_df = pd.read_csv(pref+'omg_att_spela_link.csv')\n",
    "urlen = omg_df.Link.values[0]\n",
    "datum = urlen.split('spel/')[1][0:10]\n",
    "#     if 'datum' not in st.session_state:\n",
    "#         omg_df = pd.read_csv('omg_att_spela_link.csv')\n",
    "#         urlen = omg_df.Link.values[0]\n",
    "#         datum = urlen.split('spel/')[1][0:10]\n",
    "#         st.session_state.datum = datum\n",
    "\n",
    "#     if 'datum' in st.session_state:\n",
    "#         datum = st.session_state['datum']\n",
    "#         year = int(datum[:4])\n",
    "#         month = int(datum[5:7])\n",
    "#         day = int(datum[8:])\n",
    "#         datum = st.sidebar.date_input(\n",
    "#             'V√§lj datum', datetime.date(year, month, day))\n",
    "#         datum = datum.strftime('%Y-%m-%d')\n",
    "\n",
    "        # if datum != st.session_state['datum']:\n",
    "        #     st.session_state['datum'] = datum\n",
    "        #     datum = \"https://www.atg.se/spel/\"+datum+\"/V75/\"\n",
    "        #     omg_df = pd.DataFrame([datum], columns=['Link'])\n",
    "        #     omg_df.to_csv('omg_att_spela_link.csv', index=False)\n",
    "\n",
    "    # st.header(f'Omg√•ng:  {st.session_state.datum}')\n",
    "\n",
    "###########################################\n",
    "# control flow with buttons               #\n",
    "###########################################\n",
    "# with buttons:\n",
    "#     if st.sidebar.button('scrape'):\n",
    "#         st.write(f'web scraping {st.session_state.datum}')\n",
    "#         try:\n",
    "#             scrape()\n",
    "#             del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "#         except:\n",
    "#             st.error(\n",
    "#                 \"Fel i web scraping. Kolla att resultat finns f√∂r datum och internet √§r tillg√§ngligt\")\n",
    "\n",
    "#     if st.sidebar.button('reuse scrape'):\n",
    "#         # del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "#         try:\n",
    "#             df = pd.read_csv('sparad_scrape_learn.csv')\n",
    "#             st.session_state.df = df\n",
    "#             if df.datum.iloc[0] != st.session_state.datum:\n",
    "#                 st.error(\n",
    "#                     f'Datum i data = {df.datum.iloc[0]} \\n\\n √§r inte samma som i omg√•ng')\n",
    "#             else:\n",
    "#                 st.success(f'inl√§st data med datum = {df.datum.iloc[0]}')\n",
    "#         except:\n",
    "#             # write error message\n",
    "#             st.error('Ingen data sparad')\n",
    "\n",
    "#     if st.session_state.df is not None:\n",
    "#         if st.sidebar.button('Learn for validation'):\n",
    "#             st.write('TimeSeries learning for validation')\n",
    "#             fraction = st.session_state.fraction\n",
    "#             df = st.session_state.df\n",
    "#             st.write(\n",
    "#                 f'learn models and meta models on first {(1-fraction)*100} % of the data')\n",
    "\n",
    "#             stacked_data = TimeSeries_learning(\n",
    "#                 df, yper, n_splits=5, meta_fraction=fraction, save=True, learn_models=True)\n",
    "#             st.success('‚úîÔ∏è TimeSeries learning done')\n",
    "\n",
    "#         if st.sidebar.button('Validate'):\n",
    "#             validate(fraction=st.session_state.fraction)\n",
    "\n",
    "#         if st.sidebar.button('Final learning'):\n",
    "#             final_learning(yper)\n",
    "\n",
    "#         if st.sidebar.button('Clear'):\n",
    "#             st.empty()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√∂r Testerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "button = 'test'   # 'scrape'  # 'reuse'  # 'learn'  # 'validate'  # 'final'  # 'clear'\n",
    "\n",
    "\n",
    "if button == 'scrape':\n",
    "    print(f'web scraping {datum}')\n",
    "    try:\n",
    "        scrape()\n",
    "        # del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "    except:\n",
    "        print(\"Fel i web scraping. Kolla att resultat finns f√∂r datum och internet √§r tillg√§ngligt\")\n",
    "else: # resuse scrape\n",
    "    try:\n",
    "        df_ny = pd.read_csv(pref+'sparad_scrape_learn.csv')\n",
    "        # st.session_state.df = df\n",
    "    except:\n",
    "        # write error message\n",
    "        print('Ingen data sparad')\n",
    "        \n",
    "if button=='learn':\n",
    "    print('TimeSeries learning for validation')\n",
    "    try:\n",
    "        df_ny = pd.read_csv(pref+'sparad_scrape_learn.csv')\n",
    "        loaded=True\n",
    "        # st.session_state.df = df\n",
    "    except:\n",
    "        loaded=False\n",
    "        # write error message\n",
    "        print('Ingen data sparad')\n",
    "    if loaded:\n",
    "        print(f'learn models and meta models on first {(1-fraction)*100} % of the data')\n",
    "\n",
    "        stacked_data = TimeSeries_learning(df_ny, modeller, n_splits=5, val_fraction=fraction, save=True, learn_models=True)\n",
    "        display(stacked_data)\n",
    "        print('‚úîÔ∏è TimeSeries learning done')\n",
    "        \n",
    "if button == 'validate':\n",
    "    print('Validation')\n",
    "\n",
    "    # validate(fraction=st.session_state.fraction)\n",
    "    validate(fraction=fraction)\n",
    "\n",
    "if button == 'test':\n",
    "    print('test inheritance')\n",
    "    \n",
    "    class v75_ny(td.v75):\n",
    "        def __init__(self, df_ny, pref=''):  #, filnamn='all_data.csv', pref=''):\n",
    "            \"\"\" init - used for df_ny in order to reuse the code in v75 \"\"\"\n",
    "            self.pref=pref\n",
    "            self.df = df_ny\n",
    "            self.work_df = self.df.copy()   # arbetskopia \n",
    "            \n",
    "    #<<<<<<<<<<<<<<<<<<<<< Skapa test-data<<<<<<<<<<<<<<<\n",
    "    v75 = td.v75(pref=pref)\n",
    "    df_work = v75.get_work_df()\n",
    "    # # H√§mta data fr√•n v75\n",
    "    # _ = v75.f√∂rbered_data(missing_num=False)  # num hanteras av catboost\n",
    "    # df_work = v75.test_l√§gg_till_kolumner()\n",
    "    ny_df = df_work.iloc[0:10].copy()\n",
    "    ny_df.pop('plac')\n",
    "    #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    ny_ =v75_ny(ny_df,pref=pref)\n",
    "    # ny_.kolla()\n",
    "    \n",
    "    ny_.f√∂rbered_data(missing_num=False)\n",
    "    df_work = ny_.test_l√§gg_till_kolumner()\n",
    "    df_work.shape\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allsk√∂ns tester  \n",
    "Fr√•ga: Kan man merga olka dataframes med olika features till en meningsfull stack_data f√∂r meta_modellerna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det blir en massa Nan v√§rden i stack_data fr√•n de modeller som inte har samma features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# pickle load ENC  \n",
    "ENC = pickle.load(open('../modeller/meta_encoder.pkl', 'rb'))\n",
    "\n",
    "fes=ENC.feature_names\n",
    "fesat=pd.DataFrame(columns=fes)\n",
    "fesat\n",
    "fesat.drop([],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from category_encoders import *\n",
    "# import pandas as pd\n",
    "# from sklearn.datasets import load_boston\n",
    "# bunch = load_boston()\n",
    "# y = bunch.target\n",
    "# X = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "# enc = TargetEncoder(cols=['CHAS','RAD'], min_samples_leaf=20, smoothing=10).fit(X.drop(['B'],axis=1), y)\n",
    "# numeric_dataset = enc.transform(X.drop(['B'],axis=1))\n",
    "# # numeric_dataset\n",
    "# y  .shape, y.  shape, 'B√ÖDA G√ÖR BRA!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_data.csv\n",
      "Loading dataframe from the file: ../all_data.csv\n",
      "plac finns i df\n",
      "['datum', 'avd', 'bana', 'h√§st', 'kusk', 'streck', 'sp√•r', 'dist', 'lopp_dist', 'start', '√•lder', 'k√∂n', 'pris', 'h1_kusk', 'h1_bana', 'h1_sp√•r', 'h1_plac', 'h1_pris', 'h1_odds', 'h1_kmtid', 'h2_kusk', 'h2_bana', 'h2_sp√•r', 'h2_plac', 'h2_pris', 'h2_odds', 'h2_kmtid', 'h3_kusk', 'h3_bana', 'h3_sp√•r', 'h3_plac', 'h3_pris', 'h3_odds', 'h3_kmtid', 'h4_kusk', 'h4_bana', 'h4_sp√•r', 'h4_plac', 'h4_pris', 'h4_odds', 'h4_kmtid', 'h5_kusk', 'h5_bana', 'h5_sp√•r', 'h5_plac', 'h5_pris', 'h5_odds', 'h5_kmtid', 'h1_dist', 'h2_dist', 'h3_dist', 'h4_dist', 'h5_dist', 'h1_auto', 'h2_auto', 'h3_auto', 'h4_auto', 'h5_auto', 'h1_perf', 'h2_perf', 'h3_perf', 'h4_perf', 'h5_perf', 'senast', 'delta1', 'delta2', 'delta3', 'delta4', 'y', 'rel_kr', 'streck_avst', 'rel_rank', 'h1_samma_bana', 'h2_samma_bana', 'h3_samma_bana', 'h1_samma_kusk', 'h2_samma_kusk', 'h3_samma_kusk']\n",
      "streck: True i init f√∂r test1\n",
      "G√∂r denna till produktion\n",
      "streck: False i init f√∂r test2\n",
      "G√∂r denna till produktion\n",
      "streck: False i init f√∂r test3\n",
      "streck: False i init f√∂r test4\n"
     ]
    }
   ],
   "source": [
    "# Skapa v75-instans\n",
    "v75 = td.v75(pref=pref)\n",
    "\n",
    "base_features = v75.get_df().columns.to_list()\n",
    "\n",
    "# H√§mta data fr√•n v75\n",
    "_ = v75.f√∂rbered_data(missing_num=False)  # num hanteras av catboost\n",
    "df_work = v75.test_l√§gg_till_kolumner()\n",
    "print(df_work.columns.to_list())\n",
    "\n",
    "#               name,   #h√§st  #motst,  motst_diff, streck, test,  pref\n",
    "test1 = tp.Typ('test1',  False,   0,    False,      True,   True,  pref=pref)\n",
    "test2 = tp.Typ('test2',  False,   0,    False,      False,   True,  pref=pref)\n",
    "test3 = tp.Typ('test3',  True,    0,    False,      False,   False, pref=pref)\n",
    "test4 = tp.Typ('test4',  True,    3,    True,       False,   False, pref=pref)\n",
    "\n",
    "modeller = [test1, test2, test3, test4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(datetime.date(2022, 8, 13), datetime.date(2022, 8, 17))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bryt_datum = pd.Timestamp('2022-08-17')\n",
    "train = df_work.loc[df_work.datum < bryt_datum]\n",
    "test = df_work.loc[df_work.datum >= bryt_datum]\n",
    "\n",
    "train.datum.max(), test.datum.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'test1',\n",
       "  'ant_h√§star': False,\n",
       "  'motst_ant': 0,\n",
       "  'motst_diff': False,\n",
       "  'streck': True,\n",
       "  'pref': '../',\n",
       "  'rel_kr': True,\n",
       "  'rel_rank': True,\n",
       "  'streck_avst': True,\n",
       "  'hx_samma_bana': True,\n",
       "  'hx_sammam_kusk': True},\n",
       " {'name': 'test2',\n",
       "  'ant_h√§star': False,\n",
       "  'motst_ant': 0,\n",
       "  'motst_diff': False,\n",
       "  'streck': False,\n",
       "  'pref': '../',\n",
       "  'rel_kr': True,\n",
       "  'rel_rank': True,\n",
       "  'streck_avst': True,\n",
       "  'hx_samma_bana': True,\n",
       "  'hx_sammam_kusk': True},\n",
       " {'name': 'test3',\n",
       "  'ant_h√§star': True,\n",
       "  'motst_ant': 0,\n",
       "  'motst_diff': False,\n",
       "  'streck': False,\n",
       "  'pref': '../',\n",
       "  'rel_kr': False,\n",
       "  'rel_rank': False,\n",
       "  'streck_avst': False,\n",
       "  'hx_samma_bana': False,\n",
       "  'hx_sammam_kusk': False},\n",
       " {'name': 'test4',\n",
       "  'ant_h√§star': True,\n",
       "  'motst_ant': 3,\n",
       "  'motst_diff': True,\n",
       "  'streck': False,\n",
       "  'pref': '../',\n",
       "  'rel_kr': False,\n",
       "  'rel_rank': False,\n",
       "  'streck_avst': False,\n",
       "  'hx_samma_bana': False,\n",
       "  'hx_sammam_kusk': False})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.get_params(), test2.get_params(), test3.get_params(), test4.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning test1\n"
     ]
    }
   ],
   "source": [
    "# data = test1.prepare_for_model(train)\n",
    "# print('streck' in data.columns)\n",
    "model1=test1.learn(train.drop('y', axis=1), train['y'], save=False)\n",
    "proba1 = test1.predict(test.drop('y', axis=1), model=model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning test2\n",
      "drop streck\n"
     ]
    }
   ],
   "source": [
    "model2=test2.learn(train.drop('y', axis=1), train['y'], save=False)\n",
    "proba2 = test2.predict(test.drop('y', axis=1), model=model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning test3\n",
      "drop streck\n"
     ]
    }
   ],
   "source": [
    "model3=test3.learn(train.drop('y', axis=1), train['y'], save=False)\n",
    "proba3 = test3.predict(test.drop('y', axis=1), model=model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning test4\n",
      "drop streck\n"
     ]
    }
   ],
   "source": [
    "model4=test4.learn(train.drop('y', axis=1), train['y'], save=False)\n",
    "proba4 = test4.predict(test.drop('y', axis=1), model=model4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature Id  Importances\n",
      "0        rel_rank    58.833500\n",
      "1     streck_avst    22.161446\n",
      "2            h√§st     5.966338\n",
      "3          rel_kr     5.392149\n",
      "4            kusk     1.153969\n",
      "5         h3_odds     0.417064\n",
      "6            sp√•r     0.336664\n",
      "7         h1_odds     0.293777\n",
      "8         h2_odds     0.282564\n",
      "9         h4_odds     0.275877\n",
      "10        h1_kusk     0.271322\n",
      "11            k√∂n     0.219368\n",
      "12        h2_plac     0.202970\n",
      "13        h1_perf     0.201586\n",
      "14        h1_plac     0.173612\n",
      "15          √•lder     0.168131\n",
      "16        h3_perf     0.159713\n",
      "17        h2_perf     0.158035\n",
      "18       h2_kmtid     0.141174\n",
      "19       h1_kmtid     0.139084\n",
      "20        h3_plac     0.131870\n",
      "21        h1_bana     0.123209\n",
      "22        h4_perf     0.118882\n",
      "23        h2_pris     0.117009\n",
      "24        h2_bana     0.116475\n",
      "25        h3_pris     0.115992\n",
      "26        h5_perf     0.114948\n",
      "27        h4_bana     0.113802\n",
      "28        h3_bana     0.112843\n",
      "29        h5_pris     0.112609\n",
      "30        h5_odds     0.110291\n",
      "31        h5_bana     0.102725\n",
      "32         senast     0.101013\n",
      "33          start     0.083649\n",
      "34        h5_kusk     0.083258\n",
      "35       h3_kmtid     0.073411\n",
      "36        h1_pris     0.072249\n",
      "37         delta1     0.069040\n",
      "38        h1_dist     0.066719\n",
      "39       h5_kmtid     0.062160\n",
      "40       h4_kmtid     0.061576\n",
      "41        h2_kusk     0.060443\n",
      "42        h4_sp√•r     0.059609\n",
      "43      lopp_dist     0.054984\n",
      "44           bana     0.054096\n",
      "45         delta2     0.047092\n",
      "46         delta3     0.045701\n",
      "47        h4_plac     0.044505\n",
      "48        h3_dist     0.043617\n",
      "49         delta4     0.042014\n",
      "50           pris     0.040923\n",
      "51        h4_kusk     0.039528\n",
      "52  h2_samma_kusk     0.039499\n",
      "53        h4_pris     0.038498\n",
      "54        h1_sp√•r     0.038145\n",
      "55        h3_sp√•r     0.035484\n",
      "56        h2_sp√•r     0.034950\n",
      "57  h3_samma_kusk     0.033560\n",
      "58        h5_dist     0.033423\n",
      "59        h4_dist     0.028333\n",
      "60        h2_auto     0.024407\n",
      "61        h3_kusk     0.022295\n",
      "62        h2_dist     0.018266\n",
      "63        h5_auto     0.017622\n",
      "64  h1_samma_kusk     0.014753\n",
      "65           dist     0.014363\n",
      "66        h5_plac     0.014281\n",
      "67        h5_sp√•r     0.012023\n",
      "68  h3_samma_bana     0.011429\n",
      "69        h4_auto     0.009793\n",
      "70  h2_samma_bana     0.008029\n",
      "71        h3_auto     0.006261\n",
      "72        h1_auto     0.000000\n",
      "73  h1_samma_bana     0.000000\n",
      "['bana', 'h√§st', 'kusk', 'sp√•r', 'dist', 'lopp_dist', 'start', '√•lder', 'k√∂n', 'pris', 'h1_kusk', 'h1_bana', 'h1_sp√•r', 'h1_plac', 'h1_pris', 'h1_odds', 'h1_kmtid', 'h2_kusk', 'h2_bana', 'h2_sp√•r', 'h2_plac', 'h2_pris', 'h2_odds', 'h2_kmtid', 'h3_kusk', 'h3_bana', 'h3_sp√•r', 'h3_plac', 'h3_pris', 'h3_odds', 'h3_kmtid', 'h4_kusk', 'h4_bana', 'h4_sp√•r', 'h4_plac', 'h4_pris', 'h4_odds', 'h4_kmtid', 'h5_kusk', 'h5_bana', 'h5_sp√•r', 'h5_plac', 'h5_pris', 'h5_odds', 'h5_kmtid', 'h1_dist', 'h2_dist', 'h3_dist', 'h4_dist', 'h5_dist', 'h1_auto', 'h2_auto', 'h3_auto', 'h4_auto', 'h5_auto', 'h1_perf', 'h2_perf', 'h3_perf', 'h4_perf', 'h5_perf', 'senast', 'delta1', 'delta2', 'delta3', 'delta4', 'rel_kr', 'streck_avst', 'rel_rank', 'h1_samma_bana', 'h2_samma_bana', 'h3_samma_bana', 'h1_samma_kusk', 'h2_samma_kusk', 'h3_samma_kusk'] 74\n"
     ]
    }
   ],
   "source": [
    "# print(model1.get_feature_importance(prettified=True))\n",
    "print(model2.get_feature_importance(prettified=True))\n",
    "# print(model3.get_feature_importance(prettified=True))\n",
    "# print(model4.get_feature_importance(prettified=True))\n",
    "model1_features = model1.feature_names_\n",
    "model2_features = model2.feature_names_\n",
    "model3_features = model3.feature_names_\n",
    "model4_features = model4.feature_names_\n",
    "# print(model1_features,len(model1_features))\n",
    "print(model2_features,len(model2_features))\n",
    "# print(model3_features,len(model3_features))\n",
    "# print(model4_features,len(model4_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avd', 'datum', 'y'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = test1.prepare_for_model(test.drop('y', axis=1))\n",
    "X1=tp.prepare_for_catboost(X1,model1_features)[0]\n",
    "X2 = test2.prepare_for_model(test.drop('y', axis=1))\n",
    "X2=tp.prepare_for_catboost(X2,model2_features)[0]\n",
    "X3 = test3.prepare_for_model(test.drop('y', axis=1))\n",
    "X3=tp.prepare_for_catboost(X3,model3_features)[0]\n",
    "X4 = test4.prepare_for_model(test.drop('y', axis=1))\n",
    "X4=tp.prepare_for_catboost(X4,model4_features)[0]\n",
    "set(train.columns.to_list()) - set(model1.feature_names_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kolla hur vi kan f√• fram en dataframes med alla m√∂jliga features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>avd</th>\n",
       "      <th>bana</th>\n",
       "      <th>h√§st</th>\n",
       "      <th>kusk</th>\n",
       "      <th>streck</th>\n",
       "      <th>sp√•r</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>√•lder</th>\n",
       "      <th>k√∂n</th>\n",
       "      <th>pris</th>\n",
       "      <th>h1_kusk</th>\n",
       "      <th>h1_bana</th>\n",
       "      <th>h1_sp√•r</th>\n",
       "      <th>h1_plac</th>\n",
       "      <th>h1_pris</th>\n",
       "      <th>h1_odds</th>\n",
       "      <th>h1_kmtid</th>\n",
       "      <th>h2_kusk</th>\n",
       "      <th>h2_bana</th>\n",
       "      <th>h2_sp√•r</th>\n",
       "      <th>h2_plac</th>\n",
       "      <th>h2_pris</th>\n",
       "      <th>h2_odds</th>\n",
       "      <th>h2_kmtid</th>\n",
       "      <th>h3_kusk</th>\n",
       "      <th>h3_bana</th>\n",
       "      <th>h3_sp√•r</th>\n",
       "      <th>h3_plac</th>\n",
       "      <th>h3_pris</th>\n",
       "      <th>h3_odds</th>\n",
       "      <th>h3_kmtid</th>\n",
       "      <th>h4_kusk</th>\n",
       "      <th>h4_bana</th>\n",
       "      <th>h4_sp√•r</th>\n",
       "      <th>h4_plac</th>\n",
       "      <th>h4_pris</th>\n",
       "      <th>h4_odds</th>\n",
       "      <th>h4_kmtid</th>\n",
       "      <th>h5_kusk</th>\n",
       "      <th>h5_bana</th>\n",
       "      <th>h5_sp√•r</th>\n",
       "      <th>h5_plac</th>\n",
       "      <th>h5_pris</th>\n",
       "      <th>h5_odds</th>\n",
       "      <th>h5_kmtid</th>\n",
       "      <th>h1_dist</th>\n",
       "      <th>h2_dist</th>\n",
       "      <th>h3_dist</th>\n",
       "      <th>h4_dist</th>\n",
       "      <th>h5_dist</th>\n",
       "      <th>h1_auto</th>\n",
       "      <th>h2_auto</th>\n",
       "      <th>h3_auto</th>\n",
       "      <th>h4_auto</th>\n",
       "      <th>h5_auto</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>rel_kr</th>\n",
       "      <th>streck_avst</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>h1_samma_bana</th>\n",
       "      <th>h2_samma_bana</th>\n",
       "      <th>h3_samma_bana</th>\n",
       "      <th>h1_samma_kusk</th>\n",
       "      <th>h2_samma_kusk</th>\n",
       "      <th>h3_samma_kusk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46420</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>str√§ngn√§s palema</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>v</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>g√§vle</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.91</td>\n",
       "      <td>12.1</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>axevalla</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.05</td>\n",
       "      <td>13.9</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>bergs√•ker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>14.4</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>f√§rjestad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>13.8</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.43</td>\n",
       "      <td>15.3</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>6651.416330</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>569.318327</td>\n",
       "      <td>697.269701</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.108275</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46421</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>destino d.j.</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>axevalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>13.2</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>√∂rebro</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>15.5</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.51</td>\n",
       "      <td>14.3</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>15.3</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>eskilstuna</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.94</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>12260.731443</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.183798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46422</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>nat king cole</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>g√§vle</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.95</td>\n",
       "      <td>12.8</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.58</td>\n",
       "      <td>14.9</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15.0</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.63</td>\n",
       "      <td>15.8</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>halmstad</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54.60</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>1895.375557</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.081840</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            datum  avd      bana              h√§st             kusk  streck  sp√•r    dist  lopp_dist  start  √•lder k√∂n      pris          h1_kusk   h1_bana  h1_sp√•r  h1_plac  h1_pris  h1_odds  h1_kmtid          h2_kusk   h2_bana  h2_sp√•r  h2_plac  h2_pris  \\\n",
       "46420  2022-08-17  1.0  solvalla  str√§ngn√§s palema      hans crebas    14.0   1.0  1640.0     1640.0      1      3   v  300000.0      hans crebas     g√§vle      1.0      5.0    100.0    22.91      12.1      hans crebas  axevalla      1.0      2.0    100.0   \n",
       "46421  2022-08-17  1.0  solvalla      destino d.j.     jorma kontio    63.0   3.0  1640.0     1640.0      1      3   h  300000.0     jorma kontio  axevalla      2.0      1.0    100.0     2.30      13.2     jorma kontio    √∂rebro      2.0      1.0    125.0   \n",
       "46422  2022-08-17  1.0  solvalla     nat king cole  adrian kolgjini     2.0   4.0  1640.0     1640.0      1      3   h  300000.0  adrian kolgjini     g√§vle      3.0      6.0    100.0    40.95      12.8  adrian kolgjini  solvalla      3.0      4.0     60.0   \n",
       "\n",
       "       h2_odds  h2_kmtid          h3_kusk    h3_bana  h3_sp√•r  h3_plac  h3_pris  h3_odds  h3_kmtid          h4_kusk    h4_bana  h4_sp√•r  h4_plac  h4_pris  h4_odds  h4_kmtid          h5_kusk     h5_bana  h5_sp√•r  h5_plac  h5_pris  h5_odds  h5_kmtid  h1_dist  \\\n",
       "46420    38.05      13.9      hans crebas  bergs√•ker      1.0      6.0    100.0     4.08      14.4      hans crebas  f√§rjestad      1.0      6.0     40.0     3.51      13.8      hans crebas    solvalla      1.0      6.0     60.0    10.43      15.3   1640.0   \n",
       "46421     2.77      15.5     jorma kontio   solvalla      2.0      1.0     60.0     6.51      14.3     jorma kontio   solvalla      2.0      1.0     60.0    10.30      15.3     jorma kontio  eskilstuna      2.0      1.0     40.0     2.94      16.8   2140.0   \n",
       "46422     3.58      14.9  adrian kolgjini   solvalla      3.0      1.0     60.0     3.47      15.0  adrian kolgjini   solvalla      3.0      1.0     40.0    20.63      15.8  adrian kolgjini    halmstad      3.0      1.0     40.0    54.60      23.4   1640.0   \n",
       "\n",
       "       h2_dist  h3_dist  h4_dist  h5_dist  h1_auto  h2_auto  h3_auto  h4_auto  h5_auto       h1_perf       h2_perf      h3_perf      h4_perf      h5_perf  senast  delta1  delta2  delta3  delta4    rel_kr  streck_avst  rel_rank  h1_samma_bana  h2_samma_bana  \\\n",
       "46420   2140.0   2140.0   2140.0   2140.0        1        1        1        1        0   1484.131591   6651.416330   900.171313   569.318327   697.269701    13.0    11.0    38.0    17.0    12.0  0.108275         49.0  0.285714          False          False   \n",
       "46421   2609.0   2140.0   2140.0   2140.0        1        1        1        0        0  10966.331584  12260.731443  8494.483919  8494.483919  6935.717077    24.0    10.0    27.0    30.0     8.0  0.183798          0.0  0.142857          False          False   \n",
       "46422   2140.0   2140.0   2140.0   2140.0        1        1        0        0        0    900.171313   1895.375557  8494.483919  6935.717077  6935.717077    13.0    22.0    21.0     9.0    21.0  0.081840         61.0  0.857143          False           True   \n",
       "\n",
       "       h3_samma_bana  h1_samma_kusk  h2_samma_kusk  h3_samma_kusk  \n",
       "46420          False           True           True           True  \n",
       "46421           True           True           True           True  \n",
       "46422           True           True           True           True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>avd</th>\n",
       "      <th>bana</th>\n",
       "      <th>h√§st</th>\n",
       "      <th>kusk</th>\n",
       "      <th>streck</th>\n",
       "      <th>sp√•r</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>√•lder</th>\n",
       "      <th>k√∂n</th>\n",
       "      <th>pris</th>\n",
       "      <th>h1_kusk</th>\n",
       "      <th>h1_bana</th>\n",
       "      <th>h1_sp√•r</th>\n",
       "      <th>h1_plac</th>\n",
       "      <th>h1_pris</th>\n",
       "      <th>h1_odds</th>\n",
       "      <th>h1_kmtid</th>\n",
       "      <th>h2_kusk</th>\n",
       "      <th>h2_bana</th>\n",
       "      <th>h2_sp√•r</th>\n",
       "      <th>h2_plac</th>\n",
       "      <th>h2_pris</th>\n",
       "      <th>h2_odds</th>\n",
       "      <th>h2_kmtid</th>\n",
       "      <th>h3_kusk</th>\n",
       "      <th>h3_bana</th>\n",
       "      <th>h3_sp√•r</th>\n",
       "      <th>h3_plac</th>\n",
       "      <th>h3_pris</th>\n",
       "      <th>h3_odds</th>\n",
       "      <th>h3_kmtid</th>\n",
       "      <th>h4_kusk</th>\n",
       "      <th>h4_bana</th>\n",
       "      <th>h4_sp√•r</th>\n",
       "      <th>h4_plac</th>\n",
       "      <th>h4_pris</th>\n",
       "      <th>h4_odds</th>\n",
       "      <th>h4_kmtid</th>\n",
       "      <th>h5_kusk</th>\n",
       "      <th>h5_bana</th>\n",
       "      <th>h5_sp√•r</th>\n",
       "      <th>h5_plac</th>\n",
       "      <th>h5_pris</th>\n",
       "      <th>h5_odds</th>\n",
       "      <th>h5_kmtid</th>\n",
       "      <th>h1_dist</th>\n",
       "      <th>h2_dist</th>\n",
       "      <th>h3_dist</th>\n",
       "      <th>h4_dist</th>\n",
       "      <th>h5_dist</th>\n",
       "      <th>h1_auto</th>\n",
       "      <th>h2_auto</th>\n",
       "      <th>h3_auto</th>\n",
       "      <th>h4_auto</th>\n",
       "      <th>h5_auto</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>ant_per_lopp</th>\n",
       "      <th>diff1</th>\n",
       "      <th>diff2</th>\n",
       "      <th>diff3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46420</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>str√§ngn√§s palema</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>v</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>g√§vle</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.91</td>\n",
       "      <td>12.1</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>axevalla</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.05</td>\n",
       "      <td>13.9</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>bergs√•ker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>14.4</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>f√§rjestad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>13.8</td>\n",
       "      <td>hans crebas</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.43</td>\n",
       "      <td>15.3</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>6651.416330</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>569.318327</td>\n",
       "      <td>697.269701</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46421</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>destino d.j.</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>axevalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>13.2</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>√∂rebro</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>15.5</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.51</td>\n",
       "      <td>14.3</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>15.3</td>\n",
       "      <td>jorma kontio</td>\n",
       "      <td>eskilstuna</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.94</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>12260.731443</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46422</th>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>nat king cole</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>g√§vle</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.95</td>\n",
       "      <td>12.8</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.58</td>\n",
       "      <td>14.9</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15.0</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>solvalla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.63</td>\n",
       "      <td>15.8</td>\n",
       "      <td>adrian kolgjini</td>\n",
       "      <td>halmstad</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54.60</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>1895.375557</td>\n",
       "      <td>8494.483919</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7</td>\n",
       "      <td>61.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            datum  avd      bana              h√§st             kusk  streck  sp√•r    dist  lopp_dist  start  √•lder k√∂n      pris          h1_kusk   h1_bana  h1_sp√•r  h1_plac  h1_pris  h1_odds  h1_kmtid          h2_kusk   h2_bana  h2_sp√•r  h2_plac  h2_pris  \\\n",
       "46420  2022-08-17  1.0  solvalla  str√§ngn√§s palema      hans crebas    14.0   1.0  1640.0     1640.0      1      3   v  300000.0      hans crebas     g√§vle      1.0      5.0    100.0    22.91      12.1      hans crebas  axevalla      1.0      2.0    100.0   \n",
       "46421  2022-08-17  1.0  solvalla      destino d.j.     jorma kontio    63.0   3.0  1640.0     1640.0      1      3   h  300000.0     jorma kontio  axevalla      2.0      1.0    100.0     2.30      13.2     jorma kontio    √∂rebro      2.0      1.0    125.0   \n",
       "46422  2022-08-17  1.0  solvalla     nat king cole  adrian kolgjini     2.0   4.0  1640.0     1640.0      1      3   h  300000.0  adrian kolgjini     g√§vle      3.0      6.0    100.0    40.95      12.8  adrian kolgjini  solvalla      3.0      4.0     60.0   \n",
       "\n",
       "       h2_odds  h2_kmtid          h3_kusk    h3_bana  h3_sp√•r  h3_plac  h3_pris  h3_odds  h3_kmtid          h4_kusk    h4_bana  h4_sp√•r  h4_plac  h4_pris  h4_odds  h4_kmtid          h5_kusk     h5_bana  h5_sp√•r  h5_plac  h5_pris  h5_odds  h5_kmtid  h1_dist  \\\n",
       "46420    38.05      13.9      hans crebas  bergs√•ker      1.0      6.0    100.0     4.08      14.4      hans crebas  f√§rjestad      1.0      6.0     40.0     3.51      13.8      hans crebas    solvalla      1.0      6.0     60.0    10.43      15.3   1640.0   \n",
       "46421     2.77      15.5     jorma kontio   solvalla      2.0      1.0     60.0     6.51      14.3     jorma kontio   solvalla      2.0      1.0     60.0    10.30      15.3     jorma kontio  eskilstuna      2.0      1.0     40.0     2.94      16.8   2140.0   \n",
       "46422     3.58      14.9  adrian kolgjini   solvalla      3.0      1.0     60.0     3.47      15.0  adrian kolgjini   solvalla      3.0      1.0     40.0    20.63      15.8  adrian kolgjini    halmstad      3.0      1.0     40.0    54.60      23.4   1640.0   \n",
       "\n",
       "       h2_dist  h3_dist  h4_dist  h5_dist  h1_auto  h2_auto  h3_auto  h4_auto  h5_auto       h1_perf       h2_perf      h3_perf      h4_perf      h5_perf  senast  delta1  delta2  delta3  delta4  ant_per_lopp  diff1  diff2  diff3  \n",
       "46420   2140.0   2140.0   2140.0   2140.0        1        1        1        1        0   1484.131591   6651.416330   900.171313   569.318327   697.269701    13.0    11.0    38.0    17.0    12.0             7   49.0    0.0   -5.0  \n",
       "46421   2609.0   2140.0   2140.0   2140.0        1        1        1        0        0  10966.331584  12260.731443  8494.483919  8494.483919  6935.717077    24.0    10.0    27.0    30.0     8.0             7    0.0  -49.0  -54.0  \n",
       "46422   2140.0   2140.0   2140.0   2140.0        1        1        0        0        0    900.171313   1895.375557  8494.483919  6935.717077  6935.717077    13.0    22.0    21.0     9.0    21.0             7   61.0   12.0    7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X1.head(3))\n",
    "display(X4.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46224, 78), (726, 77), (726, 77), (726, 69), (726, 72))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# differrence between dicts\n",
    "def diff(li1, li2):\n",
    "    return (list(set(li1) - set(li2))+list(set(li2) - set(li1)))\n",
    "\n",
    "# make dict from X1\n",
    "X1_dict = X1.to_dict('list')\n",
    "X4_dict = X4.to_dict('list')\n",
    "X1_dict\n",
    "test_dict = test.to_dict('list')\n",
    "diff(X1_dict,X4_dict)\n",
    "train.shape, X1.shape,X2.shape,X3.shape,X4.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
