{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "### Kopia av 2_üè´_Learn.py\n",
    "### I ett f√∂rsta steg inf√∂r travdata\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\')\n",
    "\n",
    "import typ_copy as tp\n",
    "import travdata as td\n",
    "import V75_scraping as vs\n",
    "import concurrent.futures\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "# import streamlit as st\n",
    "from logging import PlaceHolder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, mean_absolute_error\n",
    "# import streamlit as st\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 260)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "\n",
    "\n",
    "# sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel\\\\modeller\\\\')\n",
    "\n",
    "\n",
    "pref =  '../'\n",
    "\n",
    "############## streamlit grejer #############################################\n",
    "# st.set_page_config(page_title=\"V75 Learning\", page_icon=\"üè´\")\n",
    "# st.markdown(\"# üè´ V75 Learning\")\n",
    "print('st.sidebar.header(\"üè´ V75 Learning\")')\n",
    "#############################################################################\n",
    "\n",
    "#%%\n",
    "# -------------- skapa test-modeller\n",
    "    #               name,   #h√§st     proba,    kelly,  #motst,  motst_diff, #fav, only_cl, streck, test,  pref\n",
    "test1 = tp.model('test1',  True,    True,     False,       0,   False,      0,   False,    True,  True, pref=pref)\n",
    "test2 = tp.model('test2',  True,    True,     False,       0,   False,      0,   False,    False, True, pref=pref)\n",
    "test3 = tp.model('test3',  True,    True,     False,       0,   False,      0,   False,    False, True, pref=pref)\n",
    "test4 = tp.model('test4',  True,    True,     False,       0,   False,      0,   False,    True,  False, pref=pref)\n",
    "\n",
    "modeller = [test1, test2, test3, test4]\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "#%%\n",
    "################################################\n",
    "#              Web scraping                    #\n",
    "################################################\n",
    "\n",
    "def v75_scraping():\n",
    "    df = vs.v75_scraping(history=True, resultat=True, headless=True)\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        df[f] = df[f].str.lower()\n",
    "    return df\n",
    "\n",
    "def remove_features(df_, remove_mer=[]):\n",
    "    df = df_.copy()\n",
    "    df.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "            'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "    \n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "###############################################\n",
    "#              LEARNING                       #\n",
    "###############################################\n",
    "\n",
    "def f√∂rbered_old(df, meta_fraction=None):\n",
    "    # F√∂ljande datum saknar avd==5 och kan inte anv√§ndas\n",
    "    saknas = ['2015-08-15', '2016-08-13', '2017-08-12']\n",
    "    df = df[~df.datum.isin(saknas)]\n",
    "    X = df.copy()\n",
    "    X.drop('plac', axis=1, inplace=True)\n",
    "\n",
    "    # l√§s in FEATURES.txt\n",
    "    with open(pref+'FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        features = f.read().splitlines()\n",
    "\n",
    "    X = X[features]\n",
    "\n",
    "    assert len(features) == len(\n",
    "        X.columns), f'features {len(features)} and X.columns {len(X.columns)} are not the same length'\n",
    "    assert set(features) == set(\n",
    "        X.columns), f'features {set(features)} and X.columns {set(X.columns)} are not the same'\n",
    "\n",
    "    y = (df.plac == 1)*1   # plac 1 eller 0\n",
    "\n",
    "    for f in ['h√§st', 'bana', 'kusk', 'h1_kusk', 'h2_kusk', 'h3_kusk', 'h4_kusk', 'h5_kusk', 'h1_bana', 'h2_bana', 'h3_bana', 'h4_bana', 'h5_bana']:\n",
    "        X[f] = X[f].str.lower()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    if meta_fraction == 0:\n",
    "        # no meta data\n",
    "        return X, y, None, None\n",
    "\n",
    "    # use a fraction for meta data\n",
    "    meta_antal = int(len(X.datum.unique())*meta_fraction)\n",
    "    meta_datum = X.datum.unique()[-meta_antal:]\n",
    "\n",
    "    X_val = X.loc[X.datum.isin(meta_datum)]\n",
    "    y_val = y[X_val.index]\n",
    "    X = X.loc[~X.datum.isin(meta_datum)]\n",
    "    y = y.loc[X.index]\n",
    "    return X, y, X_val, y_val\n",
    "\n",
    "\n",
    "def concat_data_old(df_all, df_ny, save=True):\n",
    "    df_ny = df_ny[df_all.columns]\n",
    "    df_all = pd.concat([df_all, df_ny])\n",
    "    # remove duplicates\n",
    "    all_shape = df_all.shape\n",
    "\n",
    "    df_all = df_all.drop_duplicates(subset=['datum', 'avd', 'h√§st'])\n",
    "    assert df_all.shape[0] + \\\n",
    "        90 > all_shape[0], f'{df_all.shape[0]+90} should be more than {all_shape[0]}'\n",
    "    assert df_all.shape[1] == all_shape[1], f'{df_all.shape[1]} should be {all_shape[1]}'\n",
    "    if save == True:\n",
    "        df_all.to_csv(pref+'all_data.csv', index=False)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nytt ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skapa_stack_data(model, name, X_meta, stack_data):\n",
    "    \"\"\"Skapa stack_data\"\"\"\n",
    "    assert 'y' in stack_data.columns, 'y is missing in stack_data'\n",
    "    this_proba = model.predict(X_meta)\n",
    "    # print(f'X_meta.shape = {X_meta.shape} this_proba.shape={this_proba.shape}')\n",
    "\n",
    "    # Bygg up meta-kolumnerna (proba) f√∂r denns modell\n",
    "    nr = name[3:]\n",
    "    stack_data['proba'+nr] = this_proba\n",
    "    return stack_data\n",
    "\n",
    "\n",
    "def prepare_stack_data(stack_data_):\n",
    "    # \"\"\"Hantera missing values, NaN, etc f√∂r meta-modellerna\"\"\"\n",
    "\n",
    "    assert 'y' in stack_data_.columns, 'y is missing in stack_data'\n",
    "    stack_data = stack_data_.copy()\n",
    "    stack_data.y = stack_data.y.astype(int)\n",
    "\n",
    "    #\"\"\" rensa bort features som inte ska anv√§ndas \"\"\"\n",
    "    # stack_data.drop(['startnr', 'vodds', 'podds', 'bins', 'h1_dat',\n",
    "    #             'h2_dat', 'h3_dat', 'h4_dat', 'h5_dat'], axis=1, inplace=True)\n",
    "\n",
    "    #\"\"\" Fyll i saknade numeriska v√§rden med 0 \"\"\"\n",
    "    numericals = stack_data.drop('y', axis=1).select_dtypes(\n",
    "        exclude=['object']).columns\n",
    "    stack_data[numericals] = stack_data[numericals].fillna(0)\n",
    "\n",
    "    #\"\"\" Fyll i saknade kategoriska v√§rden med 'missing' \"\"\"\n",
    "    categoricals = stack_data.select_dtypes(include=['object']).columns\n",
    "    stack_data[categoricals] = stack_data[categoricals].fillna('missing')\n",
    "\n",
    "    # \"\"\" Hantera high cardinality \"\"\"\n",
    "    # cardinality_list=['h√§st','kusk','h1_kusk','h2_kusk','h3_kusk','h4_kusk','h5_kusk']\n",
    "\n",
    "    #\"\"\" Target encoding\"\"\"\n",
    "    target_encode_list = ['bana', 'h√§st', 'kusk', 'k√∂n', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana',\n",
    "                          'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
    "\n",
    "    y = stack_data['y']\n",
    "    enc = TargetEncoder(cols=target_encode_list,\n",
    "                        min_samples_leaf=20, smoothing=10).fit(stack_data, y)\n",
    "    stack_data = enc.transform(stack_data)\n",
    "\n",
    "    return stack_data, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries learning for validation\n",
      "learn models and meta models on first 75.0 % of the data\n",
      "st.info(f'Train: {X.datum.iloc[0]} --{X.datum.iloc[-1]}{validation_text}')\n",
      "shape of X (35203, 78) shape of X_train (5868, 78) shape of X_test (5867, 78)\n",
      "shape of X (35203, 78) shape of X_train (11735, 78) shape of X_test (5867, 78)\n",
      "shape of X (35203, 78) shape of X_train (17602, 78) shape of X_test (5867, 78)\n",
      "shape of X (35203, 78) shape of X_train (23469, 78) shape of X_test (5867, 78)\n",
      "shape of X (35203, 78) shape of X_train (29336, 78) shape of X_test (5867, 78)\n",
      "Learning meta models\n",
      "Learn models on all of Train\n",
      "‚úîÔ∏è TimeSeries learning done\n"
     ]
    }
   ],
   "source": [
    "def learn_modeller(modeller, X_train, y_train, X_meta, y_meta):\n",
    "    ############################################################################################################\n",
    "    #                        H√§r g√∂rs en f√∂rsta learn av modeller och sedan skapas stack_data\n",
    "    #                        - Learn modeller p√• X,y\n",
    "    #                        - Ha en egen skapa_stack_funktion (som ocks√• anv√§nds l√§ngre ner)\n",
    "    #                           - Skapa stack_data med predict X_meta med nya modellerna\n",
    "    #                           - Spara √§ven X_meta, y_meta i stack_data\n",
    "    ############################################################################################################\n",
    "    stack_data = X_meta.copy()\n",
    "    stack_data['y'] = y_meta\n",
    "    assert 'y' in stack_data.columns, '1. y is missing in stack_data'\n",
    "    for model in modeller:\n",
    "        name = model.name\n",
    "        print(f'first Learn {name} {X_train.datum.min()} -{X_train.datum.max()}')\n",
    "\n",
    "        model.learn(X_train, y_train, params=None, save=True)\n",
    "\n",
    "        stack_data = skapa_stack_data(model, name, X_meta, stack_data)\n",
    "        assert 'y' in stack_data.columns, '2. y is missing in stack_data'\n",
    "\n",
    "    assert 'y' in stack_data.columns, '3. y is missing in stack_data'\n",
    "    stack_data, enc = prepare_stack_data(stack_data)\n",
    "\n",
    "    return stack_data\n",
    "\n",
    "def normal_learning(modeller, meta_modeller, X_train, y_train, X_meta, y_meta):\n",
    "    stack_data = learn_modeller(modeller, X_train, y_train, X_meta, y_meta)\n",
    "    assert 'y' in stack_data.columns, 'y is missing in stack_data'\n",
    "    stack_data.to_csv('first_stack_data.csv', index=False)\n",
    "\n",
    "    # \"\"\" Learn meta_modeller p√• stack_data \"\"\"\n",
    "    meta_features = stack_data.drop(['datum', 'avd', 'y']).columns\n",
    "    meta_modeller = learn_meta_models(meta_modeller, stack_data, meta_features)\n",
    "    \n",
    "    return stacked_data[meta_features + ['y']]\n",
    "    \n",
    "# TimeSeriesSplit learning models\n",
    "def TimeSeries_learning(df_ny_, modeller, n_splits=5, meta_fraction=None, save=True, learn_models=True):\n",
    "    \"\"\"\n",
    "    Skapar en stack med {1 - meta_fraction} av X fr√•n alla modeller. Anv√§nds som input till meta_model.\n",
    "        - learn_models=True betyder att vi b√•de g√∂r en learning och skapar en stack\n",
    "        - learn_models=False betyder att vi bara skapar en stack och d√• har param save ingen funktion\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(pref+'all_data.csv')\n",
    "\n",
    "    if df_ny_ is not None:  # Har vi en ny omg√•ng?\n",
    "        df_ny = df_ny_.copy()\n",
    "        df_all = concat_data(df_all.copy(), df_ny, save=True)\n",
    "\n",
    "    X, y, X_val, _ = f√∂rbered(df_all, meta_fraction=meta_fraction)\n",
    "\n",
    "    validation_text = \"\"\n",
    "\n",
    "    if X_val is not None:\n",
    "        validation_text = f', Validation: _{X_val.datum.iloc[0]} - -{X_val.datum.iloc[-1]}'\n",
    "\n",
    "    print(\"st.info(f'Train: {X.datum.iloc[0]} --{X.datum.iloc[-1]}{validation_text}')\")\n",
    "\n",
    "    ts = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    stacked_data = pd.DataFrame()\n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 1: Learn the models on ts.split X_train and predict on X_test  #\n",
    "    ###############################################################################\n",
    "    # st.write('Skapar stacked_data till meta')\n",
    "    # my_bar = st.progress(0)\n",
    "\n",
    "    step = 1/(n_splits*len(modeller))-0.0000001\n",
    "    steps = 0.0\n",
    "\n",
    "    for enum, (train_index, test_index) in enumerate(ts.split(X, y)):\n",
    "        print('shape of X', X.shape, 'shape of X_train',\n",
    "              X.iloc[train_index].shape, 'shape of X_test', X.iloc[test_index].shape)\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['y'] = y_test\n",
    "        for model in modeller:\n",
    "            steps += step\n",
    "            # progress bar continues to complete from 0 to 100\n",
    "            \n",
    "            # my_bar.progress(steps)\n",
    "\n",
    "            if learn_models:\n",
    "        \n",
    "                with open(pref+'optimera/params_'+model.name+'.json', 'r') as f:\n",
    "                    params = json.load(f)\n",
    "                    params = params['params']\n",
    "                # learn p√• X_train-delen\n",
    "                \n",
    "                cbc = model.learn(X_train, y_train, X_test,\n",
    "                                y_test, params=params, save=save)\n",
    "\n",
    "            # predict the new fitted model on X_test-delen\n",
    "            nr = model.name[3:]\n",
    "            this_proba = model.predict(X_test)\n",
    "\n",
    "            # Bygg up meta-kolumnen proba f√∂r denns modell\n",
    "            temp_df['proba'+nr] = this_proba\n",
    "\n",
    "        if stacked_data.columns == []:\n",
    "            stacked_data = temp_df.copy()\n",
    "        else:        \n",
    "            stacked_data = pd.concat([stacked_data, temp_df], ignore_index=True)\n",
    "        stacked_data.y = stacked_data.y.astype(int)\n",
    "\n",
    "    stacked_data_y = stacked_data.pop('y')\n",
    "    meta_features = stacked_data.drop(['datum', 'avd'], axis=1).columns\n",
    "\n",
    "    # my_bar.progress(1.0)\n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 2:       Learn the meta models                                 #\n",
    "    ###############################################################################\n",
    "    # st.write('Learning meta models')\n",
    "    print('Learning meta models')\n",
    "    _, _, _, _ = learn_meta_models(stacked_data[meta_features], stacked_data_y)\n",
    "\n",
    "    ###############################################################################\n",
    "    #         Step 3: learn models on all of X - what iteration to use?           #\n",
    "    ###############################################################################\n",
    "    # st.write('Learn models on all of Train')\n",
    "    print('Learn models on all of Train')\n",
    "    \n",
    "    # my_bar2 = st.progress(0)\n",
    "    ant_meta_models = 4\n",
    "    step = 1/(ant_meta_models) - 0.0000001\n",
    "    steps = 0.0\n",
    "    # my_bar2.progress(steps)\n",
    "\n",
    "    for model in modeller:\n",
    "        steps += step\n",
    "        # my_bar2.progress(steps)\n",
    "        if learn_models:\n",
    "            with open(pref+'optimera/params_'+model.name+'.json', 'r') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "            params = params['params']\n",
    "            cbc = model.learn(X, y, None, None, iterations=500,\n",
    "                            params=params, save=save)\n",
    "\n",
    "    # my_bar2.progress(1.0)\n",
    "    # st.empty()\n",
    "    \n",
    "    return stacked_data[meta_features + ['y']]\n",
    "\n",
    "\n",
    "def skapa_stack_learning(X_, y,meta_features):\n",
    "    # F√∂r validate\n",
    "    X = X_.copy()\n",
    "\n",
    "    stacked_data = pd.DataFrame(columns=meta_features)\n",
    "    for model in modeller:\n",
    "        nr = model.name[3:]\n",
    "        stacked_data['proba'+nr] = model.predict(X)\n",
    "\n",
    "    assert list(\n",
    "        stacked_data.columns) == meta_features, f'columns in stacked_data is wrong {list(stacked_data.columns)}'\n",
    "    assert len(stacked_data) == len(y), f'stacked_data {len(stacked_data)} and y {len(y)} should have same length'\n",
    "    return stacked_data[meta_features], y   # enbart stack-info\n",
    "\n",
    "##### RidgeClassifier (meta model) #####\n",
    "def learn_meta_ridge_model(X, y, save=True):\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "    with open(pref+'optimera/params_ridge.json', 'r') as f:\n",
    "        params = json.load(f)['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    ridge_model = RidgeClassifier(**params, random_state=2022)\n",
    "\n",
    "    ridge_model.fit(X, y)\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_ridge_model', 'wb') as f:\n",
    "            pickle.dump(ridge_model, f)\n",
    "\n",
    "    return ridge_model\n",
    "\n",
    "##### RandomForestClassifier (meta model) #####\n",
    "\n",
    "\n",
    "def learn_meta_rf_model(X, y, save=True):\n",
    "\n",
    "    with open(pref+'optimera/params_rf.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    rf_model = RandomForestClassifier(**params, n_jobs=6, random_state=2022)\n",
    "    rf_model.fit(X, y)\n",
    "\n",
    "    ######################### for testing ###############################\n",
    "    rf_train = X.copy(deep=True)\n",
    "    rf_train['y'] = y\n",
    "    rf_train.to_csv(pref+'rf_train.csv', index=False)\n",
    "    #########################              ###############################\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_rf_model', 'wb') as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "\n",
    "##### KNeighborsClassifier (meta model) #####\n",
    "def learn_meta_knn_model(X, y, save=True):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    with open(pref+'optimera/params_knn_meta.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "        # st.write(params)\n",
    "\n",
    "    knn_model = KNeighborsClassifier(**params, n_jobs=6)\n",
    "    knn_model.fit(X, y)\n",
    "\n",
    "    if save:\n",
    "        with open(pref+'modeller/meta_knn.model', 'wb') as f:\n",
    "            pickle.dump(knn_model, f)\n",
    "\n",
    "    return knn_model\n",
    "\n",
    "\n",
    "def learn_meta_models(X, y, meta_features, save=True):\n",
    "    \"\"\" all meta models will be fitted on X and y \"\"\"\n",
    "    Ridge_Classifier = learn_meta_ridge_model(X[meta_features], y, save=save)\n",
    "    RandomForest_Classifier = learn_meta_rf_model(X[meta_features], y, save=save)\n",
    "    Knn_model = learn_meta_knn_model(X[meta_features], y, save=save)\n",
    "\n",
    "    return Ridge_Classifier, RandomForest_Classifier, Knn_model\n",
    "\n",
    "#%%\n",
    "##############################################################\n",
    "#                     VALIDATE                               #\n",
    "##############################################################\n",
    "\n",
    "def predict_meta_ridge_model(X, ridge_model=None):\n",
    "    if ridge_model is None:\n",
    "        with open(pref+'modeller/meta_ridge.model', 'rb') as f:\n",
    "            ridge_model = pickle.load(f)\n",
    "\n",
    "    return ridge_model._predict_proba_lr(X)\n",
    "\n",
    "\n",
    "def predict_meta_rf_model(X, rf_model=None):\n",
    "    if rf_model is None:\n",
    "        with open(pref+'modeller/meta_rf.model', 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "\n",
    "    return rf_model.predict_proba(X)\n",
    "\n",
    "\n",
    "def predict_meta_knn_model(X, knn_model=None):\n",
    "    if knn_model is None:\n",
    "        with open(pref+'modeller/meta_knn.model', 'rb') as f:\n",
    "            knn_model = pickle.load(f)\n",
    "\n",
    "    return knn_model.predict_proba(X)\n",
    "\n",
    "\n",
    "def predict_meta_model(X, meta_model=None):\n",
    "    if meta_model == 'ridge':\n",
    "        return predict_meta_ridge_model(X)[:, 1]\n",
    "    elif meta_model == 'rf':\n",
    "        X.to_csv(pref+'rf_validate.csv', index=False)\n",
    "        y_pred = predict_meta_rf_model(X)\n",
    "        # write y_pred to file for testing\n",
    "        # first make y_pred a dataframe\n",
    "        rf_y_pred = pd.DataFrame(y_pred, columns=['0', '1'])\n",
    "        rf_y_pred.to_csv(pref+'rf_y_pred.csv', index=False)\n",
    "        return y_pred[:, 1]\n",
    "    elif meta_model == 'knn':\n",
    "        y_pred = predict_meta_knn_model(X)\n",
    "        return y_pred[:, 1]\n",
    "    elif meta_model == None:\n",
    "        assert False, 'ingen meta_model angiven'\n",
    "    else:\n",
    "        return meta_model.predict(X)\n",
    "\n",
    "# write the scores\n",
    "\n",
    "\n",
    "def display_scores(y_true, y_pred, spelade):\n",
    "    # st.write('AUC', round(roc_auc_score(y_true, y_pred), 5), 'F1', round(f1_score(y_true, y_pred), 5), 'Acc', round(\n",
    "    #     accuracy_score(y_true, y_pred), 5), 'MAE', round(mean_absolute_error(y_true, y_pred), 5), '\\n', spelade)\n",
    "    print('AUC', round(roc_auc_score(y_true, y_pred), 5), 'F1', round(f1_score(y_true, y_pred), 5), 'Acc', round(\n",
    "        accuracy_score(y_true, y_pred), 5), 'MAE', round(mean_absolute_error(y_true, y_pred), 5), '\\n', spelade)\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def find_threshold(y_pred, fr, to, margin):\n",
    "    \"\"\" hitta threshold som ger 2.5 spelade per avdelning \"\"\"\n",
    "    thresh = 0\n",
    "    cnt = 0\n",
    "    # make a binary search\n",
    "    while cnt < 1000:\n",
    "        thresh = (fr + to) / 2\n",
    "        antal_spelade_per_avd = 12 * sum(y_pred > thresh)/len(y_pred)\n",
    "        if (antal_spelade_per_avd > (2.5 - margin)) and (antal_spelade_per_avd < (2.5 + margin)):\n",
    "            break\n",
    "\n",
    "        if antal_spelade_per_avd > 2.5:\n",
    "            fr = thresh-0.00001\n",
    "        else:\n",
    "            to = thresh+0.00001\n",
    "        cnt += 1\n",
    "\n",
    "    print('ant', cnt, 'thresh', round(thresh, 4))\n",
    "    if cnt >= 1000:\n",
    "        print('threshold not found', 'fr', round(fr, 6), 'to', round(to, 6))\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model, fr=0.0, to=0.9, margin=0.001):\n",
    "\n",
    "    #### F√∂rst:  hitta ett threshold som tippar ca 2.5 h√§star per avd ####\n",
    "    # thresh = 0\n",
    "    # for thresh in np.arange(fr, to, step):\n",
    "    #     cost = 12*sum(y_pred > thresh)/len(y_pred)\n",
    "    #     if cost < 2.5:\n",
    "    #         break\n",
    "    thresh = round(find_threshold(y_pred, fr, to, margin), 4)\n",
    "    print(f'Threshold: {thresh}\\n')\n",
    "    y_pred = (y_pred > thresh).astype(int)\n",
    "    # confusion_matrix_graph(y_true, y_pred, f'{model} threshold={thresh}')\n",
    "\n",
    "    #### Sedan: confusion matrix graph ####\n",
    "    title = f'{model} threshold={thresh}'\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred,)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(font_scale=2.0)\n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", linewidths=.5,\n",
    "                square=True, cmap='Blues_r')\n",
    "\n",
    "    # increase font size\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    # st.write(fig)\n",
    "\n",
    "    # read dict from disk\n",
    "    try:\n",
    "        with open(pref+'modeller/meta_scores.pkl', 'rb') as f:\n",
    "            meta_scores = pickle.load(f)\n",
    "    except:\n",
    "        # st.write('No meta_scores.pkl found')\n",
    "        print('No meta_scores.pkl found')\n",
    "        meta_scores = {'knn': 0, 'rf': 0, 'ridge': 0}\n",
    "\n",
    "    #### print scores ####\n",
    "    typ_AUC = display_scores(\n",
    "        y_true, y_pred, f'spelade per lopp: {round(12 * sum(y_pred)/len(y_pred),4)}')\n",
    "    meta_scores[model] = float(typ_AUC)\n",
    "    #### save dict to disk ####\n",
    "    with open(pref+'modeller/meta_scores.pkl', 'wb') as f:\n",
    "        pickle.dump(meta_scores, f)\n",
    "\n",
    "\n",
    "def validate(drop=[], fraction=None):\n",
    "    pass\n",
    "    # st.info('skall endast  k√∂ras efter \"Learn for Validation\"')\n",
    "    # df_all = pd.read_csv(pref+'all_data.csv')\n",
    "\n",
    "    #_, _, X_val, y_val = f√∂rbered(df_all, meta_fraction=fraction)\n",
    "    # st.info(f'Validerar p√•:  {X_val.datum.iloc[0]} - -{X_val.datum.iloc[-1]}')\n",
    "\n",
    "    # # create the stack from validation data\n",
    "    # stacked_val, y_val = skapa_stack_learning(X_val, y_val)\n",
    "    # stacked_val = stacked_val.drop(drop, axis=1)\n",
    "\n",
    "    # ##############################################################\n",
    "    # #                          Meta models                       #\n",
    "    # ##############################################################\n",
    "\n",
    "    # y_true = y_val.values\n",
    "    # y_pred_rf = predict_meta_model(stacked_val, meta_model='rf')\n",
    "\n",
    "    # ############## write y_true to file for testing ##############\n",
    "    # # first make y_true a dataframe\n",
    "    # rf_y_true = pd.DataFrame(y_true, columns=['y'])\n",
    "    # rf_y_true.to_csv(pref+'rf_y_true.csv', index=False)\n",
    "    # ##############################################################\n",
    "\n",
    "    # st.info('f√∂rbereder rf plot')\n",
    "    # plot_confusion_matrix(y_true, y_pred_rf,\n",
    "    #                       'rf', fr=0.0, to=1.0, margin=0.01)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    # st.info('f√∂rbereder knn plot')\n",
    "    # plot_confusion_matrix(y_true, predict_meta_model(stacked_val, meta_model='knn'),\n",
    "    #                       'knn', fr=0.0, to=0.9)\n",
    "\n",
    "    # st.write('\\n')\n",
    "    # st.info('f√∂rbereder ridge plot')\n",
    "    # plot_confusion_matrix(y_true, predict_meta_model(stacked_val, meta_model='ridge'),\n",
    "    #                       'ridge', fr=0.0, to=0.9)\n",
    "    # # placeholder.empty()\n",
    "\n",
    "    # st.write('\\n')\n",
    "    # st.info('f√∂rbereder lasso plot')\n",
    "    # plot_confusion_matrix(y_true, predict_meta_model(stacked_val, meta_model='lasso'),\n",
    "    #                       'lasso', fr=0.0, to=0.9)\n",
    "    # # placeholder.empty()\n",
    "\n",
    "    # stacked_val['meta'] = y_pred_rf  # default\n",
    "    # stacked_val['y'] = y_true\n",
    "    # stacked_val['avd'] = X_val.avd.values\n",
    "\n",
    "    # ################################################################\n",
    "    # #                         proba 6, 1, 9, (16)                  #\n",
    "    # ################################################################\n",
    "    # st.write('\\n')\n",
    "    # for model in yper:\n",
    "    #     st.write('\\n')\n",
    "    #     name = 'proba' + model.name[3:]\n",
    "    #     y_pred = stacked_val[name]\n",
    "    #     plot_confusion_matrix(y_true, y_pred, name, fr=0.0, to=0.9)\n",
    "\n",
    "#%%\n",
    "##############################################################\n",
    "#            FINAL LEARNING                                  #\n",
    "##############################################################\n",
    "\n",
    "\n",
    "def final_learning(modeller, n_splits=5):\n",
    "    # st.info('Final learning on all the data')\n",
    "    print('Final learning on all the data')\n",
    "    _ = TimeSeries_learning(\n",
    "        None, modeller, n_splits=n_splits, meta_fraction=0, save=True)\n",
    "\n",
    "    # st.info('Step 2: Final learn meta model')\n",
    "    # _, _, _, _= learn_meta_models(stacked_data.drop(['y'], axis=1), stacked_data['y'])\n",
    "\n",
    "    # st.success('‚úîÔ∏è Final learning done')\n",
    "    print('Final learning done')\n",
    "\n",
    "#%%\n",
    "def scrape(full=True):\n",
    "    # scraping.write('Starta web-scraping f√∂r ny data')\n",
    "    # with st.spinner('Ta det lugnt!'):\n",
    "        # st.image('winning_horse.png')  # ,use_column_width=True)\n",
    "        #####################\n",
    "        # start v75_scraping as a thread\n",
    "        #####################\n",
    "        i = 0.0\n",
    "        # placeholder = st.empty()\n",
    "        seconds = 0\n",
    "        # my_bar = st.progress(i)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(v75_scraping)\n",
    "            while future.running():\n",
    "                time.sleep(1)\n",
    "                seconds += 1\n",
    "                # placeholder.write(f\"‚è≥ {seconds} sekunder\")\n",
    "                i += 1/65\n",
    "                if i < 0.99:\n",
    "                    pass\n",
    "                    # my_bar.progress(i)\n",
    "            # my_bar.progress(1.0)\n",
    "            # scraping.write('‚úîÔ∏è Scraping done, pls wait')\n",
    "            print('‚úîÔ∏è Scraping done, pls wait')\n",
    "            time.sleep(2)\n",
    "            df = future.result()\n",
    "            # scraping.write(f'‚úîÔ∏è {len(df)} rader h√§mtade')\n",
    "            print(f'‚úîÔ∏è {len(df)} rader h√§mtade')\n",
    "            \n",
    "            df.to_csv('sparad_scrape_learn.csv', index=False)\n",
    "\n",
    "        # st.balloons()\n",
    "        # my_bar.empty()\n",
    "        # placeholder.empty()\n",
    "\n",
    "        # st.session_state.df = df\n",
    "\n",
    "\n",
    "# models = [typ6, typ1, typ9]\n",
    "\n",
    "#%%\n",
    "# top = st.container()\n",
    "# buttons = st.container()\n",
    "# scraping = st.container()\n",
    "\n",
    "############################################\n",
    "#   Init session_state                     #\n",
    "############################################\n",
    "# with top:\n",
    "#     if 'fraction' not in st.session_state:\n",
    "#         st.session_state['fraction'] = 0.25\n",
    "fraction = 0.25\n",
    "#     if 'df' not in st.session_state:\n",
    "#         st.session_state['df'] = None\n",
    "omg_df = pd.read_csv(pref+'omg_att_spela_link.csv')\n",
    "urlen = omg_df.Link.values[0]\n",
    "datum = urlen.split('spel/')[1][0:10]\n",
    "#     if 'datum' not in st.session_state:\n",
    "#         omg_df = pd.read_csv('omg_att_spela_link.csv')\n",
    "#         urlen = omg_df.Link.values[0]\n",
    "#         datum = urlen.split('spel/')[1][0:10]\n",
    "#         st.session_state.datum = datum\n",
    "\n",
    "#     if 'datum' in st.session_state:\n",
    "#         datum = st.session_state['datum']\n",
    "#         year = int(datum[:4])\n",
    "#         month = int(datum[5:7])\n",
    "#         day = int(datum[8:])\n",
    "#         datum = st.sidebar.date_input(\n",
    "#             'V√§lj datum', datetime.date(year, month, day))\n",
    "#         datum = datum.strftime('%Y-%m-%d')\n",
    "\n",
    "        # if datum != st.session_state['datum']:\n",
    "        #     st.session_state['datum'] = datum\n",
    "        #     datum = \"https://www.atg.se/spel/\"+datum+\"/V75/\"\n",
    "        #     omg_df = pd.DataFrame([datum], columns=['Link'])\n",
    "        #     omg_df.to_csv('omg_att_spela_link.csv', index=False)\n",
    "\n",
    "    # st.header(f'Omg√•ng:  {st.session_state.datum}')\n",
    "\n",
    "###########################################\n",
    "# control flow with buttons               #\n",
    "###########################################\n",
    "# with buttons:\n",
    "#     if st.sidebar.button('scrape'):\n",
    "#         st.write(f'web scraping {st.session_state.datum}')\n",
    "#         try:\n",
    "#             scrape()\n",
    "#             del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "#         except:\n",
    "#             st.error(\n",
    "#                 \"Fel i web scraping. Kolla att resultat finns f√∂r datum och internet √§r tillg√§ngligt\")\n",
    "\n",
    "#     if st.sidebar.button('reuse scrape'):\n",
    "#         # del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "#         try:\n",
    "#             df = pd.read_csv('sparad_scrape_learn.csv')\n",
    "#             st.session_state.df = df\n",
    "#             if df.datum.iloc[0] != st.session_state.datum:\n",
    "#                 st.error(\n",
    "#                     f'Datum i data = {df.datum.iloc[0]} \\n\\n √§r inte samma som i omg√•ng')\n",
    "#             else:\n",
    "#                 st.success(f'inl√§st data med datum = {df.datum.iloc[0]}')\n",
    "#         except:\n",
    "#             # write error message\n",
    "#             st.error('Ingen data sparad')\n",
    "\n",
    "#     if st.session_state.df is not None:\n",
    "#         if st.sidebar.button('Learn for validation'):\n",
    "#             st.write('TimeSeries learning for validation')\n",
    "#             fraction = st.session_state.fraction\n",
    "#             df = st.session_state.df\n",
    "#             st.write(\n",
    "#                 f'learn models and meta models on first {(1-fraction)*100} % of the data')\n",
    "\n",
    "#             stacked_data = TimeSeries_learning(\n",
    "#                 df, yper, n_splits=5, meta_fraction=fraction, save=True, learn_models=True)\n",
    "#             st.success('‚úîÔ∏è TimeSeries learning done')\n",
    "\n",
    "#         if st.sidebar.button('Validate'):\n",
    "#             validate(fraction=st.session_state.fraction)\n",
    "\n",
    "#         if st.sidebar.button('Final learning'):\n",
    "#             final_learning(yper)\n",
    "\n",
    "#         if st.sidebar.button('Clear'):\n",
    "#             st.empty()\n",
    "\n",
    "button = 'learn'   # 'scrape'  # 'reuse'  # 'learn'  # 'validate'  # 'final'  # 'clear'\n",
    "\n",
    "\n",
    "if button == 'scrape':\n",
    "    print(f'web scraping {datum}')\n",
    "    try:\n",
    "        scrape()\n",
    "        # del st.session_state.datum  # s√§kra att datum √§r samma som i scraping\n",
    "    except:\n",
    "        print(\"Fel i web scraping. Kolla att resultat finns f√∂r datum och internet √§r tillg√§ngligt\")\n",
    "else: # resuse scrape\n",
    "    try:\n",
    "        df = pd.read_csv(pref+'sparad_scrape_learn.csv')\n",
    "        # st.session_state.df = df\n",
    "    except:\n",
    "        # write error message\n",
    "        print('Ingen data sparad')\n",
    "        \n",
    "if button=='learn':\n",
    "    print('TimeSeries learning for validation')\n",
    "    try:\n",
    "        df = pd.read_csv(pref+'sparad_scrape_learn.csv')\n",
    "        loaded=True\n",
    "        # st.session_state.df = df\n",
    "    except:\n",
    "        loaded=False\n",
    "        # write error message\n",
    "        print('Ingen data sparad')\n",
    "    if loaded:\n",
    "        print(f'learn models and meta models on first {(1-fraction)*100} % of the data')\n",
    "\n",
    "        stacked_data = TimeSeries_learning(df, modeller, n_splits=5, meta_fraction=fraction, save=True, learn_models=True)\n",
    "        print('‚úîÔ∏è TimeSeries learning done')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
