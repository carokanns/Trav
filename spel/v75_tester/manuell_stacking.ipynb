{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try mauell stacking for V75\r\n",
    "TimeSeries kan inte användas i sklearn.stackClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from IPython.display import display \r\n",
    "\r\n",
    "from catboost import CatBoostClassifier,Pool, cv, utils \r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, Pipeline\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\r\n",
    "# import sklearn.metrics, sklearn.compose\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    X_train[cat_features]=X_train[cat_features].fillna('missing')\r\n",
    "    if X_test is not None:  ## om X_test är med\r\n",
    "        X_test[cat_features]=X_test[cat_features].fillnal('missing')    ### byt ut None-värden till texten 'None\r\n",
    "\r\n",
    "    return X_train,X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def basic_data(df, NaN=True, frac=0.25):\r\n",
    "    X_train,X_test,y_train,y_test = None,None,None,None\r\n",
    "    dfnew = remove_features(df.copy())\r\n",
    "    dfnew['plac'] = (dfnew.plac==1) * 1\r\n",
    "    cat_features = list(dfnew.loc[:,df.dtypes=='O'].columns)\r\n",
    "    if NaN:\r\n",
    "        dfnew,_ = replace_NaN(dfnew.copy(), cat_features=cat_features)    \r\n",
    "    \r\n",
    "    alla_datum = df.datum.unique()\r\n",
    "    split_dat = alla_datum[int(len(alla_datum)* (1 - 0.25))]     # större än split_dat är test\r\n",
    "\r\n",
    "    X_train = dfnew.loc[dfnew.datum <= split_dat].copy()\r\n",
    "    y_train=X_train.plac\r\n",
    "    X_train.drop('plac',axis=1,inplace=True)\r\n",
    "    \r\n",
    "    X_test = dfnew.loc[dfnew.datum > split_dat].copy()\r\n",
    "    y_test=X_test.plac\r\n",
    "    X_test.drop('plac',axis=1,inplace=True)\r\n",
    "    \r\n",
    "    return X_train,X_test, y_train,y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df skall innehålla datum,avd,vodds\r\n",
    "def proba_order_score(df_, y,proba):\r\n",
    "    kassa=1000\r\n",
    "    df = df_.copy()\r\n",
    "    df['proba'] = proba[:,1]\r\n",
    "    df['f'] = (df.proba*df.vodds - 1) / (df.vodds-1)  # kelly formel\r\n",
    "    df['spela'] = df.f >0\r\n",
    "    df['insats'] = df.spela * df.f * kassa\r\n",
    "\r\n",
    "    df.sort_values(['datum','avd','proba'],ascending=[True,True,False],inplace=True)\r\n",
    "    proba_order=df.groupby(['datum','avd']).proba.cumcount()\r\n",
    "\r\n",
    "    df['prob_order']=proba_order+1\r\n",
    "    df['y'] = y\r\n",
    "    \r\n",
    "    return df, df.loc[df.y==1].prob_order.mean()   # mean prob_order för vinnarhäst"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# Set a smooth mean value to the features in X_train  ##\r\n",
    "def calc_smooth_mean(X, y, by, m=100, tot_mean=None):\r\n",
    "    Xcopy = X.copy()\r\n",
    "    Xcopy['plac'] = y\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = Xcopy.groupby(by)['plac'].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "    \r\n",
    "    return smooth.to_dict()    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MyOwnSearchCV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def load_data(frac=0.25):\r\n",
    "    dforg = pd.read_csv('..\\\\all_data.csv')  \r\n",
    "    alla_datum = list(dforg.datum.unique())\r\n",
    "    # split_dat = int(len(alla_datum) * (1-frac))\r\n",
    "    # split_ix = dforg.loc[dforg.datum==split_dat]\r\n",
    "    \r\n",
    "    return dforg, alla_datum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df,alla_datum = load_data() \r\n",
    "df = remove_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Min manuella stacking (TimeSeries)\r\n",
    "TimeSeries kan iinte använda sklearn.stacking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features=CAT_FEATURES):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# SKIPPA DENNA\r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, y, by=pref+'ekipage',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(df.select_dtypes('object').columns))\r\n",
    "print()\r\n",
    "print(list(df.select_dtypes('number').columns))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    # df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön h'\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "       assert False, 'Felaktigt kön s'\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön v'\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df\r\n",
    "# s_c = FunctionTransformer(set_cols)\r\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# union = FeatureUnion([('o',df.select_dtypes('object')), \r\n",
    "#                      ('n',df.select_dtypes('object')), \r\n",
    "#                       ]\r\n",
    "#                       )\r\n",
    "\r\n",
    "pipe=make_pipeline(SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1))\r\n",
    "\r\n",
    "# mapper = DataFrameMapper([\r\n",
    "#     (['datum'], None),\r\n",
    "#     (['bana'], [lower,\r\n",
    "#                 SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing'), ]),\r\n",
    "#     (['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'], [lower,\r\n",
    "#                 SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing'), ],{'alias':'hx_bana'}),\r\n",
    "#     (['kön'], OneHotEncoder(sparse=False),{'alias':'kön'}),\r\n",
    "    \r\n",
    "#     (['kusk','h1_kusk','häst','plac'], lower, CustomSmoothMean(cols=['kusk','h1_kusk'],col2='häst',y='plac')),\r\n",
    "#     (['h1_kusk','häst'], lower,{'alias':'h1ekipage'}),\r\n",
    "#     (['h2_kusk','häst'], lower,{'alias':'h2ekipage'}),\r\n",
    "#     (['h3_kusk','häst'], lower,{'alias':'h3ekipage'}),\r\n",
    "#     (['h4_kusk','häst'], lower,{'alias':'h4ekipage'}),\r\n",
    "#     (['h5_kusk','häst'], lower,{'alias':'h5ekipage'}),\r\n",
    "    \r\n",
    "# ],df_out=True,input_df=True)\r\n",
    "# pipe2=Pipeline([('the_mapper',mapper), ('the_pipe',pipe)])\r\n",
    "# display(CustomSmoothMean(['kusk','h1_kusk'],['häst'],y='plac').fit(df).transform(df))\r\n",
    "\r\n",
    "# # svar1f = CustomSmoothMean.fit(df)\r\n",
    "# # svar1=mapper.fit_transform(df)\r\n",
    "# svar2=pipe.fit_transform(df.select_dtypes(include='number'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# test_pipe=make_pipeline(CustomSmoothMean(col1='kusk',col2='häst',y='plac'))\r\n",
    "# def date_to_num(df):\r\n",
    "#     return pd.DataFrame(pd.to_datetime(df.datum).view(float)*10e210)\r\n",
    "\r\n",
    "# tranf_datum = FunctionTransformer(date_to_num)\r\n",
    "    \r\n",
    "# preprocessor = make_column_transformer(\r\n",
    "                                    \r\n",
    "#                                     (CustomSmoothMean(cols=['kusk','h1_kusk'],col2='häst',y='plac',m=30), ['kusk','h1_kusk','häst','plac']),\r\n",
    "#                                     (tranf_datum, ['datum']),\r\n",
    "#                                     (OneHotEncoder(), ['kön']), \r\n",
    "#                                      remainder='drop')\r\n",
    "\r\n",
    "# # test_pipe.fit_transform(df.copy())\r\n",
    "# display(preprocessor.fit_transform(df.copy()))\r\n",
    "\r\n",
    "# type((pd.to_datetime(df.datum).view(float)*10e210).values)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## test test\r\n",
    "# Partition data\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['plac']), \r\n",
    "                                                    df['plac'], \r\n",
    "                                                    test_size=.2, \r\n",
    "                                                    random_state=2021)\r\n",
    "\r\n",
    "# Define categorical columns\r\n",
    "categorical = list(X_train.select_dtypes('object').columns)\r\n",
    "print(f\"Categorical columns are: {categorical}\")\r\n",
    "\r\n",
    "# Define numerical columns\r\n",
    "numerical = list(X_train.select_dtypes('number').columns)\r\n",
    "print(f\"Numerical columns are: {numerical}\")# Define categorical pipeline\r\n",
    "cat_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\r\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\r\n",
    "])\r\n",
    "\r\n",
    "# Define numerical pipeline\r\n",
    "num_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='median')),\r\n",
    "    ('scaler', MinMaxScaler())\r\n",
    "])\r\n",
    "\r\n",
    "# Fit column transformer to training data\r\n",
    "preprocessor = ColumnTransformer([\r\n",
    "    ('cat', cat_pipe, categorical),\r\n",
    "    ('num', num_pipe, numerical)\r\n",
    "])\r\n",
    "preprocessor.fit(X_train)\r\n",
    "\r\n",
    "# Prepare column names\r\n",
    "cat_columns = preprocessor.named_transformers_['cat']['encoder'].get_feature_names(categorical)\r\n",
    "columns = np.append(cat_columns, numerical)\r\n",
    "\r\n",
    "# Inspect training data before and after\r\n",
    "print(\"******************** Training data ********************\")\r\n",
    "display(X_train.shape)\r\n",
    "display(len(columns))\r\n",
    "display(preprocessor.transform(X_train).shape)\r\n",
    "final=pd.DataFrame(preprocessor.transform(X_train),columns=columns)\r\n",
    "\r\n",
    "# Inspect test data before and after\r\n",
    "print(\"******************** Test data ********************\")\r\n",
    "# display(X_test)\r\n",
    "display(pd.DataFrame(preprocessor.transform(X_test), columns=columns))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        assert False\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## stacking prepare and run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trdf,y=prepare_all(df)\r\n",
    "scorer = make_scorer(roc_auc_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500,1000],\r\n",
    "          'depth': [2,3,4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['AUC'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best gridsearch',catb_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XGBoost model \r\n",
    "import xgboost as xgb\r\n",
    "label = y\r\n",
    "dtrain = xgb.DMatrix(trdf, label=label)\r\n",
    "param = {'max_depth':2, 'eta':1 }\r\n",
    "num_round = 10\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'nthread':[4], #when use hyperthread, xgboost may become slower\r\n",
    "              'objective':['binary:logistic'],\r\n",
    "              'learning_rate': [0.09,0.1,0.15], #so called `eta` value\r\n",
    "              'max_depth': [7,8,9],\r\n",
    "              'min_child_weight': [9,10,11],\r\n",
    "              'use_label_encoder':[False],\r\n",
    "            #   'silent': [1],\r\n",
    "              'eval_metric': ['logloss'],\r\n",
    "              'subsample': [0.5,0.9,1.0],\r\n",
    "              'colsample_bytree': [0.7, 0.9, 1.0],\r\n",
    "              'n_estimators': [7,8,9], #number of trees, change it to 1000 for better results\r\n",
    "              'missing':[-999],\r\n",
    "              'seed': [2021],\r\n",
    "              }\r\n",
    "\r\n",
    "xgb_clf = xgb.XGBClassifier(num_round=num_round)\r\n",
    "xgb_grid = GridSearchCV(estimator=xgb_clf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "xgb_grid.fit(trdf, y )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_xgb = xgb_grid.best_estimator_\r\n",
    "print('best gridsearch', xgb_grid.best_score_)\r\n",
    "best_param = xgb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(best_xgb.feature_importances_,index=trdf.columns).sort_values(by=0,ascending=False).head(6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ExtraTree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ExtraTree  model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.tree import ExtraTreeClassifier\r\n",
    "et = ExtraTreeClassifier(min_samples_split=2, random_state=2021,class_weight='balanced')\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': [ 'balanced'  ,None], \r\n",
    "          'max_depth': [None, 5, 10  ,15 ,20],\r\n",
    "          'min_samples_leaf': [1, 2 ,3, 4,],\r\n",
    "          'min_samples_split': [2,30, 30,  40  ,45],   \r\n",
    "          'criterion': [ 'gini'  ,'entropy'],   \r\n",
    "          'splitter': ['random',  'best']\r\n",
    "         }\r\n",
    "\r\n",
    "et_grid = GridSearchCV(estimator=et, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "et_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_et = et_grid.best_estimator_\r\n",
    "print('best gridsearch', et_grid.best_score_)\r\n",
    "best_param = et_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN model\r\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=4, )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_neighbors': [10,15,20],\r\n",
    "          \r\n",
    "         }\r\n",
    "\r\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "knn_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_knn = knn_grid.best_estimator_\r\n",
    "print('best gridsearch',knn_grid.best_score_)\r\n",
    "best_param = knn_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearch\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearchCV\r\n",
    "# from sklearn.svm import SVC\r\n",
    "# svc = SVC(C=1.0, gamma='scale', tol=0.001, cache_size=200, class_weight='balanced', random_state=2021)\r\n",
    "\r\n",
    "# tscv = TimeSeriesSplit()\r\n",
    "# params = {'C': [1,2,3],\r\n",
    "#           'gamma': ['scale','auto'],\r\n",
    "#           'class_weight': ['balanced'],\r\n",
    "#           'random_state': [2021],\r\n",
    "#          }\r\n",
    "\r\n",
    "# svc_grid = GridSearchCV(estimator=svc, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# svc_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get best estimator and params\r\n",
    "# best_svc = svc_grid.best_estimator_\r\n",
    "# print('best gridsearch', svc_grid.best_score_)\r\n",
    "# best_param = svc_grid.best_params_\r\n",
    "# best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('xgb',best_xgb,),\r\n",
    "               ('rf',best_rf,),\r\n",
    "               ('catb', best_catb,),\r\n",
    "              ('knn', best_knn, ),\r\n",
    "              ('et', best_et)              # ger  sämre res\r\n",
    "               # ('ridge', best_ridge, ) ,   # saknar predict_proba - usless!\r\n",
    "            #    ('svc', best_svc, ),        # tar extremt lång tid för fit\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV(class_weight='balanced')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_model(model, X, y, scoring=scorer):\r\n",
    "    print('scorer =',scorer)\r\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=tscv, verbose=1, n_jobs=3, error_score='raise')\r\n",
    "    return scores\r\n",
    "\r\n",
    "def Stacking(model_item, X_tr, y_tr, X_final, n_fold):\r\n",
    "    model=model_item[1]\r\n",
    "    print(model_item[0], end=' ')\r\n",
    "    tscv = TimeSeriesSplit(n_splits=n_fold)\r\n",
    "    # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "    train_pred=np.empty((0,1),float)\r\n",
    "    for n, (train_indices, test_indices) in enumerate(tscv.split(X_tr)):\r\n",
    "        if n==0:\r\n",
    "            the_first_set_len = len(train_indices) # the first set that cannot be used i timeSeries stacking\r\n",
    "            \r\n",
    "        X_train, X_test = X_tr.iloc[train_indices], X_tr.iloc[test_indices]\r\n",
    "        y_train, y_test = y_tr.iloc[train_indices], y_tr.iloc[test_indices]\r\n",
    "        print(n,end=' ')\r\n",
    "        model.fit(X=X_train,y=y_train)\r\n",
    "        train_pred=np.append(train_pred,model.predict_proba(X_test)[:,1])\r\n",
    "    print(f'- final fit (the_first_set_len={the_first_set_len})' )\r\n",
    "    model.fit(X=X_tr,y=y_tr) # fit on all data (except the final data)   \r\n",
    "    valid_pred = model.predict_proba(X_final)[:,1]\r\n",
    "    return model,valid_pred.reshape(-1,1), train_pred, the_first_set_len\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_ix = int(len(trdf)*.8)\r\n",
    "train_X = trdf[trdf.index <  split_ix]\r\n",
    "valid_X = trdf[trdf.index >= split_ix]\r\n",
    "train_y = y[y.index <  split_ix]\r\n",
    "valid_y = y[y.index >=  split_ix]\r\n",
    "\r\n",
    "# the estimators\r\n",
    "valid_pred=[None] * len(base_models)\r\n",
    "train_pred=[None] * len(base_models)\r\n",
    "model=[None] * len(base_models)\r\n",
    "for n, model_item in enumerate(base_models):\r\n",
    "    model[n],valid_pred[n] ,train_pred[n], the_first_set_len = Stacking(model_item,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "    train_pred[n]=pd.DataFrame(train_pred[n],columns=[model_item[0]])\r\n",
    "    valid_pred[n]=pd.DataFrame(valid_pred[n],columns=[model_item[0]])\r\n",
    "    \r\n",
    "    scores=evaluate_model(model[n],train_X,train_y)\r\n",
    "    print(f'mean={np.mean(scores)}: {scores}')\r\n",
    "train_y = train_y.iloc[the_first_set_len:]      # remove the first set that can't be used in timeseries stacking\r\n",
    "train_pred=pd.concat(train_pred,axis=1)\r\n",
    "valid_pred=pd.concat(valid_pred,axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final estimation with the meta model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "meta_model.fit(train_pred,train_y)\r\n",
    "scores=evaluate_model(meta_model,valid_pred,valid_y)\r\n",
    "time.sleep(0.2)\r\n",
    "print('models', list(valid_pred.columns))\r\n",
    "print(f'mean={np.mean(scores)}: {scores}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rf, catb, knn, et - 0.88803\r\n",
    "## xgb, rf, catb, knn, et - 0.88720"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}