{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try stacking for V75"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool, cv, utils \r\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "source": [
    "### return a CatBoost model with some default parameters\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "# def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "#     # print('cat_features',cat_features)\r\n",
    "#     for c in cat_features:\r\n",
    "#         # print(c)\r\n",
    "#         X_train.loc[X_train[c].isna(),c] = 'missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "#         if X_test is not None:  ## om X_test är med\r\n",
    "#             X_test.loc [X_test[c].isna(),c] = 'missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "    \r\n",
    "#     return X_train, X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "source": [
    "def remove_not_used_features(df):\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "df,alla_datum,split_ix = load_data() \r\n",
    "df = remove_not_used_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.208773316093193"
      ]
     },
     "metadata": {},
     "execution_count": 362
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "# för ´categorical\r\n",
    "def impute_test(df):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "source": [
    "# from sklearn.datasets import load_iris\r\n",
    "from sklearn_pandas import DataFrameMapper\r\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline, Pipeline\r\n",
    "import sklearn.pipeline, sklearn.metrics, sklearn.compose\r\n",
    "from sklearn.preprocessing import FunctionTransformer\r\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "# Import modules for feature engineering and modelling\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features=CAT_FEATURES):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # return dict with value for each 'häst'\r\n",
    "    # df['smooth']=df[by].map(smooth)\r\n",
    "    \r\n",
    "    # return df[['häst','smooth']].to_dict()\r\n",
    "    display('used m = ',m)\r\n",
    "    return smooth.to_dict()\r\n",
    "\r\n",
    "# dict1=calc_smooth_mean(df,'häst','plac',tot_mean=df.plac.mean())\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# pd.DataFrame.from_dict(dict,orient='index',columns=['smooth'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "source": [
    "# Handle häst and kusk \r\n",
    "class CustomSmoothMean(BaseEstimator, TransformerMixin):\r\n",
    "    def __init__(self,cols,col2,y,m=30):\r\n",
    "        super().__init__()\r\n",
    "        self.map = {}\r\n",
    "        self.total_mean=None\r\n",
    "        self.cols=cols\r\n",
    "        self.col2=col2\r\n",
    "        self.y = y\r\n",
    "        self.m=m\r\n",
    "\r\n",
    "    def fit(self, df, y=None):\r\n",
    "        \r\n",
    "        self.total_mean=df[self.y].mean()\r\n",
    "        self.map = calc_smooth_mean(df, y=self.y,by=self.cols[0],m=self.m,tot_mean=df[self.y].mean())\r\n",
    "        self.map[None] = 'missing'\r\n",
    "        self.map[np.nan] = 'missing'\r\n",
    "        for col in self.cols:\r\n",
    "            df[col] = df[col].str.cat(df[self.col2], sep =\", \")\r\n",
    "            \r\n",
    "        display(f'using m={self.m}')\r\n",
    "    \r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, df, y=None):\r\n",
    "        print('shape',df.shape)\r\n",
    "        for col in self.cols:\r\n",
    "            df[col] = df[col].map(self.map)\r\n",
    "\r\n",
    "        # df.drop(self.col2,axis=1)\r\n",
    "        return df\r\n",
    "    def get_feature_names(self):\r\n",
    "        return self.cols,self.col2,self.y\r\n",
    "\r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "source": [
    "print(list(df.select_dtypes('object').columns))\r\n",
    "print()\r\n",
    "print(list(df.select_dtypes('number').columns))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['datum', 'bana', 'häst', 'kusk', 'kön', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana', 'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
      "\n",
      "['streck', 'kr', 'spår', 'dist', 'lopp_dist', 'start', 'ålder', 'plac', 'pris', 'h1_spår', 'h1_plac', 'h1_pris', 'h1_odds', 'h1_kmtid', 'h2_spår', 'h2_plac', 'h2_pris', 'h2_odds', 'h2_kmtid', 'h3_spår', 'h3_plac', 'h3_pris', 'h3_odds', 'h3_kmtid', 'h4_spår', 'h4_plac', 'h4_pris', 'h4_odds', 'h4_kmtid', 'h5_spår', 'h5_plac', 'h5_pris', 'h5_odds', 'h5_kmtid', 'h1_dist', 'h2_dist', 'h3_dist', 'h4_dist', 'h5_dist', 'h1_auto', 'h2_auto', 'h3_auto', 'h4_auto', 'h5_auto', 'h1_perf', 'h2_perf', 'h3_perf', 'h4_perf', 'h5_perf', 'senast', 'delta1', 'delta2', 'delta3', 'delta4']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    # df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön h'\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "       assert False, 'Felaktigt kön s'\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön v'\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df\r\n",
    "def set_lower(dfo):\r\n",
    "    df=dfo.copy()\r\n",
    "    for c in df.columns:\r\n",
    "        df[c] = df[c].str.lower()\r\n",
    "    return df\r\n",
    "def set_cols(df):\r\n",
    "    return pd.DataFrame(df,columns=['kön_h','kön_s','kön_v'])\r\n",
    "\r\n",
    "# print(lower(df[['kön']]))\r\n",
    "lower =  FunctionTransformer(set_lower)\r\n",
    "# s_c = FunctionTransformer(set_cols)\r\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# union = FeatureUnion([('o',df.select_dtypes('object')), \r\n",
    "#                      ('n',df.select_dtypes('object')), \r\n",
    "#                       ]\r\n",
    "#                       )\r\n",
    "\r\n",
    "pipe=make_pipeline(SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1))\r\n",
    "\r\n",
    "mapper = DataFrameMapper([\r\n",
    "    (['datum'], None),\r\n",
    "    (['bana'], [lower,\r\n",
    "                SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing'), ]),\r\n",
    "    (['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'], [lower,\r\n",
    "                SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing'), ],{'alias':'hx_bana'}),\r\n",
    "    (['kön'], OneHotEncoder(sparse=False),{'alias':'kön'}),\r\n",
    "    \r\n",
    "    (['kusk','h1_kusk','häst','plac'], lower, CustomSmoothMean(cols=['kusk','h1_kusk'],col2='häst',y='plac')),\r\n",
    "    (['h1_kusk','häst'], lower,{'alias':'h1ekipage'}),\r\n",
    "    (['h2_kusk','häst'], lower,{'alias':'h2ekipage'}),\r\n",
    "    (['h3_kusk','häst'], lower,{'alias':'h3ekipage'}),\r\n",
    "    (['h4_kusk','häst'], lower,{'alias':'h4ekipage'}),\r\n",
    "    (['h5_kusk','häst'], lower,{'alias':'h5ekipage'}),\r\n",
    "    \r\n",
    "],df_out=True,input_df=True)\r\n",
    "pipe2=Pipeline([('the_mapper',mapper), ('the_pipe',pipe)])\r\n",
    "display(CustomSmoothMean(['kusk','h1_kusk'],['häst'],y='plac').fit(df).transform(df))\r\n",
    "\r\n",
    "# svar1f = CustomSmoothMean.fit(df)\r\n",
    "# svar1=mapper.fit_transform(df)\r\n",
    "svar2=pipe.fit_transform(df.select_dtypes(include='number'))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'used m = '"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'using m=30'"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape (41763, 69)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>bana</th>\n",
       "      <th>häst</th>\n",
       "      <th>kusk</th>\n",
       "      <th>streck</th>\n",
       "      <th>kr</th>\n",
       "      <th>spår</th>\n",
       "      <th>dist</th>\n",
       "      <th>lopp_dist</th>\n",
       "      <th>start</th>\n",
       "      <th>...</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>ÖREBRO</td>\n",
       "      <td>ALLABALLAKAITOZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3935.030968</td>\n",
       "      <td>6006.507182</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>8.366600</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>ÖREBRO</td>\n",
       "      <td>ARISTOCAT BOKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23466.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2735.738970</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>753.137355</td>\n",
       "      <td>753.137355</td>\n",
       "      <td>900.171313</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>ÖREBRO</td>\n",
       "      <td>ART ON LINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20696.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2974.603812</td>\n",
       "      <td>0.917738</td>\n",
       "      <td>4904.292577</td>\n",
       "      <td>6006.507182</td>\n",
       "      <td>6935.717077</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>ÖREBRO</td>\n",
       "      <td>BEAR DANCER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>27477.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>14.142136</td>\n",
       "      <td>11501.585598</td>\n",
       "      <td>10966.331584</td>\n",
       "      <td>11501.585598</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>ÖREBRO</td>\n",
       "      <td>BY AIR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30589.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>11.180340</td>\n",
       "      <td>2735.738970</td>\n",
       "      <td>2735.738970</td>\n",
       "      <td>3629.367876</td>\n",
       "      <td>16.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41758</th>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>JÄGERSRO</td>\n",
       "      <td>ULTRA BRIGHT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>74765.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6976.064300</td>\n",
       "      <td>4231.196882</td>\n",
       "      <td>5983.816016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41759</th>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>JÄGERSRO</td>\n",
       "      <td>WILD LOVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>58857.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4034.287935</td>\n",
       "      <td>15.811388</td>\n",
       "      <td>5471.477941</td>\n",
       "      <td>3460.466492</td>\n",
       "      <td>7737.838310</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41760</th>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>JÄGERSRO</td>\n",
       "      <td>RACING BRODDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>135036.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6651.416330</td>\n",
       "      <td>6651.416330</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>1484.131591</td>\n",
       "      <td>878.024090</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41761</th>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>JÄGERSRO</td>\n",
       "      <td>KALI SMART</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53634.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4693.236175</td>\n",
       "      <td>5705.344712</td>\n",
       "      <td>11501.585598</td>\n",
       "      <td>18994.243477</td>\n",
       "      <td>11501.585598</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41762</th>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>JÄGERSRO</td>\n",
       "      <td>DEVS DAFFODIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22087.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3124.945997</td>\n",
       "      <td>6006.507182</td>\n",
       "      <td>347.317790</td>\n",
       "      <td>491.181529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41763 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            datum      bana             häst kusk  streck        kr  spår  \\\n",
       "0      2014-12-28    ÖREBRO  ALLABALLAKAITOZ  NaN     5.0   21018.0   6.0   \n",
       "1      2014-12-28    ÖREBRO   ARISTOCAT BOKO  NaN     7.0   23466.0  12.0   \n",
       "2      2014-12-28    ÖREBRO      ART ON LINE  NaN    23.0   20696.0   2.0   \n",
       "3      2014-12-28    ÖREBRO      BEAR DANCER  NaN    48.0   27477.0   1.0   \n",
       "4      2014-12-28    ÖREBRO           BY AIR  NaN     5.0   30589.0   5.0   \n",
       "...           ...       ...              ...  ...     ...       ...   ...   \n",
       "41758  2021-09-04  JÄGERSRO     ULTRA BRIGHT  NaN     4.0   74765.0   6.0   \n",
       "41759  2021-09-04  JÄGERSRO        WILD LOVE  NaN     8.0   58857.0   7.0   \n",
       "41760  2021-09-04  JÄGERSRO    RACING BRODDA  NaN     3.0  135036.0   8.0   \n",
       "41761  2021-09-04  JÄGERSRO       KALI SMART  NaN     3.0   53634.0   9.0   \n",
       "41762  2021-09-04  JÄGERSRO    DEVS DAFFODIL  NaN     1.0   22087.0  10.0   \n",
       "\n",
       "         dist  lopp_dist  start  ...       h1_perf      h2_perf       h3_perf  \\\n",
       "0      2100.0     2100.0      0  ...   3935.030968  6006.507182     11.180340   \n",
       "1      2100.0     2100.0      0  ...   2735.738970  1484.131591    753.137355   \n",
       "2      2100.0     2100.0      0  ...   2974.603812     0.917738   4904.292577   \n",
       "3      2100.0     2100.0      0  ...  10966.331584    14.142136  11501.585598   \n",
       "4      2100.0     2100.0      0  ...      5.477226    11.180340   2735.738970   \n",
       "...       ...        ...    ...  ...           ...          ...           ...   \n",
       "41758  2140.0     2140.0      0  ...           NaN  6976.064300   4231.196882   \n",
       "41759  2140.0     2140.0      0  ...   4034.287935    15.811388   5471.477941   \n",
       "41760  2140.0     2140.0      0  ...   6651.416330  6651.416330   1484.131591   \n",
       "41761  2140.0     2140.0      0  ...   4693.236175  5705.344712  11501.585598   \n",
       "41762  2140.0     2140.0      0  ...   3124.945997  6006.507182    347.317790   \n",
       "\n",
       "            h4_perf       h5_perf senast  delta1  delta2  delta3  delta4  \n",
       "0          8.366600      5.000000   21.0    19.0    17.0    10.0    18.0  \n",
       "1        753.137355    900.171313    8.0     7.0    24.0    14.0    11.0  \n",
       "2       6006.507182   6935.717077    9.0     6.0    19.0    41.0    15.0  \n",
       "3      10966.331584  11501.585598   18.0    25.0    14.0    13.0     8.0  \n",
       "4       2735.738970   3629.367876   16.0   230.0    14.0    49.0    14.0  \n",
       "...             ...           ...    ...     ...     ...     ...     ...  \n",
       "41758   5983.816016           NaN    7.0    49.0    13.0    29.0    13.0  \n",
       "41759   3460.466492   7737.838310   13.0    26.0    10.0     9.0    19.0  \n",
       "41760   1484.131591    878.024090   17.0    10.0    25.0    32.0    11.0  \n",
       "41761  18994.243477  11501.585598   17.0    22.0    17.0    41.0    36.0  \n",
       "41762    491.181529           NaN   10.0    15.0   206.0    21.0    19.0  \n",
       "\n",
       "[41763 rows x 69 columns]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(svar1, columns=list(svar1.columns))\r\n",
    "# svar1.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "source": [
    "\r\n",
    "# test_pipe=make_pipeline(CustomSmoothMean(col1='kusk',col2='häst',y='plac'))\r\n",
    "def date_to_num(df):\r\n",
    "    return pd.DataFrame(pd.to_datetime(df.datum).view(float)*10e210)\r\n",
    "\r\n",
    "tranf_datum = FunctionTransformer(date_to_num)\r\n",
    "    \r\n",
    "preprocessor = make_column_transformer(\r\n",
    "                                    \r\n",
    "                                    (CustomSmoothMean(cols=['kusk','h1_kusk'],col2='häst',y='plac',m=30), ['kusk','h1_kusk','häst','plac']),\r\n",
    "                                    (tranf_datum, ['datum']),\r\n",
    "                                    (OneHotEncoder(), ['kön']), \r\n",
    "                                     remainder='drop')\r\n",
    "\r\n",
    "# test_pipe.fit_transform(df.copy())\r\n",
    "display(preprocessor.fit_transform(df.copy()))\r\n",
    "\r\n",
    "# type((pd.to_datetime(df.datum).view(float)*10e210).values)\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'used m = '"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'using m=30'"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape (41763, 4)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "array([[nan, nan, 'ALLABALLAKAITOZ', ..., 0.0, 0.0, 1.0],\n",
       "       [nan, nan, 'ARISTOCAT BOKO', ..., 0.0, 0.0, 1.0],\n",
       "       [nan, nan, 'ART ON LINE', ..., 0.0, 0.0, 1.0],\n",
       "       ...,\n",
       "       [nan, nan, 'RACING BRODDA', ..., 0.0, 1.0, 0.0],\n",
       "       [nan, nan, 'KALI SMART', ..., 0.0, 1.0, 0.0],\n",
       "       [nan, nan, 'DEVS DAFFODIL', ..., 0.0, 1.0, 0.0]], dtype=object)"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "## TEST TEST\r\n",
    "\r\n",
    "# Set seed for reproducibility\r\n",
    "seed = 123\r\n",
    "\r\n",
    "# Import package/module for data\r\n",
    "import pandas as pd\r\n",
    "from seaborn import load_dataset\r\n",
    "\r\n",
    "# Import modules for feature engineering and modelling\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "# Load dataset\r\n",
    "# df = load_dataset('tips').drop(columns=['tip', 'sex']).sample(n=5, random_state=seed)\r\n",
    "\r\n",
    "# # Add missing values\r\n",
    "# df.iloc[[1, 2, 4], [2, 4]] = np.nan\r\n",
    "# df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "source": [
    "## test test\r\n",
    "# Partition data\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['plac']), \r\n",
    "                                                    df['plac'], \r\n",
    "                                                    test_size=.2, \r\n",
    "                                                    random_state=seed)\r\n",
    "\r\n",
    "# Define categorical columns\r\n",
    "categorical = list(X_train.select_dtypes('object').columns)\r\n",
    "print(f\"Categorical columns are: {categorical}\")\r\n",
    "\r\n",
    "# Define numerical columns\r\n",
    "numerical = list(X_train.select_dtypes('number').columns)\r\n",
    "print(f\"Numerical columns are: {numerical}\")# Define categorical pipeline\r\n",
    "cat_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\r\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\r\n",
    "])\r\n",
    "\r\n",
    "# Define numerical pipeline\r\n",
    "num_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='median')),\r\n",
    "    ('scaler', MinMaxScaler())\r\n",
    "])\r\n",
    "\r\n",
    "# Fit column transformer to training data\r\n",
    "preprocessor = ColumnTransformer([\r\n",
    "    ('cat', cat_pipe, categorical),\r\n",
    "    ('num', num_pipe, numerical)\r\n",
    "])\r\n",
    "preprocessor.fit(X_train)\r\n",
    "\r\n",
    "# Prepare column names\r\n",
    "cat_columns = preprocessor.named_transformers_['cat']['encoder'].get_feature_names(categorical)\r\n",
    "columns = np.append(cat_columns, numerical)\r\n",
    "\r\n",
    "# Inspect training data before and after\r\n",
    "print(\"******************** Training data ********************\")\r\n",
    "display(X_train.shape)\r\n",
    "display(len(columns))\r\n",
    "display(preprocessor.transform(X_train).shape)\r\n",
    "final=pd.DataFrame(preprocessor.transform(X_train),columns=columns)\r\n",
    "\r\n",
    "# Inspect test data before and after\r\n",
    "print(\"******************** Test data ********************\")\r\n",
    "# display(X_test)\r\n",
    "display(pd.DataFrame(preprocessor.transform(X_test), columns=columns))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Categorical columns are: ['datum', 'bana', 'häst', 'kusk', 'kön', 'h1_kusk', 'h1_bana', 'h2_kusk', 'h2_bana', 'h3_kusk', 'h3_bana', 'h4_kusk', 'h4_bana', 'h5_kusk', 'h5_bana']\n",
      "Numerical columns are: ['streck', 'kr', 'spår', 'dist', 'lopp_dist', 'start', 'ålder', 'pris', 'h1_spår', 'h1_plac', 'h1_pris', 'h1_odds', 'h1_kmtid', 'h2_spår', 'h2_plac', 'h2_pris', 'h2_odds', 'h2_kmtid', 'h3_spår', 'h3_plac', 'h3_pris', 'h3_odds', 'h3_kmtid', 'h4_spår', 'h4_plac', 'h4_pris', 'h4_odds', 'h4_kmtid', 'h5_spår', 'h5_plac', 'h5_pris', 'h5_odds', 'h5_kmtid', 'h1_dist', 'h2_dist', 'h3_dist', 'h4_dist', 'h5_dist', 'h1_auto', 'h2_auto', 'h3_auto', 'h4_auto', 'h5_auto', 'h1_perf', 'h2_perf', 'h3_perf', 'h4_perf', 'h5_perf', 'senast', 'delta1', 'delta2', 'delta3', 'delta4']\n",
      "******************** Training data ********************\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "(33410, 68)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "21098"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "(33410, 21098)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "******************** Test data ********************\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum_2014-12-28</th>\n",
       "      <th>datum_2015-01-03</th>\n",
       "      <th>datum_2015-01-10</th>\n",
       "      <th>datum_2015-01-17</th>\n",
       "      <th>datum_2015-01-24</th>\n",
       "      <th>datum_2015-01-25</th>\n",
       "      <th>datum_2015-01-31</th>\n",
       "      <th>datum_2015-02-07</th>\n",
       "      <th>datum_2015-02-14</th>\n",
       "      <th>datum_2015-02-21</th>\n",
       "      <th>...</th>\n",
       "      <th>h1_perf</th>\n",
       "      <th>h2_perf</th>\n",
       "      <th>h3_perf</th>\n",
       "      <th>h4_perf</th>\n",
       "      <th>h5_perf</th>\n",
       "      <th>senast</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018350</td>\n",
       "      <td>0.049891</td>\n",
       "      <td>0.070556</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.041209</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>0.021239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049889</td>\n",
       "      <td>0.132286</td>\n",
       "      <td>0.122471</td>\n",
       "      <td>0.099997</td>\n",
       "      <td>0.270799</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.010619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.135622</td>\n",
       "      <td>0.023437</td>\n",
       "      <td>0.017469</td>\n",
       "      <td>0.011047</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.008850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.249998</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.115929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122471</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.036784</td>\n",
       "      <td>0.036785</td>\n",
       "      <td>0.047490</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.037369</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.013274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234517</td>\n",
       "      <td>0.055780</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>0.223604</td>\n",
       "      <td>0.258197</td>\n",
       "      <td>0.079670</td>\n",
       "      <td>0.091181</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.013274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060649</td>\n",
       "      <td>0.055780</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>0.042476</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.020927</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>0.042231</td>\n",
       "      <td>0.020354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244946</td>\n",
       "      <td>0.158112</td>\n",
       "      <td>0.095898</td>\n",
       "      <td>0.060650</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.453297</td>\n",
       "      <td>0.079223</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.024779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041740</td>\n",
       "      <td>0.091968</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>0.042476</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.027473</td>\n",
       "      <td>0.016442</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.014159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035276</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.074176</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>0.006195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8353 rows × 21098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      datum_2014-12-28  datum_2015-01-03  datum_2015-01-10  datum_2015-01-17  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "8348               0.0               0.0               0.0               0.0   \n",
       "8349               0.0               0.0               0.0               0.0   \n",
       "8350               0.0               0.0               0.0               0.0   \n",
       "8351               0.0               0.0               0.0               0.0   \n",
       "8352               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      datum_2015-01-24  datum_2015-01-25  datum_2015-01-31  datum_2015-02-07  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "8348               0.0               0.0               0.0               0.0   \n",
       "8349               0.0               0.0               0.0               0.0   \n",
       "8350               0.0               0.0               0.0               0.0   \n",
       "8351               0.0               0.0               0.0               0.0   \n",
       "8352               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      datum_2015-02-14  datum_2015-02-21  ...   h1_perf   h2_perf   h3_perf  \\\n",
       "0                  0.0               0.0  ...  0.018350  0.049891  0.070556   \n",
       "1                  0.0               0.0  ...  0.049889  0.132286  0.122471   \n",
       "2                  0.0               0.0  ...  0.000116  0.135622  0.023437   \n",
       "3                  0.0               0.0  ...  0.122008  0.249998  0.033830   \n",
       "4                  0.0               0.0  ...  0.122471  0.000109  0.036784   \n",
       "...                ...               ...  ...       ...       ...       ...   \n",
       "8348               0.0               0.0  ...  0.234517  0.055780  0.054246   \n",
       "8349               0.0               0.0  ...  0.060649  0.055780  0.054246   \n",
       "8350               0.0               0.0  ...  0.244946  0.158112  0.095898   \n",
       "8351               0.0               0.0  ...  0.041740  0.091968  0.054246   \n",
       "8352               0.0               0.0  ...  0.035276  0.027325  0.000108   \n",
       "\n",
       "       h4_perf   h5_perf    senast    delta1    delta2    delta3    delta4  \n",
       "0     0.040735  0.040735  0.041209  0.052317  0.007299  0.013034  0.021239  \n",
       "1     0.099997  0.270799  0.065934  0.022422  0.006257  0.014599  0.010619  \n",
       "2     0.017469  0.011047  0.060440  0.028401  0.005735  0.013556  0.008850  \n",
       "3     0.014615  0.024706  0.076923  0.031390  0.003650  0.009906  0.115929  \n",
       "4     0.036785  0.047490  0.030220  0.037369  0.007299  0.008863  0.013274  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8348  0.223604  0.258197  0.079670  0.091181  0.003650  0.007821  0.013274  \n",
       "8349  0.042476  0.040735  0.057692  0.020927  0.005735  0.042231  0.020354  \n",
       "8350  0.060650  0.040735  0.453297  0.079223  0.008863  0.007299  0.024779  \n",
       "8351  0.042476  0.040735  0.027473  0.016442  0.011992  0.003128  0.014159  \n",
       "8352  0.000163  0.000163  0.074176  0.017937  0.014599  0.016163  0.006195  \n",
       "\n",
       "[8353 rows x 21098 columns]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        9.224990e-03\n",
       "1        9.224990e-03\n",
       "2        9.224990e-03\n",
       "3        9.224990e-03\n",
       "4        9.224990e-03\n",
       "             ...     \n",
       "41758    1.140408e+12\n",
       "41759    1.140408e+12\n",
       "41760    1.140408e+12\n",
       "41761    1.140408e+12\n",
       "41762    1.140408e+12\n",
       "Name: datum, Length: 41763, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        assert False\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## stacking prepare and run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trdf,y=prepare_all(df)\r\n",
    "scorer = make_scorer(roc_auc_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500,1000],\r\n",
    "          'depth': [2,3,4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['AUC'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best gridsearch',catb_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XGBoost model \r\n",
    "import xgboost as xgb\r\n",
    "label = y\r\n",
    "dtrain = xgb.DMatrix(trdf, label=label)\r\n",
    "param = {'max_depth':2, 'eta':1 }\r\n",
    "num_round = 10\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'nthread':[4], #when use hyperthread, xgboost may become slower\r\n",
    "              'objective':['binary:logistic'],\r\n",
    "              'learning_rate': [0.09,0.1,0.15], #so called `eta` value\r\n",
    "              'max_depth': [7,8,9],\r\n",
    "              'min_child_weight': [9,10,11],\r\n",
    "              'use_label_encoder':[False],\r\n",
    "            #   'silent': [1],\r\n",
    "              'eval_metric': ['logloss'],\r\n",
    "              'subsample': [0.5,0.9,1.0],\r\n",
    "              'colsample_bytree': [0.7, 0.9, 1.0],\r\n",
    "              'n_estimators': [7,8,9], #number of trees, change it to 1000 for better results\r\n",
    "              'missing':[-999],\r\n",
    "              'seed': [2021],\r\n",
    "              }\r\n",
    "\r\n",
    "xgb_clf = xgb.XGBClassifier(num_round=num_round)\r\n",
    "xgb_grid = GridSearchCV(estimator=xgb_clf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "xgb_grid.fit(trdf, y )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_xgb = xgb_grid.best_estimator_\r\n",
    "print('best gridsearch', xgb_grid.best_score_)\r\n",
    "best_param = xgb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(best_xgb.feature_importances_,index=trdf.columns).sort_values(by=0,ascending=False).head(6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ExtraTree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ExtraTree  model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.tree import ExtraTreeClassifier\r\n",
    "et = ExtraTreeClassifier(min_samples_split=2, random_state=2021,class_weight='balanced')\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': [ 'balanced'  ,None], \r\n",
    "          'max_depth': [None, 5, 10  ,15 ,20],\r\n",
    "          'min_samples_leaf': [1, 2 ,3, 4,],\r\n",
    "          'min_samples_split': [2,30, 30,  40  ,45],   \r\n",
    "          'criterion': [ 'gini'  ,'entropy'],   \r\n",
    "          'splitter': ['random',  'best']\r\n",
    "         }\r\n",
    "\r\n",
    "et_grid = GridSearchCV(estimator=et, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "et_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get best estimator and params\r\n",
    "best_et = et_grid.best_estimator_\r\n",
    "print('best gridsearch', et_grid.best_score_)\r\n",
    "best_param = et_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN model\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=4, )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_neighbors': [10,15,20],\r\n",
    "          \r\n",
    "         }\r\n",
    "\r\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "knn_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_knn = knn_grid.best_estimator_\r\n",
    "print('best gridsearch',knn_grid.best_score_)\r\n",
    "best_param = knn_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearch\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearchCV\r\n",
    "# from sklearn.svm import SVC\r\n",
    "# svc = SVC(C=1.0, gamma='scale', tol=0.001, cache_size=200, class_weight='balanced', random_state=2021)\r\n",
    "\r\n",
    "# tscv = TimeSeriesSplit()\r\n",
    "# params = {'C': [1,2,3],\r\n",
    "#           'gamma': ['scale','auto'],\r\n",
    "#           'class_weight': ['balanced'],\r\n",
    "#           'random_state': [2021],\r\n",
    "#          }\r\n",
    "\r\n",
    "# svc_grid = GridSearchCV(estimator=svc, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# svc_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get best estimator and params\r\n",
    "# best_svc = svc_grid.best_estimator_\r\n",
    "# print('best gridsearch', svc_grid.best_score_)\r\n",
    "# best_param = svc_grid.best_params_\r\n",
    "# best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('xgb',best_xgb,),\r\n",
    "               ('rf',best_rf,),\r\n",
    "               ('catb', best_catb,),\r\n",
    "              ('knn', best_knn, ),\r\n",
    "              ('et', best_et)              # ger  sämre res\r\n",
    "               # ('ridge', best_ridge, ) ,   # saknar predict_proba - usless!\r\n",
    "            #    ('svc', best_svc, ),        # tar extremt lång tid för fit\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV(class_weight='balanced')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_model(model, X, y, scoring=scorer):\r\n",
    "    print('scorer =',scorer)\r\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=tscv, verbose=1, n_jobs=3, error_score='raise')\r\n",
    "    return scores\r\n",
    "\r\n",
    "def Stacking(model_item, X_tr, y_tr, X_final, n_fold):\r\n",
    "    model=model_item[1]\r\n",
    "    print(model_item[0], end=' ')\r\n",
    "    tscv = TimeSeriesSplit(n_splits=n_fold)\r\n",
    "    # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "    train_pred=np.empty((0,1),float)\r\n",
    "    for n, (train_indices, test_indices) in enumerate(tscv.split(X_tr)):\r\n",
    "        if n==0:\r\n",
    "            the_first_set_len = len(train_indices) # the first set that cannot be used i timeSeries stacking\r\n",
    "            \r\n",
    "        X_train, X_test = X_tr.iloc[train_indices], X_tr.iloc[test_indices]\r\n",
    "        y_train, y_test = y_tr.iloc[train_indices], y_tr.iloc[test_indices]\r\n",
    "        print(n,end=' ')\r\n",
    "        model.fit(X=X_train,y=y_train)\r\n",
    "        train_pred=np.append(train_pred,model.predict_proba(X_test)[:,1])\r\n",
    "    print(f'- final fit (the_first_set_len={the_first_set_len})' )\r\n",
    "    model.fit(X=X_tr,y=y_tr) # fit on all data (except the final data)   \r\n",
    "    valid_pred = model.predict_proba(X_final)[:,1]\r\n",
    "    return model,valid_pred.reshape(-1,1), train_pred, the_first_set_len\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_ix = int(len(trdf)*.8)\r\n",
    "train_X = trdf[trdf.index <  split_ix]\r\n",
    "valid_X = trdf[trdf.index >= split_ix]\r\n",
    "train_y = y[y.index <  split_ix]\r\n",
    "valid_y = y[y.index >=  split_ix]\r\n",
    "# test 2 models\r\n",
    "valid_pred=[None] * len(base_models)\r\n",
    "train_pred=[None] * len(base_models)\r\n",
    "model=[None] * len(base_models)\r\n",
    "for n, model_item in enumerate(base_models):\r\n",
    "    model[n],valid_pred[n] ,train_pred[n], the_first_set_len = Stacking(model_item,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "    train_pred[n]=pd.DataFrame(train_pred[n],columns=[model_item[0]])\r\n",
    "    valid_pred[n]=pd.DataFrame(valid_pred[n],columns=[model_item[0]])\r\n",
    "    \r\n",
    "    scores=evaluate_model(model[n],train_X,train_y)\r\n",
    "    print(f'mean={np.mean(scores)}: {scores}')\r\n",
    "train_y = train_y.iloc[the_first_set_len:]      # remove the first set that can't be used in timeseries stacking\r\n",
    "train_pred=pd.concat(train_pred,axis=1)\r\n",
    "valid_pred=pd.concat(valid_pred,axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final estimation with the meta model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "meta_model.fit(train_pred,train_y)\r\n",
    "scores=evaluate_model(meta_model,valid_pred,valid_y)\r\n",
    "time.sleep(0.2)\r\n",
    "print('models', list(valid_pred.columns))\r\n",
    "print(f'mean={np.mean(scores)}: {scores}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rf, catb, knn, et - 0.88803\r\n",
    "## xgb, rf, catb, knn, et - 0.88720"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}