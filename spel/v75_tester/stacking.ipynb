{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learn v75 med walkthrough-metoden"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool,cv,utils \r\n",
    "from sklearn.impute import SimpleImputer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "source": [
    "### returnera en modell med parametrar satta\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    for c in cat_features:\r\n",
    "        # print(c)\r\n",
    "        X_train.loc[X_train[c].isna(),c] = 'Missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "        if X_test is not None:  ## om X_test är med\r\n",
    "            X_test.loc [X_test[c].isna(),c] = 'Missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "\r\n",
    "    return X_train,X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "df,alla_datum,_ = load_data() \r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.210009837088222"
      ]
     },
     "metadata": {},
     "execution_count": 384
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # Replace each value by the according smoothed mean\r\n",
    "    return df[by].map(smooth)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "# Hantera datum \r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "source": [
    "\r\n",
    "\r\n",
    "# Hantera Bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0\r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "source": [
    "# Hantera Häst och Kusk \r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "source": [
    "# Hantera kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        print('Felaktigt kön','h')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "        print('Felaktigt kön','s')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        print('Felaktigt kön','v')\r\n",
    "        error()\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "source": [
    "def transf_all(df):\r\n",
    "    trdf = df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=False) \r\n",
    "    trdf=transf_bana(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return trdf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "def impute_all_numerics(df):\r\n",
    "    columns=df.columns\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=columns)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "# trdf = df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=False) #\r\n",
    "trdf = impute_all_numerics(transf_all(df))\r\n",
    "if trdf.isna().sum().sum() != 0:\r\n",
    "    print('still NaNs')\r\n",
    "    error()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        df = df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=False) \r\n",
    "    \r\n",
    "        cat_features=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "        numeric_features=[item for item in df.columns if item not in cat_features]\r\n",
    "        print('alla¨cat-NaNs:',df[cat_features].isna().sum().sum())\r\n",
    "\r\n",
    "        df = impute_cat_features(df,cat_features=cat_features)\r\n",
    "        \r\n",
    "        return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "df_clean = catB_preprocess(df)\r\n",
    "\r\n",
    "df_clean[cat_features].isna().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alla¨cat-NaNs: 246\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60945"
      ]
     },
     "metadata": {},
     "execution_count": 399
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "# metrics\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CatBoost model \r\n",
    "# cat_features - fix NaNs\r\n",
    "# GridSearchCV some settings - comåpare with default"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XGBoost model \r\n",
    "# cat?\r\n",
    "# NaN's?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ANN model - Approx near neighbours\r\n",
    "# kör all preproc ovan\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# RandomForrest model \r\n",
    "# All preproc ovan?\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM  model\r\n",
    "# All preproc ovan?\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Walkthrough-funktionen  här"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "### Kör en walkthrough learn här, en datum i taget framåt\r\n",
    "\r\n",
    "# Jag har ändrat till att alla steg kör utan test-datam ed fast iterations=100\r\n",
    "def walkthrough(classic_test=False, verbose=False):\r\n",
    "    \r\n",
    "    df, nya_lopp, alla_datum, split_ix = get_alla_datum()\r\n",
    "\r\n",
    "    l2_leaf_regs=2\r\n",
    "    model=get_model(use_best=False,iterations=100)\r\n",
    "    df=remove_features(df.copy())\r\n",
    "    cat_features = list(df.loc[:,df.dtypes=='O'].columns)\r\n",
    "    df,_ = replace_NaN(df.copy(), cat_features=cat_features)    \r\n",
    "    print(f'cat_features {cat_features}\\n')\r\n",
    "\r\n",
    "    df['plac']=(df.plac==1)*1\r\n",
    "        \r\n",
    "    for nr,datum in enumerate(alla_datum[split_ix:]):\r\n",
    "        print(f'walk-iter {nr+1} av {len(alla_datum[split_ix:])} ',end=': ')\r\n",
    "\r\n",
    "        X_train = df.loc[df.datum<datum,:].copy()\r\n",
    "        y_train = X_train.plac; X_train.drop(['plac'],axis=1,inplace=True)\r\n",
    "\r\n",
    "        if classic_test:    ### klassisk train/test utan walkthrough\r\n",
    "            X_test  = df.loc[df.datum>=datum,:].copy()\r\n",
    "            y_test  = X_test.plac;  X_test.drop(['plac'],axis=1,inplace=True)\r\n",
    "            train_pool = Pool(X_train,y_train,cat_features=cat_features)\r\n",
    "            test_pool = Pool(X_test,y_test,cat_features=cat_features)\r\n",
    "            model.fit(train_pool,use_best_model=True, verbose=verbose,eval_set=test_pool)\r\n",
    "        else:\r\n",
    "            X_test  = df.loc[df.datum==datum,:].copy()\r\n",
    "            y_test  = X_test.plac;  X_test.drop(['plac'],axis=1,inplace=True)\r\n",
    "            train_pool = Pool(X_train,y_train,cat_features=cat_features)\r\n",
    "            test_pool = Pool(X_test,y_test,cat_features=cat_features)\r\n",
    "            model.fit(train_pool,use_best_model=False, verbose=verbose)\r\n",
    "\r\n",
    "        print('best iteration',model.get_best_iteration(), '\\tbest score', round(model.get_best_score()['learn']['Accuracy'],3) )\r\n",
    "        ##['validation']['Logloss'],3),'\\t', round(model.get_best_score()['validation']['Accuracy:use_weights=true'],3))\r\n",
    "        \r\n",
    "        if classic_test:    ### klassisk train/test utan walkthrough\r\n",
    "            return model,cat_features\r\n",
    "    \r\n",
    "        model.save_model('modeller/model_'+datum)\r\n",
    "\r\n",
    "    X_train =df.copy().drop('plac',axis=1)\r\n",
    "    y_train = df.plac \r\n",
    "    model.fit(X_train,y=y_train,cat_features=cat_features)\r\n",
    "    print(f'spara model_senaste',datum)\r\n",
    "    model.save_model('modeller/model_senaste')\r\n",
    "\r\n",
    "    return df,nya_lopp, model,cat_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Här körs hela walkthrough"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df, nya_lopp, model, cat_features = walkthrough(classic_test=False, verbose=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = get_model().load_model('modeller/model_senaste')\r\n",
    "df = pd.read_csv('all_data.csv')     \r\n",
    "# print(df.columns)\r\n",
    "df=remove_features(df.copy())\r\n",
    "cat_features = list(df.loc[:,df.dtypes=='O'].columns)\r\n",
    "df,_ = replace_NaN(df.copy(), cat_features=cat_features)    \r\n",
    "y=df.plac\r\n",
    "y=(y==1)*1\r\n",
    "df.drop('plac',axis=1,inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.columns[(df.dtypes=='object').values.tolist()]].info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## cv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "cv_pool = Pool(df,y,cat_features=cat_features)\r\n",
    "\r\n",
    "params = {\r\n",
    "         'use_best_model': True,\r\n",
    "         'eval_metric' : 'Recall',\r\n",
    "         \"loss_function\": \"Logloss\",\r\n",
    "         'early_stopping_rounds': 100,\r\n",
    "         'verbose': 50,\r\n",
    "}\r\n",
    "\r\n",
    "cv_score =cv(pool=cv_pool, \r\n",
    "   params=params, \r\n",
    "   dtrain=None, \r\n",
    "   iterations=2000, \r\n",
    "   num_boost_round=None,\r\n",
    "   fold_count=5, \r\n",
    "   nfold=None,\r\n",
    "   inverted=False,\r\n",
    "   partition_random_seed=0,\r\n",
    "   seed=2021, \r\n",
    "   shuffle=False, \r\n",
    "   logging_level=None, \r\n",
    "   stratified=True,\r\n",
    "   as_pandas=True,\r\n",
    "   type='TimeSeries')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_score[cv_score['test-Logloss-mean'].min() == cv_score['test-Logloss-mean']]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}