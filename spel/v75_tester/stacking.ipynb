{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try stacking for V75"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool,cv,utils \r\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### return a CatBoost model with some default parameters\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    for c in cat_features:\r\n",
    "        # print(c)\r\n",
    "        X_train.loc[X_train[c].isna(),c] = 'missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "        if X_test is not None:  ## om X_test är med\r\n",
    "            X_test.loc [X_test[c].isna(),c] = 'missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "    \r\n",
    "    return X_train, X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def remove_not_used_features(df):\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df,alla_datum,split_ix = load_data() \r\n",
    "df = remove_not_used_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.208773316093193"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # Replace each value by the according smoothed mean\r\n",
    "    return df[by].map(smooth)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Handle häst and kusk \r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön h'\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "       assert False, 'Felaktigt kön s'\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön v'\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        assert False\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## stacking prepare and run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "trdf,y=prepare_all(df)\r\n",
    "scorer = make_scorer(roc_auc_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500,1000],\r\n",
    "          'depth': [2,3,4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['AUC'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "                   estimator=<catboost.core.CatBoostClassifier object at 0x0000021A7D9DF250>,\n",
       "                   param_distributions={'depth': [2, 3, 4, 5, 6],\n",
       "                                        'eval_metric': ['AUC'],\n",
       "                                        'iterations': [50, 100, 500, 1000],\n",
       "                                        'l2_leaf_reg': array([1.00000000e-20, 3.16227766e-20, 1.00000000e-19]),\n",
       "                                        'leaf_estimation_iterations': [10],\n",
       "                                        'logging_level': ['Silent'],\n",
       "                                        'loss_function': ['Logloss'],\n",
       "                                        'random_seed': [2021]},\n",
       "                   scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best gridsearch',catb_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.7878977269333003\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'random_seed': 2021,\n",
       " 'loss_function': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'l2_leaf_reg': 3.162277660168379e-20,\n",
       " 'iterations': 1000,\n",
       " 'eval_metric': 'AUC',\n",
       " 'depth': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(30)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h5_ekipage</td>\n",
       "      <td>15.772651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h1_ekipage</td>\n",
       "      <td>14.772550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h4_ekipage</td>\n",
       "      <td>13.970035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2_ekipage</td>\n",
       "      <td>13.833793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h3_ekipage</td>\n",
       "      <td>13.367222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ekipage</td>\n",
       "      <td>10.177373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>streck</td>\n",
       "      <td>5.888591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>senast</td>\n",
       "      <td>1.286310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h2_kmtid</td>\n",
       "      <td>0.797085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kr</td>\n",
       "      <td>0.671566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>delta1</td>\n",
       "      <td>0.516046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h5_odds</td>\n",
       "      <td>0.472560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>datum</td>\n",
       "      <td>0.442835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>h1_perf</td>\n",
       "      <td>0.431642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h4_odds</td>\n",
       "      <td>0.413828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h1_kmtid</td>\n",
       "      <td>0.323562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h2_dist</td>\n",
       "      <td>0.316767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h3_perf</td>\n",
       "      <td>0.299258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spår</td>\n",
       "      <td>0.299131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h4_perf</td>\n",
       "      <td>0.296921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>delta3</td>\n",
       "      <td>0.295550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>delta4</td>\n",
       "      <td>0.294206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ålder</td>\n",
       "      <td>0.277426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>h5_pris</td>\n",
       "      <td>0.267612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>h5_dist</td>\n",
       "      <td>0.259613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h5_perf</td>\n",
       "      <td>0.257646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h3_pris</td>\n",
       "      <td>0.250942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>h2_perf</td>\n",
       "      <td>0.249741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>h1_dist</td>\n",
       "      <td>0.246085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h4_kmtid</td>\n",
       "      <td>0.226819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature Id  Importances\n",
       "0   h5_ekipage    15.772651\n",
       "1   h1_ekipage    14.772550\n",
       "2   h4_ekipage    13.970035\n",
       "3   h2_ekipage    13.833793\n",
       "4   h3_ekipage    13.367222\n",
       "5      ekipage    10.177373\n",
       "6       streck     5.888591\n",
       "7       senast     1.286310\n",
       "8     h2_kmtid     0.797085\n",
       "9           kr     0.671566\n",
       "10      delta1     0.516046\n",
       "11     h5_odds     0.472560\n",
       "12       datum     0.442835\n",
       "13     h1_perf     0.431642\n",
       "14     h4_odds     0.413828\n",
       "15    h1_kmtid     0.323562\n",
       "16     h2_dist     0.316767\n",
       "17     h3_perf     0.299258\n",
       "18        spår     0.299131\n",
       "19     h4_perf     0.296921\n",
       "20      delta3     0.295550\n",
       "21      delta4     0.294206\n",
       "22       ålder     0.277426\n",
       "23     h5_pris     0.267612\n",
       "24     h5_dist     0.259613\n",
       "25     h5_perf     0.257646\n",
       "26     h3_pris     0.250942\n",
       "27     h2_perf     0.249741\n",
       "28     h1_dist     0.246085\n",
       "29    h4_kmtid     0.226819"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# XGBoost model \r\n",
    "import xgboost as xgb\r\n",
    "label = y\r\n",
    "dtrain = xgb.DMatrix(trdf, label=label)\r\n",
    "param = {'max_depth':2, 'eta':1 }\r\n",
    "num_round = 10\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'nthread':[4], #when use hyperthread, xgboost may become slower\r\n",
    "              'objective':['binary:logistic'],\r\n",
    "              'learning_rate': [0.09,0.1,0.15], #so called `eta` value\r\n",
    "              'max_depth': [7,8,9],\r\n",
    "              'min_child_weight': [9,10,11],\r\n",
    "              'use_label_encoder':[False],\r\n",
    "            #   'silent': [1],\r\n",
    "              'eval_metric': ['logloss'],\r\n",
    "              'subsample': [0.5,0.9,1.0],\r\n",
    "              'colsample_bytree': [0.7, 0.9, 1.0],\r\n",
    "              'n_estimators': [7,8,9], #number of trees, change it to 1000 for better results\r\n",
    "              'missing':[-999],\r\n",
    "              'seed': [2021],\r\n",
    "              }\r\n",
    "\r\n",
    "xgb_clf = xgb.XGBClassifier(num_round=num_round)\r\n",
    "xgb_grid = GridSearchCV(estimator=xgb_clf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "xgb_grid.fit(trdf, y )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[23:22:50] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight...\n",
       "             n_jobs=3,\n",
       "             param_grid={'colsample_bytree': [0.7, 0.9, 1.0],\n",
       "                         'eval_metric': ['logloss'],\n",
       "                         'learning_rate': [0.09, 0.1, 0.15],\n",
       "                         'max_depth': [7, 8, 9],\n",
       "                         'min_child_weight': [9, 10, 11], 'missing': [-999],\n",
       "                         'n_estimators': [7, 8, 9], 'nthread': [4],\n",
       "                         'objective': ['binary:logistic'], 'seed': [2021],\n",
       "                         'subsample': [0.5, 0.9, 1.0],\n",
       "                         'use_label_encoder': [False]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# get best estimator and params\r\n",
    "best_xgb = xgb_grid.best_estimator_\r\n",
    "print('best gridsearch', xgb_grid.best_score_)\r\n",
    "best_param = xgb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.7553428398502139\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1.0,\n",
       " 'eval_metric': 'logloss',\n",
       " 'learning_rate': 0.15,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'missing': -999,\n",
       " 'n_estimators': 9,\n",
       " 'nthread': 4,\n",
       " 'objective': 'binary:logistic',\n",
       " 'seed': 2021,\n",
       " 'subsample': 1.0,\n",
       " 'use_label_encoder': False}"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ExtraTree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# ExtraTree  model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.tree import ExtraTreeClassifier\r\n",
    "et = ExtraTreeClassifier(min_samples_split=2, random_state=2021,class_weight='balanced')\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': [ 'balanced'  ,None], \r\n",
    "          'max_depth': [None, 5, 10  ,15 ,20],\r\n",
    "          'min_samples_leaf': [1, 2 ,3, 4,],\r\n",
    "          'min_samples_split': [2,30, 30,  40  ,45],   \r\n",
    "          'criterion': [ 'gini'  ,'entropy'],   \r\n",
    "          'splitter': ['random',  'best']\r\n",
    "         }\r\n",
    "\r\n",
    "et_grid = GridSearchCV(estimator=et, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "et_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=ExtraTreeClassifier(class_weight='balanced',\n",
       "                                           random_state=2021),\n",
       "             n_jobs=3,\n",
       "             param_grid={'class_weight': ['balanced', None],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [None, 5, 10, 15, 20],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4],\n",
       "                         'min_samples_split': [2, 30, 30, 40, 45],\n",
       "                         'splitter': ['random', 'best']},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# get best estimator and params\r\n",
    "best_et = et_grid.best_estimator_\r\n",
    "print('best gridsearch', et_grid.best_score_)\r\n",
    "best_param = et_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.7650589419071162\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'splitter': 'best'}"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ridge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ridge model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.linear_model import RidgeClassifier\r\n",
    "ridge = RidgeClassifier(normalize=False )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': ['balanced',None], \r\n",
    "          'alpha': [0.5,1.0,2.0 ],      \r\n",
    "         }\r\n",
    "\r\n",
    "ridge_grid = GridSearchCV(estimator=ridge, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "ridge_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "# get best estimator and params\r\n",
    "best_ridge = ridge_grid.best_estimator_\r\n",
    "print('best gridsearch', ridge_grid.best_score_)\r\n",
    "best_param = ridge_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.18166847877031872\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'class_weight': 'balanced'}"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# KNN model\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=3, )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_neighbors': [10,15,20],\r\n",
    "          \r\n",
    "         }\r\n",
    "\r\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "knn_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=KNeighborsClassifier(n_jobs=3), n_jobs=3,\n",
       "             param_grid={'n_neighbors': [10, 15, 20]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "best_knn = knn_grid.best_estimator_\r\n",
    "print('best gridsearch',knn_grid.best_score_)\r\n",
    "best_param = knn_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.5019462640497194\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_neighbors': 15}"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# GridSearch\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=RandomForestClassifier(), n_jobs=3,\n",
       "             param_grid={'class_weight': ['balanced'],\n",
       "                         'max_depth': [4, 5, 6, None],\n",
       "                         'n_estimators': [5, 10, 100], 'random_state': [2021]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.8209536438465564\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 2021}"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearchCV\r\n",
    "# from sklearn.svm import SVC\r\n",
    "# svc = SVC(C=1.0, gamma='scale', tol=0.001, cache_size=200, class_weight='balanced', random_state=2021)\r\n",
    "\r\n",
    "# tscv = TimeSeriesSplit()\r\n",
    "# params = {'C': [1,2,3],\r\n",
    "#           'gamma': ['scale','auto'],\r\n",
    "#           'class_weight': ['balanced'],\r\n",
    "#           'random_state': [2021],\r\n",
    "#          }\r\n",
    "\r\n",
    "# svc_grid = GridSearchCV(estimator=svc, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# svc_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get best estimator and params\r\n",
    "# best_svc = svc_grid.best_estimator_\r\n",
    "# print('best gridsearch', svc_grid.best_score_)\r\n",
    "# best_param = svc_grid.best_params_\r\n",
    "# best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('xgb',best_xgb,),\r\n",
    "               ('rf',best_rf,),\r\n",
    "               ('catb', best_catb,),\r\n",
    "              ('knn', best_knn, ),\r\n",
    "              ('et', best_et)              # ger mycket sämre res\r\n",
    "               # ('ridge', best_ridge, ) ,   # saknar predict_proba - usless!\r\n",
    "            #    ('svc', best_svc, ),\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV(class_weight='balanced')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def evaluate_model(model, X, y, scoring=scorer):\r\n",
    "    print('scorer =',scorer)\r\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=tscv, verbose=1, n_jobs=3, error_score='raise')\r\n",
    "    return scores\r\n",
    "\r\n",
    "def Stacking(model_item, X_tr, y_tr, X_final, n_fold):\r\n",
    "    model=model_item[1]\r\n",
    "    print(model_item[0], end=' ')\r\n",
    "    tscv = TimeSeriesSplit(n_splits=n_fold)\r\n",
    "    # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "    train_pred=np.empty((0,1),float)\r\n",
    "    for n, (train_indices, test_indices) in enumerate(tscv.split(X_tr)):\r\n",
    "        if n==0:\r\n",
    "            the_first_set_len = len(train_indices) # the first set that cannot be used i timeSeries stacking\r\n",
    "            \r\n",
    "        X_train, X_test = X_tr.iloc[train_indices], X_tr.iloc[test_indices]\r\n",
    "        y_train, y_test = y_tr.iloc[train_indices], y_tr.iloc[test_indices]\r\n",
    "        print(n,end=' ')\r\n",
    "        model.fit(X=X_train,y=y_train)\r\n",
    "        train_pred=np.append(train_pred,model.predict_proba(X_test)[:,1])\r\n",
    "    print(f'- final fit (the_first_set_len={the_first_set_len})' )\r\n",
    "    model.fit(X=X_tr,y=y_tr) # fit on all data (except the final data)   \r\n",
    "    valid_pred = model.predict_proba(X_final)[:,1]\r\n",
    "    return model,valid_pred.reshape(-1,1), train_pred, the_first_set_len\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "split_ix = int(len(trdf)*.8)\r\n",
    "train_X = trdf[trdf.index <  split_ix]\r\n",
    "valid_X = trdf[trdf.index >= split_ix]\r\n",
    "train_y = y[y.index <  split_ix]\r\n",
    "valid_y = y[y.index >=  split_ix]\r\n",
    "# test 2 models\r\n",
    "valid_pred=[None] * len(base_models)\r\n",
    "train_pred=[None] * len(base_models)\r\n",
    "model=[None] * len(base_models)\r\n",
    "for n, model_item in enumerate(base_models):\r\n",
    "    model[n],valid_pred[n] ,train_pred[n], the_first_set_len = Stacking(model_item,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "    train_pred[n]=pd.DataFrame(train_pred[n],columns=[model_item[0]])\r\n",
    "    valid_pred[n]=pd.DataFrame(valid_pred[n],columns=[model_item[0]])\r\n",
    "    \r\n",
    "    scores=evaluate_model(model[n],train_X,train_y)\r\n",
    "    print(f'mean={np.mean(scores)}: {scores}')\r\n",
    "train_y = train_y.iloc[the_first_set_len:]      # remove the first set that can't be used in timeseries stacking\r\n",
    "train_pred=pd.concat(train_pred,axis=1)\r\n",
    "valid_pred=pd.concat(valid_pred,axis=1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "xgb 0 [23:32:25] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "1 [23:32:25] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "2 [23:32:25] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "3 [23:32:26] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "4 [23:32:26] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "- final fit (the_first_set_len=5570)\n",
      "[23:32:26] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.7387945920838975: [0.68150788 0.74626719 0.73562024 0.74204201 0.78853565]\n",
      "rf 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.8128112934243298: [0.7856774  0.80706858 0.81982219 0.82257813 0.82891016]\n",
      "catb 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   20.8s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.7945946250953815: [0.75069027 0.77673468 0.82354298 0.78790108 0.83410412]\n",
      "knn 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.5023032279714801: [0.50494645 0.5038894  0.50277849 0.49990179 0.5       ]\n",
      "et 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.7557201976910901: [0.742866   0.72839845 0.75693172 0.77421062 0.77619419]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final estimation with the meta model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "import time\r\n",
    "meta_model.fit(train_pred,train_y)\r\n",
    "scores=evaluate_model(meta_model,valid_pred,valid_y)\r\n",
    "time.sleep(0.2)\r\n",
    "print('models', list(valid_pred.columns))\r\n",
    "print(f'mean={np.mean(scores)}: {scores}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "models ['xgb', 'rf', 'catb', 'knn', 'et']\n",
      "mean=0.8871988720787722: [0.85833371 0.90382673 0.89240001 0.8987632  0.88267071]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rf, catb, knn, et - 0.88803\r\n",
    "## xgb, rf, catb, knn, et - 0.88720"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}