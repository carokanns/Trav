{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try stacking for V75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from IPython.display import display \n",
    "\n",
    "from catboost import CatBoostClassifier,Pool, cv, utils \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree   import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***What to do***:\n",
    "- Kör en enkel modell utan 'streck'  \n",
    "- kolla om ekipage-nr gör någon skillnad  optimera m\n",
    "- kolla om 'avd' göt någon skillnad \n",
    "- skapa en flaml-ensemble utan streck\n",
    "- Se hu bra denna ensemble är jämfört med 'streck' inkluderat\n",
    "- Blir den lika bra som min pipe?\n",
    "- Kan  med och utan 'streck' användas tillsammans ?\n",
    "  - Man kan ha som innan på den första och proba-order eller f/insats som kriterium för den andra som ju bara skall hitta överraskningar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## byt ut alla NaN till text för cat_features\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\n",
    "    # print('cat_features',cat_features)\n",
    "    X_train[cat_features]=X_train[cat_features].fillna('missing')\n",
    "    if X_test is not None:  ## om X_test är med\n",
    "        X_test[cat_features]=X_test[cat_features].fillnal('missing')    ### byt ut None-värden till texten 'None\n",
    "\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\n",
    "def basic_data(df, NaN=True, frac=0.25):\n",
    "    X_train,X_test,y_train,y_test = None,None,None,None\n",
    "    dfnew = remove_features(df.copy())\n",
    "    dfnew['plac'] = (dfnew.plac==1) * 1\n",
    "    cat_features = list(dfnew.loc[:,df.dtypes=='O'].columns)\n",
    "    if NaN:\n",
    "        dfnew,_ = replace_NaN(dfnew.copy(), cat_features=cat_features)    \n",
    "    \n",
    "    alla_datum = df.datum.unique()\n",
    "    split_dat = alla_datum[int(len(alla_datum)* (1 - 0.25))]     # större än split_dat är test\n",
    "\n",
    "    X_train = dfnew.loc[dfnew.datum <= split_dat].copy()\n",
    "    y_train=X_train.plac\n",
    "    X_train.drop('plac',axis=1,inplace=True)\n",
    "    \n",
    "    X_test = dfnew.loc[dfnew.datum > split_dat].copy()\n",
    "    y_test=X_test.plac\n",
    "    X_test.drop('plac',axis=1,inplace=True)\n",
    "    \n",
    "    return X_train,X_test, y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df skall innehålla datum,avd,vodds\n",
    "def proba_order_score(df_, y,proba):\n",
    "    kassa=1000\n",
    "    df = df_.copy()\n",
    "    df['proba'] = proba[:,1]\n",
    "    df['f'] = (df.proba*df.vodds - 1) / (df.vodds-1)  # kelly formel\n",
    "    df['spela'] = df.f >0\n",
    "    df['insats'] = df.spela * df.f * kassa\n",
    "\n",
    "    df.sort_values(['datum','avd','proba'],ascending=[True,True,False],inplace=True)\n",
    "    proba_order=df.groupby(['datum','avd']).proba.cumcount()\n",
    "\n",
    "    df['prob_order']=proba_order+1\n",
    "    df['y'] = y\n",
    "    \n",
    "    return df, df.loc[df.y==1].prob_order.mean()   # mean prob_order för vinnarhäst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features som inte används vid träning\n",
    "def remove_features(df,remove_mer=[]):\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\n",
    "    if remove_mer:\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\n",
    "    \n",
    "    # df=check_unique(df.copy())\n",
    "    # df=check_corr(df.copy())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bygg min första riktiga Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a smooth mean value to the features in X_train  ##\n",
    "def calc_smooth_mean(X, y, by, m=100, tot_mean=None):\n",
    "    Xcopy = X.copy()\n",
    "    Xcopy[by] = Xcopy[by].str.lower()\n",
    "    Xcopy['plac'] = y\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = Xcopy.groupby(by)['plac'].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\n",
    "    \n",
    "    return smooth.to_dict()\n",
    "\n",
    "\n",
    "# transform model to stacking estimator\n",
    "class ModelTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(self.model.predict_proba(X))\n",
    "    \n",
    "def create_ekipage(df):\n",
    "    prefix = ['','h1_','h2_','h3_','h4_','h5_',]\n",
    "    ekipage=[]\n",
    "    for pref in prefix:\n",
    "        df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\n",
    "        ekipage.append(pref+'ekipage')\n",
    "        df.drop([pref+'kusk'],axis=1,inplace=True)\n",
    "        \n",
    "    df.drop(['häst'],axis=1,inplace=True )   \n",
    "        \n",
    "    return df, ekipage   \n",
    "    \n",
    "# Handle ekipage (häst and kusk)\n",
    "class CustomSmoothMean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,cols,plac='plac', m=100):\n",
    "        super().__init__()\n",
    "        self.map = {}\n",
    "        self.total_mean=None\n",
    "        self.cols = cols    \n",
    "        self.plac=plac\n",
    "        self.m=m\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # print('CustSmooth fit start')\n",
    "        self.total_mean=y.mean()\n",
    "        self.map = calc_smooth_mean(X, y, by='ekipage', m=self.m, tot_mean=self.total_mean)\n",
    "        self.map['missing'] = 0\n",
    "\n",
    "        display(f'using m={self.m}')\n",
    "        # print('CustSmooth fit end')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('CustSmooth transform start')   \n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].str.lower()\n",
    "            X[col] = X[col].map(self.map)\n",
    "            X[col].fillna(0,inplace=True)\n",
    "            \n",
    "        # display(X.isna().sum())    \n",
    "        # print('CustSmooth transform end')    \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.cols, self.plac, self.m, self.total_mean,self.map    \n",
    "####\n",
    "\n",
    "def set_lower(dfo):\n",
    "    # print('set_lower start')\n",
    "    df=dfo.copy()\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].str.lower()\n",
    "    # print('set_lower end')    \n",
    "    return df\n",
    "\n",
    "lower =  FunctionTransformer(set_lower)\n",
    "\n",
    "def datum_to_num(df):\n",
    "    dfc=df.copy()\n",
    "    dfc['datum'] = pd.to_datetime(df.datum).view(float)*10e210\n",
    "    return dfc\n",
    "\n",
    "# Handle ekipage (häst and kusk)\n",
    "class transf_bana(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,banor):\n",
    "        super().__init__()\n",
    "        self.map = {}\n",
    "        self.banor = banor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # print('fit bana start')\n",
    "        huvud_bana=self.banor[0]\n",
    "        \n",
    "        self.map = X[huvud_bana].str.lower().value_counts() \n",
    "        self.map[None] = 0\n",
    "        self.map['missing'] = 0\n",
    "        # print('fit bana end')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('transform bana',X.shape)\n",
    "        for bana in self.banor:\n",
    "            X[bana] = X[bana].fillna('missing-1')\n",
    "            X[bana] = X[bana].str.lower()\n",
    "            X[bana] = [item[0] for item in X[bana].str.split('-')]  # remove '-10' from 'solvalla-10' etc\n",
    "            # print('transform bana effter split',bana)\n",
    "            # print(X[bana][:10])\n",
    "            X[bana] = X[bana].map(self.map)\n",
    "            X[bana] = X[bana].fillna(0)\n",
    "            # print('transform bana innan return')\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.map, self.banor\n",
    "    \n",
    "####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Själva pipen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using m=100'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dforg = pd.read_csv('..\\\\all_data.csv')     \n",
    "dforg,ekipage = create_ekipage(dforg.copy())      # alla ekipage, h1_ekipage-h5_ekipage\n",
    "X_train,X_test,y_train,y_test = basic_data(dforg.copy())\n",
    "cat_features = list(X_train.loc[:,X_train.dtypes=='O'].columns)\n",
    "\n",
    "\n",
    "streck=True\n",
    "if not streck:\n",
    "    X_train.drop('streck',axis=1,inplace=True)\n",
    "    X_test.drop('streck',axis=1,inplace=True)\n",
    "\n",
    "könen=['kön1','kön2','kön3']\n",
    "banor = ['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]\n",
    "all_nums = list(X_train.select_dtypes('number').columns)\n",
    "\n",
    "lower_and_ohe = Pipeline([\n",
    "        ('lower', lower),\n",
    "        ('encode', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "############ Parms ###############################\n",
    "rf_parms={ 'n_jobs': 5,'class_weight': 'balanced'}\n",
    "etr_parms={'n_jobs': 5,'class_weight': 'balanced'}\n",
    "cb_parms={'iterations': 200,'early_stopping_rounds': 50,'auto_class_weights': 'Balanced','verbose': False}\n",
    "final_parms={'n_jobs': 5,'class_weight': 'balanced'}\n",
    "################################################### \n",
    "\n",
    "col_pipe = make_column_transformer(\n",
    "                        (transf_bana(banor), banor),\n",
    "                        (FunctionTransformer(datum_to_num), ['datum']),\n",
    "                        (lower_and_ohe, ['kön']), \n",
    "                        (CustomSmoothMean(ekipage,m=100), ekipage),\n",
    "                        (SimpleImputer(strategy='constant', fill_value=-1),all_nums  ),\n",
    "                        remainder='passthrough',\n",
    "                        )\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('transformers', col_pipe ),\n",
    "    \n",
    "    ('estimators', FeatureUnion([\n",
    "        ('knn', ModelTransformer(KNeighborsClassifier(n_neighbors=5, n_jobs=4))),\n",
    "        ('cat', ModelTransformer(CatBoostClassifier(**cb_parms))),\n",
    "        ('dtr', ModelTransformer(DecisionTreeClassifier(class_weight='balanced'))),\n",
    "        ('etr', ModelTransformer(ExtraTreesClassifier(**etr_parms))),\n",
    "        ('rf', ModelTransformer(RandomForestClassifier(**rf_parms))),  \n",
    "    ])),\n",
    "    ('estimator', RandomForestClassifier(**final_parms)),\n",
    "])\n",
    "\n",
    "\n",
    "with_streck = pipe.fit(X_train,y=y_train)\n",
    "# no_streck = pipe.fit(X_train_nostr,y=y_train)\n",
    "# the_stack = pipe.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jämför olika modellers score (med och utan 'streck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc train 1.0\n",
      "auc test 0.5184634774239365\n",
      "prob_score test 6.272321428571429 0.5184634774239365\n",
      "prob_score train 1.0022066936373666 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score,mean_absolute_error,accuracy_score,matthews_corrcoef,f1_score\n",
    "X_train,X_test,y_train,y_test = basic_data(dforg)\n",
    "\n",
    "if not streck:\n",
    "    print(\"utan 'streck'\")\n",
    "train_pred= pipe.predict_proba(X_train)\n",
    "print('auc train',roc_auc_score(y_train, train_pred[:,1]))\n",
    "test_pred = pipe.predict_proba(X_test)\n",
    "print('auc test', roc_auc_score(y_test,test_pred[:,1]))\n",
    "# print('mae test', mean_absolute_error(y_test,test_pred[:,1]))\n",
    "# po=proba_ordning(X_test.copy(),y_test,df.copy(),test_pred )[['datum','avd','proba','prob_order','vann']]\n",
    "# print('mean proba-order för vunna',po.loc[po.vann==1].prob_order.mean())\n",
    "\n",
    "X_test[['datum','avd','vodds']] = dforg[['datum','avd','vodds']]\n",
    "_,prob_score = proba_order_score(X_test,y_test,test_pred)\n",
    "print('prob_score test',prob_score,roc_auc_score(y_test,test_pred[:,1]))\n",
    "X_train[['datum','avd','vodds']] = dforg[['datum','avd','vodds']]\n",
    "_,prob_score = proba_order_score(X_train,y_train,train_pred)\n",
    "print('prob_score train',prob_score,roc_auc_score(y_train,train_pred[:,1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klart överinlärd - prova en gridsearch om det går att nå estimators parametrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jämför med CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using m=100'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.084837\n",
      "0:\tlearn: 0.5977540\ttotal: 14.9ms\tremaining: 7.42s\n",
      "100:\tlearn: 0.2203682\ttotal: 1.61s\tremaining: 6.36s\n",
      "200:\tlearn: 0.1781405\ttotal: 3.19s\tremaining: 4.74s\n",
      "300:\tlearn: 0.1464653\ttotal: 4.67s\tremaining: 3.09s\n",
      "400:\tlearn: 0.1228111\ttotal: 5.92s\tremaining: 1.46s\n",
      "499:\tlearn: 0.1037990\ttotal: 7.14s\tremaining: 0us\n",
      "mae 0.11940676036559213\n",
      "prob_score 4.234375 0.5184634774239365\n"
     ]
    }
   ],
   "source": [
    "#Jämfr CatBoost\n",
    "\n",
    "X_train,X_test,y_train,y_test = basic_data(dforg)\n",
    "\n",
    "cb = CatBoostClassifier(iterations=500,  early_stopping_rounds=100,auto_class_weights='Balanced',verbose=100)\n",
    "cb_pipe = make_pipeline(col_pipe, cb)\n",
    "cb_pipe.fit(X_train,y_train)\n",
    "cb_train_pred= cb_pipe.predict_proba(X_train)\n",
    "cb_test_pred =cb_pipe.predict_proba(X_test)\n",
    "\n",
    "if not streck:\n",
    "    print(\"utan 'streck'\")\n",
    "# print('auc', roc_auc_score(y_test,cb_test_pred[:,1]))\n",
    "print('mae', mean_absolute_error(y_test,cb_test_pred[:,1]))\n",
    "# po=proba_ordning(X_test.copy(),y_test,df.copy(),cb_test_pred )[['datum','avd','proba','prob_order','vann']]\n",
    "# print('cb mean vann ',po.loc[po.vann==1].prob_order.mean())\n",
    "\n",
    "X_test[['datum','avd','vodds']] = dforg[['datum','avd','vodds']]\n",
    "_,prob_score = proba_order_score(X_test,y_test,cb_test_pred)\n",
    "\n",
    "print('prob_score',prob_score,roc_auc_score(y_test,test_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n",
      "C:\\Users\\peter\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 09-25 01:53:46] {1541} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "# FLAML med samma transformers som alla andra och helt utan transformers (raw)\n",
    "temp_pipe = make_column_transformer(\n",
    "                        (transf_bana(banor), banor),\n",
    "                        (FunctionTransformer(datum_to_num), ['datum']),\n",
    "                        (lower_and_ohe, ['kön']), \n",
    "                        # (CustomSmCustomSmoothMean(ekipage,m=100), ekipage),\n",
    "                        (SimpleImputer(strategy='constant', fill_value=-1),all_nums  ),\n",
    "                        remainder='passthrough',\n",
    "                        )\n",
    "\n",
    "from flaml import AutoML \n",
    "automl, automl_raw = None, None\n",
    "X_train,X_test,y_train,y_test = basic_data(dforg.copy())\n",
    "# # X_train.drop('streck',axis=1,inplace=True)\n",
    "# # X_test.drop('streck',axis=1,inplace=True)\n",
    "# # X_train.drop(['avd'],axis=1,inplace=True)\n",
    "# # X_test.drop(['avd'],axis=1,inplace=True)\n",
    "# cat_features = list(X_train.loc[:,X_train.dtypes=='O'].columns)\n",
    "\n",
    "flml_parms= {'automl__task': 'classification',  'automl__verbose': False, #'automl__X_val': X_test, 'automl__y_val':y_test,\n",
    "             'automl__split_type':'time', 'automl__metric': 'roc_auc', 'automl__time_budget':320, \n",
    "             'automl__max_iter':20000000, 'automl__n_jobs':5,'automl__seed':2021, 'automl__early_stop':True, 'automl__ensemble':True}\n",
    "\n",
    "automl = AutoML( )\n",
    "flm_pipe = make_pipeline(temp_pipe, automl)\n",
    "flm=flm_pipe.fit(X_train,y_train, **flml_parms)\n",
    "\n",
    "if True: # raw - utan transformed data\n",
    "    flml_raw_parms={'task': 'classification','split_type':None, 'verbose': 1, 'metric':'roc_auc', 'verbose':False,\n",
    "            'time_budget':500, 'max_iter':20000000,'n_jobs':5, 'X_val': X_test, 'y_val':y_test,'early_stop':True, 'ensemble':True}\n",
    "\n",
    "    automl_raw = AutoML()\n",
    "    automl_raw.fit(X_train,y_train, **flml_raw_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw prob_score och auc 3.2020089285714284 0.8131355942102115\n",
      "flm prob_score och auc 3.1328125 0.8166181926067131\n"
     ]
    }
   ],
   "source": [
    "# print('indata flm',  len(banor+['datum']+könen+ekipage+all_nums))\n",
    "\n",
    "if not streck:\n",
    "    print(\"utan 'streck'\")\n",
    "X_train,X_test,y_train,y_test = basic_data(dforg.copy())\n",
    "\n",
    "if True :   # raw - ej transformed data\n",
    "    flm_raw_train_pred= automl_raw.predict_proba(X_train)\n",
    "    flm_raw_test_pred = automl_raw.predict_proba(X_test)\n",
    "    \n",
    "    X_test_raw = X_test.copy()\n",
    "    X_test_raw[['datum','avd','vodds']] = dforg[['datum','avd','vodds']]\n",
    "    _,prob_score = proba_order_score(X_test_raw,y_test, flm_raw_test_pred)\n",
    "\n",
    "    print('raw prob_score och auc', prob_score,roc_auc_score(y_test,flm_raw_test_pred[:,1]))\n",
    "    \n",
    "if True:  # med transformde data\n",
    "    flm_train_pred= flm_pipe.predict_proba(X_train)\n",
    "    flm_test_pred = flm_pipe.predict_proba(X_test)\n",
    "    # print('flm test auc',roc_auc_score(y_test, flm_test_pred[:,1]))\n",
    "    # print('flm train',roc_auc_score(y_train, flm_train_pred[:,1]))\n",
    "    # po=proba_ordning(X_test.copy(),y_test,df.copy(),flm_test_pred )[['datum','avd','proba','prob_order','vann']]\n",
    "    # print('flm test mean vann',po.loc[po.vann==1].prob_order.mean())\n",
    "    \n",
    "    X_test[['datum','avd','vodds']] = dforg[['datum','avd','vodds']]\n",
    "    _,prob_score = proba_order_score(X_test,y_test, flm_test_pred)\n",
    "\n",
    "    print('flm prob_score och auc',prob_score,roc_auc_score(y_test,flm_test_pred[:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using m=100'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.08554158456740465, 0.08554158456740465, 0.08554158456740465,\n",
       "       0.08554158456740465, 0.08554158456740465, 0.0, '2014-12-28',\n",
       "       'ÖREBRO', 5.0, 21018.0, 6.0, 2100.0, 2100.0, 0, 6, 'v', 125000.0,\n",
       "       'Eskilstuna', 3.0, 2.0, 35.0, 3.92, 16.8, 'Eskilstuna', 3.0, 1.0,\n",
       "       30.0, 3.7, 14.9, 'Eskilstuna', 3.0, 15.0, 125.0, 52.42, 14.3,\n",
       "       'Solvalla', 3.0, 15.0, 70.0, 5.2, 13.9, 'Örebro', 3.0, 15.0, 25.0,\n",
       "       2.2, 12.3, 2140.0, 2140.0, 2640.0, 2140.0, 1609.0, 1, 1, 1, 1, 1,\n",
       "       3935.030968151612, 6006.507181794034, 11.180339887498944,\n",
       "       8.366600265340756, 5.0, 21.0, 19.0, 17.0, 10.0, 18.0], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flm_pipe['columntransformer'].fit_transform(X_train,y_train)[0]\n",
    "temp_pipe = make_column_transformer(\n",
    "                        # (transf_bana(banor), banor),\n",
    "                        # (FunctionTransformer(datum_to_num), ['datum']),\n",
    "                        # (lower_and_ohe, ['kön']), \n",
    "                        (CustomSmoothMean(ekipage,m=100), ekipage),\n",
    "                        # (SimpleImputer(strategy='constant', fill_value=-1),all_nums  ),\n",
    "                        remainder='passthrough',\n",
    "                        )\n",
    "temp_pipe.fit_transform(X_train,y_train)[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
