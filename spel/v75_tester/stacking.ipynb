{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try stacking for V75"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool,cv,utils \r\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "### return a CatBoost model with some default parameters\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    for c in cat_features:\r\n",
    "        # print(c)\r\n",
    "        X_train.loc[X_train[c].isna(),c] = 'missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "        if X_test is not None:  ## om X_test är med\r\n",
    "            X_test.loc [X_test[c].isna(),c] = 'missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "    \r\n",
    "    return X_train, X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "def remove_not_used_features(df):\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "source": [
    "df,alla_datum,split_ix = load_data() \r\n",
    "df = remove_not_used_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.208773316093193"
      ]
     },
     "metadata": {},
     "execution_count": 283
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # Replace each value by the according smoothed mean\r\n",
    "    return df[by].map(smooth)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "# Handle häst and kusk \r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön h'\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "       assert False, 'Felaktigt kön s'\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        assert False, 'Felaktigt kön v'\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        assert False\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "source": [
    "\r\n",
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "source": [
    "trdf,y=prepare_all(df)\r\n",
    "scorer = make_scorer(roc_auc_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500,1000],\r\n",
    "          'depth': [2,3,4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['AUC'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training has stopped (degenerate solution on iteration 955, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "                   estimator=<catboost.core.CatBoostClassifier object at 0x000001C054585FA0>,\n",
       "                   param_distributions={'depth': [2, 3, 4, 5, 6],\n",
       "                                        'eval_metric': ['AUC'],\n",
       "                                        'iterations': [50, 100, 500, 1000],\n",
       "                                        'l2_leaf_reg': array([1.00000000e-20, 3.16227766e-20, 1.00000000e-19]),\n",
       "                                        'leaf_estimation_iterations': [10],\n",
       "                                        'logging_level': ['Silent'],\n",
       "                                        'loss_function': ['Logloss'],\n",
       "                                        'random_seed': [2021]},\n",
       "                   scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 384
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best gridsearch',catb_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.791797336182684\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'random_seed': 2021,\n",
       " 'loss_function': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'l2_leaf_reg': 1e-19,\n",
       " 'iterations': 1000,\n",
       " 'eval_metric': 'AUC',\n",
       " 'depth': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 385
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "# print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(30)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h5_ekipage</td>\n",
       "      <td>16.040404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h1_ekipage</td>\n",
       "      <td>15.294437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h3_ekipage</td>\n",
       "      <td>14.553334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2_ekipage</td>\n",
       "      <td>13.336762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h4_ekipage</td>\n",
       "      <td>12.911046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ekipage</td>\n",
       "      <td>10.813110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>streck</td>\n",
       "      <td>5.620566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h2_kmtid</td>\n",
       "      <td>0.742316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kr</td>\n",
       "      <td>0.683617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>senast</td>\n",
       "      <td>0.672007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>datum</td>\n",
       "      <td>0.640795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h1_kmtid</td>\n",
       "      <td>0.441425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>delta3</td>\n",
       "      <td>0.434333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spår</td>\n",
       "      <td>0.409016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h5_perf</td>\n",
       "      <td>0.373597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h3_perf</td>\n",
       "      <td>0.330297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h3_pris</td>\n",
       "      <td>0.317592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h1_perf</td>\n",
       "      <td>0.313503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>h5_odds</td>\n",
       "      <td>0.310660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h4_odds</td>\n",
       "      <td>0.291530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h2_dist</td>\n",
       "      <td>0.290753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>h1_dist</td>\n",
       "      <td>0.282770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>delta2</td>\n",
       "      <td>0.271360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>h2_perf</td>\n",
       "      <td>0.268476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>delta1</td>\n",
       "      <td>0.256879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h3_odds</td>\n",
       "      <td>0.250101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h2_odds</td>\n",
       "      <td>0.242797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ålder</td>\n",
       "      <td>0.230499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>delta4</td>\n",
       "      <td>0.218990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h4_pris</td>\n",
       "      <td>0.208996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature Id  Importances\n",
       "0   h5_ekipage    16.040404\n",
       "1   h1_ekipage    15.294437\n",
       "2   h3_ekipage    14.553334\n",
       "3   h2_ekipage    13.336762\n",
       "4   h4_ekipage    12.911046\n",
       "5      ekipage    10.813110\n",
       "6       streck     5.620566\n",
       "7     h2_kmtid     0.742316\n",
       "8           kr     0.683617\n",
       "9       senast     0.672007\n",
       "10       datum     0.640795\n",
       "11    h1_kmtid     0.441425\n",
       "12      delta3     0.434333\n",
       "13        spår     0.409016\n",
       "14     h5_perf     0.373597\n",
       "15     h3_perf     0.330297\n",
       "16     h3_pris     0.317592\n",
       "17     h1_perf     0.313503\n",
       "18     h5_odds     0.310660\n",
       "19     h4_odds     0.291530\n",
       "20     h2_dist     0.290753\n",
       "21     h1_dist     0.282770\n",
       "22      delta2     0.271360\n",
       "23     h2_perf     0.268476\n",
       "24      delta1     0.256879\n",
       "25     h3_odds     0.250101\n",
       "26     h2_odds     0.242797\n",
       "27       ålder     0.230499\n",
       "28      delta4     0.218990\n",
       "29     h4_pris     0.208996"
      ]
     },
     "metadata": {},
     "execution_count": 386
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## other models that need no NaN and no Categorical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "# XGBoost model \r\n",
    "# \r\n",
    "# NaN's?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ExtraTree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "# ExtraTree  model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.tree import ExtraTreeClassifier\r\n",
    "et = ExtraTreeClassifier(min_samples_split=2, random_state=2021,class_weight='balanced')\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': [ 'balanced'  ,None], \r\n",
    "          'max_depth': [None, 5, 10  ,15 ,20],\r\n",
    "          'min_samples_leaf': [1, 2 ,3, 4,],\r\n",
    "          'min_samples_split': [2,30, 30,  40  ,45],   \r\n",
    "          'criterion': [ 'gini'  ,'entropy'],   \r\n",
    "          'splitter': ['random',  'best']\r\n",
    "         }\r\n",
    "\r\n",
    "et_grid = GridSearchCV(estimator=et, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "et_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=ExtraTreeClassifier(class_weight='balanced',\n",
       "                                           random_state=2021),\n",
       "             n_jobs=3,\n",
       "             param_grid={'class_weight': ['balanced', None],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [None, 5, 10, 15, 20],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4],\n",
       "                         'min_samples_split': [2, 30, 30, 40, 45],\n",
       "                         'splitter': ['random', 'best']},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 387
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "# get best estimator and params\r\n",
    "best_et = et_grid.best_estimator_\r\n",
    "print('best gridsearch', et_grid.best_score_)\r\n",
    "best_param = et_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.7650589419071162\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'splitter': 'best'}"
      ]
     },
     "metadata": {},
     "execution_count": 388
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ridge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ridge model\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "from sklearn.linear_model import RidgeClassifier\r\n",
    "ridge = RidgeClassifier(normalize=False )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "params = {'class_weight': ['balanced',None], \r\n",
    "          'alpha': [0.5,1.0,2.0 ],      \r\n",
    "         }\r\n",
    "\r\n",
    "ridge_grid = GridSearchCV(estimator=ridge, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "ridge_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "# get best estimator and params\r\n",
    "best_ridge = ridge_grid.best_estimator_\r\n",
    "print('best gridsearch', ridge_grid.best_score_)\r\n",
    "best_param = ridge_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.18166847877031872\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'class_weight': 'balanced'}"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "source": [
    "# KNN model\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=3, )\r\n",
    "\r\n",
    "# GridSearchCV\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_neighbors': [10,15,20],\r\n",
    "          \r\n",
    "         }\r\n",
    "\r\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "knn_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=KNeighborsClassifier(n_jobs=3), n_jobs=3,\n",
       "             param_grid={'n_neighbors': [10, 15, 20]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 392
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "best_knn = knn_grid.best_estimator_\r\n",
    "print('best gridsearch',knn_grid.best_score_)\r\n",
    "best_param = knn_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.5019462640497194\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_neighbors': 15}"
      ]
     },
     "metadata": {},
     "execution_count": 393
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "# GridSearch\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=RandomForestClassifier(), n_jobs=3,\n",
       "             param_grid={'class_weight': ['balanced'],\n",
       "                         'max_depth': [4, 5, 6, None],\n",
       "                         'n_estimators': [5, 10, 100], 'random_state': [2021]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "metadata": {},
     "execution_count": 394
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "source": [
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.8209536438465564\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 2021}"
      ]
     },
     "metadata": {},
     "execution_count": 395
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h2_ekipage</th>\n",
       "      <td>0.170345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_ekipage</th>\n",
       "      <td>0.142896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_ekipage</th>\n",
       "      <td>0.137229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streck</th>\n",
       "      <td>0.130473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h5_ekipage</th>\n",
       "      <td>0.128405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kön_v</th>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4_auto</th>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2_auto</th>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_auto</th>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_auto</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance\n",
       "h2_ekipage    0.170345\n",
       "h3_ekipage    0.142896\n",
       "h1_ekipage    0.137229\n",
       "streck        0.130473\n",
       "h5_ekipage    0.128405\n",
       "...                ...\n",
       "kön_v         0.000089\n",
       "h4_auto       0.000078\n",
       "h2_auto       0.000068\n",
       "h3_auto       0.000029\n",
       "h1_auto       0.000025\n",
       "\n",
       "[63 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 396
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GridSearchCV\r\n",
    "# from sklearn.svm import SVC\r\n",
    "# svc = SVC(C=1.0, gamma='scale', tol=0.001, cache_size=200, class_weight='balanced', random_state=2021)\r\n",
    "\r\n",
    "# tscv = TimeSeriesSplit()\r\n",
    "# params = {'C': [1,2,3],\r\n",
    "#           'gamma': ['scale','auto'],\r\n",
    "#           'class_weight': ['balanced'],\r\n",
    "#           'random_state': [2021],\r\n",
    "#          }\r\n",
    "\r\n",
    "# svc_grid = GridSearchCV(estimator=svc, param_grid=params, n_jobs=3,scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# svc_grid.fit(trdf, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get best estimator and params\r\n",
    "# best_svc = svc_grid.best_estimator_\r\n",
    "# print('best gridsearch', svc_grid.best_score_)\r\n",
    "# best_param = svc_grid.best_params_\r\n",
    "# best_param"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('rf',best_rf,),\r\n",
    "               ('catb', best_catb,),\r\n",
    "              ('knn', best_knn, ),\r\n",
    "              ('et', best_et)              # ger mycket sämre res\r\n",
    "               # ('ridge', best_ridge, ) ,   # saknar predict_proba - usless!\r\n",
    "            #    ('svc', best_svc, ),\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV(class_weight='balanced')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "source": [
    "def evaluate_model(model, X, y, scoring=scorer):\r\n",
    "    print('scorer =',scorer)\r\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=tscv, verbose=1, n_jobs=3, error_score='raise')\r\n",
    "    return scores\r\n",
    "\r\n",
    "def Stacking(model_item, X_tr, y_tr, X_final, n_fold):\r\n",
    "    model=model_item[1]\r\n",
    "    print(model_item[0], end=' ')\r\n",
    "    tscv = TimeSeriesSplit(n_splits=n_fold)\r\n",
    "    # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "    train_pred=np.empty((0,1),float)\r\n",
    "    for n, (train_indices, test_indices) in enumerate(tscv.split(X_tr)):\r\n",
    "        if n==0:\r\n",
    "            the_first_set_len = len(train_indices) # the first set that cannot be used i timeSeries stacking\r\n",
    "            \r\n",
    "        X_train, X_test = X_tr.iloc[train_indices], X_tr.iloc[test_indices]\r\n",
    "        y_train, y_test = y_tr.iloc[train_indices], y_tr.iloc[test_indices]\r\n",
    "        print(n,end=' ')\r\n",
    "        model.fit(X=X_train,y=y_train)\r\n",
    "        train_pred=np.append(train_pred,model.predict_proba(X_test)[:,1])\r\n",
    "    print(f'- final fit (the_first_set_len={the_first_set_len})' )\r\n",
    "    model.fit(X=X_tr,y=y_tr) # fit on all data (except the final data)   \r\n",
    "    valid_pred = model.predict_proba(X_final)[:,1]\r\n",
    "    return model,valid_pred.reshape(-1,1), train_pred, the_first_set_len\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "source": [
    "split_ix = int(len(trdf)*.8)\r\n",
    "train_X = trdf[trdf.index <  split_ix]\r\n",
    "valid_X = trdf[trdf.index >= split_ix]\r\n",
    "train_y = y[y.index <  split_ix]\r\n",
    "valid_y = y[y.index >=  split_ix]\r\n",
    "# test 2 models\r\n",
    "valid_pred=[None] * len(base_models)\r\n",
    "train_pred=[None] * len(base_models)\r\n",
    "model=[None] * len(base_models)\r\n",
    "for n, model_item in enumerate(base_models):\r\n",
    "    model[n],valid_pred[n] ,train_pred[n], the_first_set_len = Stacking(model_item,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "    train_pred[n]=pd.DataFrame(train_pred[n],columns=[model_item[0]])\r\n",
    "    valid_pred[n]=pd.DataFrame(valid_pred[n],columns=[model_item[0]])\r\n",
    "    \r\n",
    "    scores=evaluate_model(model[n],train_X,train_y)\r\n",
    "    print(f'mean={np.mean(scores)}: {scores}')\r\n",
    "train_y = train_y.iloc[the_first_set_len:]      # remove the first set that can't be used in timeseries stacking\r\n",
    "train_pred=pd.concat(train_pred,axis=1)\r\n",
    "valid_pred=pd.concat(valid_pred,axis=1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rf 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.8128112934243298: [0.7856774  0.80706858 0.81982219 0.82257813 0.82891016]\n",
      "catb 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   18.8s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.7904074799942405: [0.7492492  0.76797355 0.81016038 0.78914572 0.83550854]\n",
      "knn 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.5023032279714801: [0.50494645 0.5038894  0.50277849 0.49990179 0.5       ]\n",
      "et 0 1 2 3 4 - final fit (the_first_set_len=5570)\n",
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean=0.7557201976910901: [0.742866   0.72839845 0.75693172 0.77421062 0.77619419]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final estimation with the meta model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "source": [
    "import time\r\n",
    "meta_model.fit(train_pred,train_y)\r\n",
    "scores=evaluate_model(meta_model,valid_pred,valid_y)\r\n",
    "time.sleep(0.2)\r\n",
    "print('models', list(valid_pred.columns))\r\n",
    "print(f'mean={np.mean(scores)}: {scores}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scorer = make_scorer(roc_auc_score)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "models ['rf', 'catb', 'knn', 'et']\n",
      "mean=0.8880260020622532: [0.85459812 0.90056573 0.88886505 0.89868276 0.89741836]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "rf, catb, knn, et - 0.88803"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}