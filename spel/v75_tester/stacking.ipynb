{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try stacking for V75"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool,cv,utils \r\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### return a CatBoost model with some default parameters\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    for c in cat_features:\r\n",
    "        # print(c)\r\n",
    "        X_train.loc[X_train[c].isna(),c] = 'missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "        if X_test is not None:  ## om X_test är med\r\n",
    "            X_test.loc [X_test[c].isna(),c] = 'missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "\r\n",
    "    return X_train,X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def remove_not_used_features(df):\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df,alla_datum,_ = load_data() \r\n",
    "df = remove_not_used_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.210009837088222"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # Replace each value by the according smoothed mean\r\n",
    "    return df[by].map(smooth)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Handle häst and kusk \r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        print('Felaktigt kön','h')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "        print('Felaktigt kön','s')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        print('Felaktigt kön','v')\r\n",
    "        error()\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        error()\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "\r\n",
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "trdf,y=prepare_all(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500],\r\n",
    "          'depth': [4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['F1'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "scorer = make_scorer(matthews_corrcoef)\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "                   estimator=<catboost.core.CatBoostClassifier object at 0x000001B547641FD0>,\n",
       "                   param_distributions={'depth': [4, 5, 6],\n",
       "                                        'eval_metric': ['F1'],\n",
       "                                        'iterations': [50, 100, 500],\n",
       "                                        'l2_leaf_reg': array([1.00000000e-20, 3.16227766e-20, 1.00000000e-19]),\n",
       "                                        'leaf_estimation_iterations': [10],\n",
       "                                        'logging_level': ['Silent'],\n",
       "                                        'loss_function': ['Logloss'],\n",
       "                                        'random_seed': [2021]},\n",
       "                   scoring=make_scorer(matthews_corrcoef))"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best Logloss gridsearch',clf_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best Logloss gridsearch 0.1766571257827087\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'random_seed': 2021,\n",
       " 'loss_function': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'l2_leaf_reg': 1e-20,\n",
       " 'iterations': 500,\n",
       " 'eval_metric': 'F1',\n",
       " 'depth': 5}"
      ]
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "source": [
    "print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(30)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'learn': {'Logloss': 0.09385285316573008, 'F1': 0.7868249456612607}}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Feature Id  Importances\n",
       "0   h5_ekipage    14.942294\n",
       "1   h2_ekipage    14.366222\n",
       "2   h1_ekipage    12.717147\n",
       "3   h3_ekipage    12.436779\n",
       "4   h4_ekipage    12.268079\n",
       "5      ekipage    11.128203\n",
       "6       streck     7.150700\n",
       "7     h2_kmtid     0.853184\n",
       "8       senast     0.823465\n",
       "9        datum     0.806665\n",
       "10          kr     0.713151\n",
       "11     h1_perf     0.699447\n",
       "12     h3_pris     0.616363\n",
       "13      delta3     0.558789\n",
       "14    h1_kmtid     0.512897\n",
       "15      delta1     0.493922\n",
       "16     h4_odds     0.399342\n",
       "17     h5_pris     0.379313\n",
       "18    h3_kmtid     0.376841\n",
       "19     h2_perf     0.363897\n",
       "20     h1_odds     0.358347\n",
       "21    h4_kmtid     0.351980\n",
       "22     h2_dist     0.336979\n",
       "23       ålder     0.326751\n",
       "24     h5_odds     0.307837\n",
       "25     h3_perf     0.307186\n",
       "26    h5_kmtid     0.306439\n",
       "27      delta2     0.301733\n",
       "28     h1_dist     0.299096\n",
       "29        spår     0.285357"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h5_ekipage</td>\n",
       "      <td>14.942294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h2_ekipage</td>\n",
       "      <td>14.366222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h1_ekipage</td>\n",
       "      <td>12.717147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h3_ekipage</td>\n",
       "      <td>12.436779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h4_ekipage</td>\n",
       "      <td>12.268079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ekipage</td>\n",
       "      <td>11.128203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>streck</td>\n",
       "      <td>7.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h2_kmtid</td>\n",
       "      <td>0.853184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>senast</td>\n",
       "      <td>0.823465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>datum</td>\n",
       "      <td>0.806665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kr</td>\n",
       "      <td>0.713151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h1_perf</td>\n",
       "      <td>0.699447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h3_pris</td>\n",
       "      <td>0.616363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>delta3</td>\n",
       "      <td>0.558789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h1_kmtid</td>\n",
       "      <td>0.512897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>delta1</td>\n",
       "      <td>0.493922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h4_odds</td>\n",
       "      <td>0.399342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h5_pris</td>\n",
       "      <td>0.379313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>h3_kmtid</td>\n",
       "      <td>0.376841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h2_perf</td>\n",
       "      <td>0.363897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h1_odds</td>\n",
       "      <td>0.358347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>h4_kmtid</td>\n",
       "      <td>0.351980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>h2_dist</td>\n",
       "      <td>0.336979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ålder</td>\n",
       "      <td>0.326751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>h5_odds</td>\n",
       "      <td>0.307837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h3_perf</td>\n",
       "      <td>0.307186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h5_kmtid</td>\n",
       "      <td>0.306439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>delta2</td>\n",
       "      <td>0.301733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>h1_dist</td>\n",
       "      <td>0.299096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spår</td>\n",
       "      <td>0.285357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "source": [
    "\r\n",
    "params = {\r\n",
    "         'use_best_model': True,\r\n",
    "         'eval_metric' : 'F1',\r\n",
    "         \"loss_function\": \"Logloss\",\r\n",
    "         'early_stopping_rounds': 100,\r\n",
    "         # 'verbose': 50,\r\n",
    "         'iterations': 2000,\r\n",
    "         'logging_level': 'Silent',\r\n",
    "         'leaf_estimation_iterations': 10,\r\n",
    "         'l2_leaf_reg': 3.162277660168379e-20,\r\n",
    "         'depth': 5,\r\n",
    "}\r\n",
    "\r\n",
    "cv_score =cv(pool=my_pool, \r\n",
    "   params=params, \r\n",
    "   seed=2021, \r\n",
    "   stratified=True,\r\n",
    "   as_pandas=True,\r\n",
    "   type='TimeSeries')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "source": [
    "cv_score[['test-F1-mean','train-F1-mean','test-Logloss-mean','train-Logloss-mean']].describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       test-F1-mean  train-F1-mean  test-Logloss-mean  train-Logloss-mean\n",
       "count    649.000000     649.000000         649.000000          649.000000\n",
       "mean       0.677948       0.729751           0.138852            0.118658\n",
       "std        0.082360       0.123635           0.059768            0.067113\n",
       "min        0.240333       0.210173           0.117617            0.071732\n",
       "25%        0.677019       0.684495           0.118504            0.084709\n",
       "50%        0.706888       0.763873           0.121889            0.102425\n",
       "75%        0.722640       0.818602           0.130317            0.125003\n",
       "max        0.729156       0.855998           0.654153            0.653558"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-F1-mean</th>\n",
       "      <th>train-F1-mean</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.677948</td>\n",
       "      <td>0.729751</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>0.118658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.082360</td>\n",
       "      <td>0.123635</td>\n",
       "      <td>0.059768</td>\n",
       "      <td>0.067113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.240333</td>\n",
       "      <td>0.210173</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.071732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.677019</td>\n",
       "      <td>0.684495</td>\n",
       "      <td>0.118504</td>\n",
       "      <td>0.084709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.706888</td>\n",
       "      <td>0.763873</td>\n",
       "      <td>0.121889</td>\n",
       "      <td>0.102425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.722640</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.130317</td>\n",
       "      <td>0.125003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.654153</td>\n",
       "      <td>0.653558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 312
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## other models that need no NaN and no Categorical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "source": [
    "# XGBoost model \r\n",
    "# cat?\r\n",
    "# NaN's?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ANN model - Approx near neighbours\r\n",
    "# kör all preproc ovan\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "source": [
    "# GridSearch\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "scorer = make_scorer(matthews_corrcoef)\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=RandomForestClassifier(),\n",
       "             param_grid={'class_weight': ['balanced'],\n",
       "                         'max_depth': [4, 5, 6, None],\n",
       "                         'n_estimators': [5, 10, 100], 'random_state': [2021]},\n",
       "             scoring=make_scorer(matthews_corrcoef))"
      ]
     },
     "metadata": {},
     "execution_count": 315
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "source": [
    "# get best estimator and params\r\n",
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.4254736603787804\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 2021}"
      ]
     },
     "metadata": {},
     "execution_count": 316
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              importance\n",
       "h2_ekipage  1.547132e-01\n",
       "h1_ekipage  1.420154e-01\n",
       "h3_ekipage  1.410606e-01\n",
       "h5_ekipage  1.345333e-01\n",
       "streck      1.303589e-01\n",
       "...                  ...\n",
       "kön_v       1.062269e-04\n",
       "kön_s       8.433718e-05\n",
       "h4_auto     8.115417e-05\n",
       "h3_auto     6.096262e-05\n",
       "h1_auto     7.177872e-07\n",
       "\n",
       "[63 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h2_ekipage</th>\n",
       "      <td>1.547132e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_ekipage</th>\n",
       "      <td>1.420154e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_ekipage</th>\n",
       "      <td>1.410606e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h5_ekipage</th>\n",
       "      <td>1.345333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streck</th>\n",
       "      <td>1.303589e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kön_v</th>\n",
       "      <td>1.062269e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kön_s</th>\n",
       "      <td>8.433718e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4_auto</th>\n",
       "      <td>8.115417e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_auto</th>\n",
       "      <td>6.096262e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_auto</th>\n",
       "      <td>7.177872e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 317
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM  model\r\n",
    "# All preproc ovan?\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('random_forest', best_rf),\r\n",
    "               ('catboost', best_catb),\r\n",
    "            #    ('knn', KNeighborsClassifier(n_neighbors=11))\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV()\r\n",
    "stacking_model = StackingClassifier(estimators=base_models, \r\n",
    "                                    final_estimator=meta_model, \r\n",
    "                                    passthrough=True, \r\n",
    "                                    cv=tscv,\r\n",
    "                                    verbose=2)\r\n",
    "\r\n",
    "stacking_model.fit(trdf,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "source": [
    "## Testar TimeSeries folders manuell loop\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "print(tscv)\r\n",
    "i=1\r\n",
    "for train_index, test_index in tscv.split(trdf):\r\n",
    "    print(f\"TRAIN1_{i}, {train_index[-1:]}, TEST_{i}, {test_index[-1:]}\")\r\n",
    "    X_train, X_test = trdf.iloc[train_index], trdf.iloc[test_index]\r\n",
    "    y_train, y_test = y[train_index], y[test_index]\r\n",
    "    for name,model in base_models:\r\n",
    "        print(name)\r\n",
    "        model.fit(X_train,y_train)\r\n",
    "        y_hat = model.predict_proba(X_test)\r\n",
    "    i+=1\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "TRAIN1_1, [6948], TEST_1, [13894]\n",
      "random_forest\n",
      "catboost\n",
      "TRAIN1_2, [13894], TEST_2, [20840]\n",
      "random_forest\n",
      "catboost\n",
      "TRAIN1_3, [20840], TEST_3, [27786]\n",
      "random_forest\n",
      "catboost\n",
      "TRAIN1_4, [27786], TEST_4, [34732]\n",
      "random_forest\n",
      "catboost\n",
      "TRAIN1_5, [34732], TEST_5, [41678]\n",
      "random_forest\n",
      "catboost\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Walkthrough-funktionen  här"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "### Kör en walkthrough learn här, en datum i taget framåt\r\n",
    "\r\n",
    "# Jag har ändrat till att alla steg kör utan test-datam ed fast iterations=100\r\n",
    "def walkthrough(classic_test=False, verbose=False):\r\n",
    "    \r\n",
    "    df, nya_lopp, alla_datum, split_ix = get_alla_datum()\r\n",
    "\r\n",
    "    l2_leaf_regs=2\r\n",
    "    model=get_model(use_best=False,iterations=100)\r\n",
    "    df=remove_features(df.copy())\r\n",
    "    cat_features = list(df.loc[:,df.dtypes=='O'].columns)\r\n",
    "    df,_ = replace_NaN(df.copy(), cat_features=cat_features)    \r\n",
    "    print(f'cat_features {cat_features}\\n')\r\n",
    "\r\n",
    "    df['plac']=(df.plac==1)*1\r\n",
    "        \r\n",
    "    for nr,datum in enumerate(alla_datum[split_ix:]):\r\n",
    "        print(f'walk-iter {nr+1} av {len(alla_datum[split_ix:])} ',end=': ')\r\n",
    "\r\n",
    "        X_train = df.loc[df.datum<datum,:].copy()\r\n",
    "        y_train = X_train.plac; X_train.drop(['plac'],axis=1,inplace=True)\r\n",
    "\r\n",
    "        if classic_test:    ### klassisk train/test utan walkthrough\r\n",
    "            X_test  = df.loc[df.datum>=datum,:].copy()\r\n",
    "            y_test  = X_test.plac;  X_test.drop(['plac'],axis=1,inplace=True)\r\n",
    "            train_pool = Pool(X_train,y_train,cat_features=cat_features)\r\n",
    "            test_pool = Pool(X_test,y_test,cat_features=cat_features)\r\n",
    "            model.fit(train_pool,use_best_model=True, verbose=verbose,eval_set=test_pool)\r\n",
    "        else:\r\n",
    "            X_test  = df.loc[df.datum==datum,:].copy()\r\n",
    "            y_test  = X_test.plac;  X_test.drop(['plac'],axis=1,inplace=True)\r\n",
    "            train_pool = Pool(X_train,y_train,cat_features=cat_features)\r\n",
    "            test_pool = Pool(X_test,y_test,cat_features=cat_features)\r\n",
    "            model.fit(train_pool,use_best_model=False, verbose=verbose)\r\n",
    "\r\n",
    "        print('best iteration',model.get_best_iteration(), '\\tbest score', round(model.get_best_score()['learn']['Accuracy'],3) )\r\n",
    "        ##['validation']['Logloss'],3),'\\t', round(model.get_best_score()['validation']['Accuracy:use_weights=true'],3))\r\n",
    "        \r\n",
    "        if classic_test:    ### klassisk train/test utan walkthrough\r\n",
    "            return model,cat_features\r\n",
    "    \r\n",
    "        model.save_model('modeller/model_'+datum)\r\n",
    "\r\n",
    "    X_train =df.copy().drop('plac',axis=1)\r\n",
    "    y_train = df.plac \r\n",
    "    model.fit(X_train,y=y_train,cat_features=cat_features)\r\n",
    "    print(f'spara model_senaste',datum)\r\n",
    "    model.save_model('modeller/model_senaste')\r\n",
    "\r\n",
    "    return df,nya_lopp, model,cat_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Här körs hela walkthrough"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df, nya_lopp, model, cat_features = walkthrough(classic_test=False, verbose=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = get_model().load_model('modeller/model_senaste')\r\n",
    "df = pd.read_csv('all_data.csv')     \r\n",
    "# print(df.columns)\r\n",
    "df=remove_features(df.copy())\r\n",
    "cat_features = list(df.loc[:,df.dtypes=='O'].columns)\r\n",
    "df,_ = replace_NaN(df.copy(), cat_features=cat_features)    \r\n",
    "y=df.plac\r\n",
    "y=(y==1)*1\r\n",
    "df.drop('plac',axis=1,inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.columns[(df.dtypes=='object').values.tolist()]].info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## cv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "cv_pool = Pool(df,y,cat_features=cat_features)\r\n",
    "\r\n",
    "params = {\r\n",
    "         'use_best_model': True,\r\n",
    "         'eval_metric' : 'Recall',\r\n",
    "         \"loss_function\": \"Logloss\",\r\n",
    "         'early_stopping_rounds': 100,\r\n",
    "         'verbose': 50,\r\n",
    "         'iterations': 2000,\r\n",
    "         'seed': 2021,\r\n",
    "         'startified': True,\r\n",
    "         'as_pandas': True,\r\n",
    "         'type': 'TimeSeries'\r\n",
    "}\r\n",
    "\r\n",
    "cv_score =cv(pool=cv_pool, \r\n",
    "   params=params, \r\n",
    "   dtrain=None, \r\n",
    "   iterations=2000, \r\n",
    "   num_boost_round=None,\r\n",
    "   fold_count=5, \r\n",
    "   nfold=None,\r\n",
    "   inverted=False,\r\n",
    "   partition_random_seed=0,\r\n",
    "   seed=2021, \r\n",
    "   shuffle=False, \r\n",
    "   logging_level=None, \r\n",
    "   stratified=True,\r\n",
    "   as_pandas=True,\r\n",
    "   type='TimeSeries')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "cv_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     iterations  test-F1-mean  test-F1-std  train-F1-mean  train-F1-std  \\\n",
       "0             0      0.000000     0.000000       0.000000      0.000000   \n",
       "1             1      0.000000     0.000000       0.000000      0.000000   \n",
       "2             2      0.000000     0.000000       0.000741      0.001284   \n",
       "3             3      0.000000     0.000000       0.000371      0.000642   \n",
       "4             4      0.002953     0.005115       0.006238      0.009857   \n",
       "..          ...           ...          ...            ...           ...   \n",
       "184         184      0.120758     0.017292       0.280924      0.003395   \n",
       "185         185      0.121312     0.017374       0.282253      0.004709   \n",
       "186         186      0.122547     0.015863       0.282494      0.003678   \n",
       "187         187      0.119446     0.016485       0.283570      0.004286   \n",
       "188         188      0.118826     0.015504       0.283504      0.003290   \n",
       "\n",
       "     test-Logloss-mean  test-Logloss-std  train-Logloss-mean  \\\n",
       "0             0.660234          0.003815            0.659794   \n",
       "1             0.629065          0.003617            0.628580   \n",
       "2             0.598386          0.009334            0.600561   \n",
       "3             0.572443          0.008103            0.574281   \n",
       "4             0.548483          0.007865            0.550048   \n",
       "..                 ...               ...                 ...   \n",
       "184           0.244545          0.004970            0.207284   \n",
       "185           0.244574          0.004972            0.207173   \n",
       "186           0.244520          0.004856            0.207063   \n",
       "187           0.244538          0.004805            0.206988   \n",
       "188           0.244605          0.004896            0.206897   \n",
       "\n",
       "     train-Logloss-std  \n",
       "0             0.004020  \n",
       "1             0.003901  \n",
       "2             0.006937  \n",
       "3             0.005280  \n",
       "4             0.005760  \n",
       "..                 ...  \n",
       "184           0.006843  \n",
       "185           0.006899  \n",
       "186           0.006966  \n",
       "187           0.007005  \n",
       "188           0.007091  \n",
       "\n",
       "[189 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-F1-mean</th>\n",
       "      <th>test-F1-std</th>\n",
       "      <th>train-F1-mean</th>\n",
       "      <th>train-F1-std</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>test-Logloss-std</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "      <th>train-Logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660234</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.628580</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.598386</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.600561</td>\n",
       "      <td>0.006937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.572443</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.574281</td>\n",
       "      <td>0.005280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.548483</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.550048</td>\n",
       "      <td>0.005760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>0.120758</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.280924</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.244545</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.207284</td>\n",
       "      <td>0.006843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>0.121312</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.282253</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.244574</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.207173</td>\n",
       "      <td>0.006899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>0.122547</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.282494</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.244520</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.207063</td>\n",
       "      <td>0.006966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>0.119446</td>\n",
       "      <td>0.016485</td>\n",
       "      <td>0.283570</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.244538</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.206988</td>\n",
       "      <td>0.007005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>0.118826</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.283504</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.244605</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.007091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "cv"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function catboost.core.cv(pool=None, params=None, dtrain=None, iterations=None, num_boost_round=None, fold_count=None, nfold=None, inverted=False, partition_random_seed=0, seed=None, shuffle=True, logging_level=None, stratified=None, as_pandas=True, metric_period=None, verbose=None, verbose_eval=None, plot=False, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, metric_update_interval=0.5, folds=None, type='Classical', return_models=False, log_cout=<ipykernel.iostream.OutStream object at 0x000001B50082C1C0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001B50082CF70>)>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_score[cv_score['test-Logloss-mean'].min() == cv_score['test-Logloss-mean']]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}