{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Try stacking for V75"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "from catboost import CatBoostClassifier,Pool,cv,utils \r\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "### return a CatBoost model with some default parameters\r\n",
    "def get_model(d=6,l2=2,iterations=3000,use_best=True,verbose=False):\r\n",
    "    model = CatBoostClassifier(iterations=iterations,use_best_model=use_best, \r\n",
    "        custom_metric=['Logloss', 'AUC','Recall', 'Precision', 'F1', 'Accuracy'],\r\n",
    "\r\n",
    "        eval_metric='Accuracy', \r\n",
    "        depth=d,l2_leaf_reg=l2,\r\n",
    "        auto_class_weights='Balanced',verbose=verbose, random_state=2021) \r\n",
    "    return model                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "### Features som inte används vid träning\r\n",
    "def remove_features(df,remove_mer=[]):\r\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) #\r\n",
    "    if remove_mer:\r\n",
    "        df.drop(remove_mer,axis=1,inplace=True)\r\n",
    "    \r\n",
    "    # df=check_unique(df.copy())\r\n",
    "    # df=check_corr(df.copy())\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    " ## byt ut alla NaN till text för cat_features\r\n",
    "def replace_NaN(X_train,X_test=None, cat_features=[]):\r\n",
    "    # print('cat_features',cat_features)\r\n",
    "    for c in cat_features:\r\n",
    "        # print(c)\r\n",
    "        X_train.loc[X_train[c].isna(),c] = 'missing'       ### byt ut None-värden till texten 'Missing'\r\n",
    "        if X_test is not None:  ## om X_test är med\r\n",
    "            X_test.loc [X_test[c].isna(),c] = 'missing'    ### byt ut None-värden till texten 'Missing'\r\n",
    "\r\n",
    "    return X_train,X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "### läs in data och returnera df, alla datum samt index till split-punkt\r\n",
    "def load_data(proc=0.75):\r\n",
    "    \r\n",
    "    df = pd.read_csv('..\\\\all_data.csv')     \r\n",
    "    alla_datum = list(df.datum.unique())\r\n",
    "    split_ix = int(len(alla_datum)*proc)\r\n",
    "    \r\n",
    "    return df,alla_datum,split_ix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def remove_not_used_features(df):\r\n",
    "    df.drop(['avd','startnr','vodds','podds','bins','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True) \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "df,alla_datum,split_ix = load_data() \r\n",
    "df = remove_not_used_features(df.copy())\r\n",
    "CAT_FEATURES=['datum', 'bana', 'häst', 'kusk', 'kön',\r\n",
    "        'h1_kusk', 'h1_bana',\r\n",
    "        'h2_kusk', 'h2_bana', \r\n",
    "        'h3_kusk',  'h3_bana', \r\n",
    "        'h4_kusk', 'h4_bana', \r\n",
    "        'h5_kusk', 'h5_bana',]\r\n",
    "\r\n",
    "NUM_FEATURES=[item for item in df.columns if item not in CAT_FEATURES and item !='plac']\r\n",
    "\r\n",
    "PLAC_MEAN=df.plac.mean()\r\n",
    "PLAC_MEAN"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.208773316093193"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# den hittade inget, kanske skall testa igen längre fram\r\n",
    "def remove_low_variance_features(df):\r\n",
    "    from sklearn.feature_selection import VarianceThreshold\r\n",
    "    print(df.shape)\r\n",
    "    selection = VarianceThreshold(threshold=(0.1))\r\n",
    "    X=selection.fit_transform(df)\r\n",
    "    print(X.shape)\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions that are doing the transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# fill missing values in categorical features\r\n",
    "def impute_cat_features(df, cat_features):\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[cat_features]=imp1.fit_transform(df[cat_features])  # replae NaN's with 'missing'\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Set a smooth mean value to the features in df\r\n",
    "def calc_smooth_mean(df, by, y, m=300, tot_mean=PLAC_MEAN):\r\n",
    "\r\n",
    "    # Compute the number of values and the mean of each group\r\n",
    "    agg = df.groupby(by)[y].agg(['count', 'mean'])\r\n",
    "    counts = agg['count']\r\n",
    "    means = agg['mean']\r\n",
    "\r\n",
    "    # Compute the \"smoothed\" means\r\n",
    "    smooth = (counts * means + m * tot_mean) / (counts + m)\r\n",
    "\r\n",
    "    # Replace each value by the according smoothed mean\r\n",
    "    return df[by].map(smooth)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Handle h1-h5_bana\r\n",
    "def transform_hx_bana(df,hx,the_map):\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    df[hx] = df[hx].str.lower()\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='missing')\r\n",
    "    df[hx]=imp1.fit_transform(df[[hx]])  # replae NaN's with 'missing'\r\n",
    "\r\n",
    "    df[hx] = [item[0] for item in df[hx].str.split('-')]  # remove '-10' from 'solvalla-10' etc\r\n",
    "    \r\n",
    "    df[hx]=df[hx].map(the_map)  # transform column to numeric by mapping\r\n",
    "    # after mapping we get new NaN's - now impute 0\r\n",
    "    imp2 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\r\n",
    "    df[hx] = imp2.fit_transform(df[[hx]])\r\n",
    "    return df\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "\r\n",
    "# Handle bana and hx_bana  \r\n",
    "def transf_bana(df):\r\n",
    "    df['bana'] = df.bana.str.lower()\r\n",
    "    the_map = df.bana.value_counts() \r\n",
    "    the_map['missing']=0    \r\n",
    "\r\n",
    "    df=transform_hx_bana(df,'h1_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h2_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h3_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h4_bana',the_map)\r\n",
    "    df=transform_hx_bana(df,'h5_bana',the_map)\r\n",
    "\r\n",
    "    df['bana']=df.bana.map(the_map)  # transform column to numeric by mapping \r\n",
    "    if df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum().sum() != 0:\r\n",
    "        print('bana NaNs not 0:',df[['h1_bana','h2_bana','h3_bana','h4_bana','h5_bana',]].isna().sum())\r\n",
    "    \r\n",
    "    df.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Handle häst and kusk \r\n",
    "def transf_kusk_häst(df,pref='',m=50,):\r\n",
    "    df[pref+'ekipage'] = df[pref+'kusk'].str.cat(df['häst'], sep =\", \")  # concatenate 'häst' and 'kusk' into one column\r\n",
    "    df[pref+'ekipage'] = calc_smooth_mean(df, by=pref+'ekipage', y='plac',m=50) # make numeric with Target encoding with smooth mean\r\n",
    "    df.drop([pref+'kusk'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Handle kön  \r\n",
    "def transf_kön(df):\r\n",
    "    from sklearn.preprocessing import OneHotEncoder\r\n",
    "    df['kön'] = df['kön'].str.lower()\r\n",
    "    ohe = OneHotEncoder(sparse=False)\r\n",
    "    dftemp=pd.DataFrame(ohe.fit_transform(df[['kön']]),columns=['kön_h','kön_s','kön_v'] )  # replae kön with One Hot Encoding\r\n",
    "    df=pd.concat([df,dftemp],axis=1)\r\n",
    "\r\n",
    "    # check that kön is correct encoded\r\n",
    "    if len(df.loc[(df.kön=='h') & (df.kön_h != 1),'kön']):\r\n",
    "        print('Felaktigt kön','h')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='s') & (df.kön_s != 1),'kön']):\r\n",
    "        print('Felaktigt kön','s')\r\n",
    "        error()\r\n",
    "    if len(df.loc[(df.kön=='v') & (df.kön_v != 1),'kön']):\r\n",
    "        print('Felaktigt kön','v')\r\n",
    "        error()\r\n",
    "    df.drop(['kön'],axis=1,inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "\r\n",
    "def impute_all_numeric_NaNs(df):\r\n",
    "    # all features must be numeric\r\n",
    "    from sklearn.impute import SimpleImputer\r\n",
    "    imp1 = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=-1)\r\n",
    "    trdf=imp1.fit_transform(df)  # replae NaN's with 'missing'\r\n",
    "    return pd.DataFrame(trdf,columns=df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the transformations in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "\r\n",
    "def transf_all(df):\r\n",
    "    \r\n",
    "    trdf=transf_bana(df.copy())\r\n",
    "    trdf=transf_kusk_häst(trdf)\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h1_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h2_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h3_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h4_')\r\n",
    "    trdf=transf_kusk_häst(trdf,pref='h5_')\r\n",
    "    trdf.drop(['häst'],axis=1,inplace=True)\r\n",
    "    trdf=transf_kön(trdf)\r\n",
    "    trdf['datum']=pd.to_datetime(trdf.datum).view(float)*10e210\r\n",
    "    \r\n",
    "    return impute_all_numeric_NaNs(trdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\r\n",
    "# transform all categoricals and impute all NaNs\r\n",
    "def prepare_all(df):\r\n",
    "    trdf = transf_all(df)\r\n",
    "    \r\n",
    "    y = (trdf.plac==1) * 1\r\n",
    "    trdf = trdf.drop('plac',axis=1)\r\n",
    "    \r\n",
    "    # all features are now numeric\r\n",
    "    trdf = impute_all_numeric_NaNs(trdf)\r\n",
    "    if trdf.isna().sum().sum() != 0:\r\n",
    "        print('still NaNs in data')\r\n",
    "        assert False\r\n",
    "    return trdf,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "#catBoost preprocessing\r\n",
    "def catB_preprocess(df):\r\n",
    "        y = (df.plac==1) * 1\r\n",
    "        df = df.drop('plac',axis=1)\r\n",
    "        df = impute_cat_features(df,cat_features=CAT_FEATURES)\r\n",
    "\r\n",
    "        return df,y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "\r\n",
    "# clean the cat_features\r\n",
    "df_catb, y = catB_preprocess(df.copy())\r\n",
    "df_catb[CAT_FEATURES].isna().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "\r\n",
    "# metrics\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import matthews_corrcoef\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# for tuning\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "trdf,y=prepare_all(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# CatBoost model GridSearchCV\r\n",
    "my_df_1=df_catb             # catboost with Nans abd cat_features\r\n",
    "my_cats_1 = CAT_FEATURES\r\n",
    "my_df_2 = trdf              # dataset common for all estimators\r\n",
    "my_cats_2 = []\r\n",
    "\r\n",
    "my_df = my_df_2\r\n",
    "my_cats = my_cats_2\r\n",
    "my_pool = Pool(my_df,y,cat_features=my_cats)\r\n",
    "my_catb = CatBoostClassifier(cat_features=my_cats)\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "params = {'iterations': [50,100,500],\r\n",
    "          'depth': [4, 5, 6],\r\n",
    "          'loss_function': ['Logloss'],\r\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\r\n",
    "          'leaf_estimation_iterations': [10],\r\n",
    "          'eval_metric': ['F1'],\r\n",
    "        #   'use_best_model': ['True'],\r\n",
    "          'logging_level':['Silent'],\r\n",
    "          'random_seed': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "scorer = make_scorer(matthews_corrcoef)\r\n",
    "catb_grid = RandomizedSearchCV(estimator=my_catb, param_distributions=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  - compare with default\r\n",
    "catb_grid.fit(my_df,y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "                   estimator=<catboost.core.CatBoostClassifier object at 0x000001C05A073BE0>,\n",
       "                   param_distributions={'depth': [4, 5, 6],\n",
       "                                        'eval_metric': ['F1'],\n",
       "                                        'iterations': [50, 100, 500],\n",
       "                                        'l2_leaf_reg': array([1.00000000e-20, 3.16227766e-20, 1.00000000e-19]),\n",
       "                                        'leaf_estimation_iterations': [10],\n",
       "                                        'logging_level': ['Silent'],\n",
       "                                        'loss_function': ['Logloss'],\n",
       "                                        'random_seed': [2021]},\n",
       "                   scoring=make_scorer(matthews_corrcoef))"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# get best estimator and params\r\n",
    "best_catb = catb_grid.best_estimator_\r\n",
    "print('best Logloss gridsearch',catb_grid.best_score_)\r\n",
    "best_param = catb_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best Logloss gridsearch 0.7099996522577294\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'random_seed': 2021,\n",
       " 'loss_function': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'l2_leaf_reg': 1e-20,\n",
       " 'iterations': 500,\n",
       " 'eval_metric': 'F1',\n",
       " 'depth': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "print(best_catb.fit(my_df,y).best_score_)\r\n",
    "best_catb.get_feature_importance(prettified=True).head(30)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'learn': {'Logloss': 0.10284111770116602, 'F1': 0.7587376993552766}}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h5_ekipage</td>\n",
       "      <td>18.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h2_ekipage</td>\n",
       "      <td>15.147799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h3_ekipage</td>\n",
       "      <td>15.049699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h4_ekipage</td>\n",
       "      <td>12.663214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h1_ekipage</td>\n",
       "      <td>12.426701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ekipage</td>\n",
       "      <td>12.275516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>streck</td>\n",
       "      <td>6.548204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>senast</td>\n",
       "      <td>0.596055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spår</td>\n",
       "      <td>0.522682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h2_kmtid</td>\n",
       "      <td>0.493945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>delta3</td>\n",
       "      <td>0.487070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h5_odds</td>\n",
       "      <td>0.352706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h4_pris</td>\n",
       "      <td>0.326284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>delta1</td>\n",
       "      <td>0.323139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h1_perf</td>\n",
       "      <td>0.306335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h2_dist</td>\n",
       "      <td>0.303369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h2_perf</td>\n",
       "      <td>0.297520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>datum</td>\n",
       "      <td>0.270439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>h1_kmtid</td>\n",
       "      <td>0.230499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h4_kmtid</td>\n",
       "      <td>0.223545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>delta4</td>\n",
       "      <td>0.212434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>h3_perf</td>\n",
       "      <td>0.171619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kr</td>\n",
       "      <td>0.171366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>h5_pris</td>\n",
       "      <td>0.158657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>h4_odds</td>\n",
       "      <td>0.151555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>delta2</td>\n",
       "      <td>0.142570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h3_kmtid</td>\n",
       "      <td>0.141833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>h4_perf</td>\n",
       "      <td>0.136532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>h3_pris</td>\n",
       "      <td>0.119268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h2_auto</td>\n",
       "      <td>0.103392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature Id  Importances\n",
       "0   h5_ekipage    18.231200\n",
       "1   h2_ekipage    15.147799\n",
       "2   h3_ekipage    15.049699\n",
       "3   h4_ekipage    12.663214\n",
       "4   h1_ekipage    12.426701\n",
       "5      ekipage    12.275516\n",
       "6       streck     6.548204\n",
       "7       senast     0.596055\n",
       "8         spår     0.522682\n",
       "9     h2_kmtid     0.493945\n",
       "10      delta3     0.487070\n",
       "11     h5_odds     0.352706\n",
       "12     h4_pris     0.326284\n",
       "13      delta1     0.323139\n",
       "14     h1_perf     0.306335\n",
       "15     h2_dist     0.303369\n",
       "16     h2_perf     0.297520\n",
       "17       datum     0.270439\n",
       "18    h1_kmtid     0.230499\n",
       "19    h4_kmtid     0.223545\n",
       "20      delta4     0.212434\n",
       "21     h3_perf     0.171619\n",
       "22          kr     0.171366\n",
       "23     h5_pris     0.158657\n",
       "24     h4_odds     0.151555\n",
       "25      delta2     0.142570\n",
       "26    h3_kmtid     0.141833\n",
       "27     h4_perf     0.136532\n",
       "28     h3_pris     0.119268\n",
       "29     h2_auto     0.103392"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "\r\n",
    "params = {\r\n",
    "         'use_best_model': True,\r\n",
    "         'eval_metric' : 'F1',\r\n",
    "         \"loss_function\": \"Logloss\",\r\n",
    "         'early_stopping_rounds': 100,\r\n",
    "         # 'verbose': 50,\r\n",
    "         'iterations': 2000,\r\n",
    "         'logging_level': 'Silent',\r\n",
    "         'leaf_estimation_iterations': 10,\r\n",
    "         'l2_leaf_reg': 3.162277660168379e-20,\r\n",
    "         'depth': 5,\r\n",
    "}\r\n",
    "\r\n",
    "cv_score =cv(pool=my_pool, \r\n",
    "   params=params, \r\n",
    "   seed=2021, \r\n",
    "   stratified=True,\r\n",
    "   as_pandas=True,\r\n",
    "   type='TimeSeries')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "cv_score[['test-F1-mean','train-F1-mean','test-Logloss-mean','train-Logloss-mean']].describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-F1-mean</th>\n",
       "      <th>train-F1-mean</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>701.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>701.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.684884</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.136203</td>\n",
       "      <td>0.114455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.087960</td>\n",
       "      <td>0.130601</td>\n",
       "      <td>0.058253</td>\n",
       "      <td>0.066165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.207294</td>\n",
       "      <td>0.184522</td>\n",
       "      <td>0.115739</td>\n",
       "      <td>0.067644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.685692</td>\n",
       "      <td>0.698208</td>\n",
       "      <td>0.116395</td>\n",
       "      <td>0.080476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.712692</td>\n",
       "      <td>0.776551</td>\n",
       "      <td>0.120257</td>\n",
       "      <td>0.098539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.729353</td>\n",
       "      <td>0.829813</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>0.121771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.734310</td>\n",
       "      <td>0.869643</td>\n",
       "      <td>0.652395</td>\n",
       "      <td>0.651956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test-F1-mean  train-F1-mean  test-Logloss-mean  train-Logloss-mean\n",
       "count    701.000000     701.000000         701.000000          701.000000\n",
       "mean       0.684884       0.739627           0.136203            0.114455\n",
       "std        0.087960       0.130601           0.058253            0.066165\n",
       "min        0.207294       0.184522           0.115739            0.067644\n",
       "25%        0.685692       0.698208           0.116395            0.080476\n",
       "50%        0.712692       0.776551           0.120257            0.098539\n",
       "75%        0.729353       0.829813           0.127725            0.121771\n",
       "max        0.734310       0.869643           0.652395            0.651956"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## other models that need no NaN and no Categorical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# XGBoost model \r\n",
    "# cat?\r\n",
    "# NaN's?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# ANN model - Approx near neighbours\r\n",
    "# kör all preproc ovan\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForrest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# GridSearch\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "params = {'n_estimators': [5,10,100],\r\n",
    "          'max_depth': [4, 5, 6, None],\r\n",
    "          'class_weight': ['balanced'],\r\n",
    "        #   'loss_function': ['Logloss'],\r\n",
    "        #   'eval_metric': ['F1'],\r\n",
    "        #   'logging_level':['Silent'],\r\n",
    "          'random_state': [2021],\r\n",
    "         }\r\n",
    "# clf.fit(df_catb,y)\r\n",
    "\r\n",
    "scorer = make_scorer(matthews_corrcoef)\r\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=params, scoring=scorer, cv=tscv)\r\n",
    "\r\n",
    "# GridSearchCV  \r\n",
    "rf_grid.fit(trdf, y)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=RandomForestClassifier(),\n",
       "             param_grid={'class_weight': ['balanced'],\n",
       "                         'max_depth': [4, 5, 6, None],\n",
       "                         'n_estimators': [5, 10, 100], 'random_state': [2021]},\n",
       "             scoring=make_scorer(matthews_corrcoef))"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# get best estimator and params\r\n",
    "best_rf = rf_grid.best_estimator_\r\n",
    "print('best gridsearch',rf_grid.best_score_)\r\n",
    "best_param = rf_grid.best_params_\r\n",
    "best_param"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best gridsearch 0.4250374092308687\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 2021}"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "\r\n",
    "pd.DataFrame(best_rf.feature_importances_,index=trdf.columns, columns=['importance']).sort_values(by='importance',ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h2_ekipage</th>\n",
       "      <td>0.170345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_ekipage</th>\n",
       "      <td>0.142896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_ekipage</th>\n",
       "      <td>0.137229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streck</th>\n",
       "      <td>0.130473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h5_ekipage</th>\n",
       "      <td>0.128405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kön_v</th>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4_auto</th>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2_auto</th>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_auto</th>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_auto</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance\n",
       "h2_ekipage    0.170345\n",
       "h3_ekipage    0.142896\n",
       "h1_ekipage    0.137229\n",
       "streck        0.130473\n",
       "h5_ekipage    0.128405\n",
       "...                ...\n",
       "kön_v         0.000089\n",
       "h4_auto       0.000078\n",
       "h2_auto       0.000068\n",
       "h3_auto       0.000029\n",
       "h1_auto       0.000025\n",
       "\n",
       "[63 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# SVM  model\r\n",
    "# All preproc ovan?\r\n",
    "# GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stack'em"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "#\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegressionCV\r\n",
    "base_models = [('random_forest', best_rf),\r\n",
    "               ('catboost', best_catb),\r\n",
    "            #    ('knn', KNeighborsClassifier(n_neighbors=11))\r\n",
    "               ]\r\n",
    "meta_model = LogisticRegressionCV()\r\n",
    "stacking_model = StackingClassifier(estimators=base_models, \r\n",
    "                                    final_estimator=meta_model, \r\n",
    "                                    passthrough=True, \r\n",
    "                                    cv=tscv,\r\n",
    "                                    verbose=2)\r\n",
    "\r\n",
    "# stacking_model.fit(trdf,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def evaluate_model(model, X, y):\r\n",
    "    tscv = TimeSeriesSplit(n_splits=5, n_repeats=2, random_state=1)\r\n",
    "    scores = cross_val_score(model, X, y, scoring='AUC', cv=tscv, verbose=1, n_jobs=3, error_score='raise')\r\n",
    "    return scores\r\n",
    "\r\n",
    "def Stacking(model, X_tr, y_tr, X_final, n_fold):\r\n",
    "    tscv = TimeSeriesSplit(n_splits=n_fold)\r\n",
    "    # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "    train_pred=np.empty((0,1),float)\r\n",
    "    for train_indices, test_indices in tscv.split(X_tr):\r\n",
    "        X_train, X_test = X_tr.iloc[train_indices], X_tr.iloc[test_indices]\r\n",
    "        y_train, y_test = y_tr.iloc[train_indices], y_tr.iloc[test_indices]\r\n",
    "        print(f'test indices {len(test_indices)} X_test {len(X_test)} train_pred before {len(train_pred)}')\r\n",
    "        model.fit(X=X_train,y=y_train)\r\n",
    "        train_pred=np.append(train_pred,model.predict_proba(X_test)[:,1])\r\n",
    "        print('len train_pred',len(train_pred), 'one predict',len(model.predict_proba(X_test)))\r\n",
    "    valid_pred = model.predict_proba(X_final)[:,1]\r\n",
    "    return valid_pred.reshape(-1,1), train_pred\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tscv = TimeSeriesSplit(n_splits=5)\r\n",
    "# totlen=0\r\n",
    "# rf = base_models[0][1]\r\n",
    "# # valid_pred=np.empty((X_valid.shape[0],1),float)\r\n",
    "# train_pred=np.empty((0,1),float)\r\n",
    "# for train_indices, test_indices in tscv.split(train_X):\r\n",
    "#     X_train, X_test = train_X.iloc[train_indices], train_X.iloc[test_indices]\r\n",
    "#     y_train, y_test = train_y.iloc[train_indices], train_y.iloc[test_indices]\r\n",
    "\r\n",
    "#     rf.fit(X=X_train,y=y_train)\r\n",
    "#     my_pred = rf.predict_proba(X_test)\r\n",
    "#     # print(my_pred)\r\n",
    "#     print(my_pred[:,1])\r\n",
    "#     break\r\n",
    "    \r\n",
    "# # print(totlen,train_X.shape[0]-totlen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "split_ix = int(len(trdf)*.8)\r\n",
    "train_X = trdf[trdf.index <  split_ix]\r\n",
    "valid_X = trdf[trdf.index >= split_ix]\r\n",
    "train_y = y[y.index <  split_ix]\r\n",
    "# test 2 models\r\n",
    "\r\n",
    "print(base_models[0][0])\r\n",
    "model1 = base_models[0][1]\r\n",
    "valid_pred1 ,train_pred1=Stacking(model=model1,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "train_pred1=pd.DataFrame(train_pred1)\r\n",
    "valid_pred1=pd.DataFrame(valid_pred1)\r\n",
    "\r\n",
    "print(base_models[1][0])\r\n",
    "model2 = base_models[1][1]\r\n",
    "valid_pred2 ,train_pred2=Stacking(model=model2,n_fold=5, X_tr=train_X, y_tr= train_y, X_final=valid_X)\r\n",
    "train_pred2=pd.DataFrame(train_pred2)\r\n",
    "valid_pred2=pd.DataFrame(valid_pred2)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "random_forest\n",
      "test indices 5568 X_test 5568 train_pred before 0\n",
      "len train_pred 5568 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 5568\n",
      "len train_pred 11136 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 11136\n",
      "len train_pred 16704 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 16704\n",
      "len train_pred 22272 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 22272\n",
      "len train_pred 27840 one predict 5568\n",
      "catboost\n",
      "test indices 5568 X_test 5568 train_pred before 0\n",
      "len train_pred 5568 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 5568\n",
      "len train_pred 11136 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 11136\n",
      "len train_pred 16704 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 16704\n",
      "len train_pred 22272 one predict 5568\n",
      "test indices 5568 X_test 5568 train_pred before 22272\n",
      "len train_pred 27840 one predict 5568\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "print(train_X.shape,train_y.shape,valid_X.shape)\r\n",
    "train_pred1.shape,valid_pred1.shape #,train_pred2.shape,valid_pred2.shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(33410, 63) (33410,) (8353, 63)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((27840, 1), (8353, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Testar TimeSeries folders manuell loop\r\n",
    "tscv = TimeSeriesSplit()\r\n",
    "print(tscv)\r\n",
    "train_pred = pd.DataFrame()\r\n",
    "test_pred=pd.DataFrame()\r\n",
    "i=1\r\n",
    "\r\n",
    "for train_index, test_index in tscv.split(trdf):\r\n",
    "    print(f\"TRAIN1_{i}, start={train_index[:1]} end={train_index[-1:]}, TEST_{i},start={test_index[0:1]} end={test_index[-1:]}\")\r\n",
    "    X_train, X_test = trdf.iloc[train_index], trdf.iloc[test_index]\r\n",
    "    y_train, y_test = y[train_index], y[test_index]\r\n",
    "    print(f'X_train.shape {X_train.shape}\\tX_test.shape {X_test.shape} ')\r\n",
    "    temp_pred=pd.DataFrame()\r\n",
    "    for name,model in base_models:\r\n",
    "        print(name)\r\n",
    "        model.fit(X_train,y_train)\r\n",
    "        y_hat = pd.DataFrame(model.predict_proba(X_test))\r\n",
    "        temp_pred = pd.concat([y_hat, y_test],axis=1)\r\n",
    "        print(temp_pred.tail())\r\n",
    "    train_pred = pd.concat([train_pred,temp_pred],axis=0)\r\n",
    "        \r\n",
    "    i+=1\r\n",
    "# train_pred.columns=['rf','cb','y']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# tscv.split(train,y.values)\r\n",
    "for train_index, test_index in tscv.split(trdf):\r\n",
    "    print(train_index[:1],train_index[-1:], test_index[:1],test_index[-1:],)\r\n",
    "    print(y.iloc[train_index][-10:].values,y.iloc[test_index][-10:].values)\r\n",
    "print()  \r\n",
    "y.tail(10).values"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0] [6962] [6963] [13922]\n",
      "[0 0 0 0 0 0 0 0 1 0] [0 0 1 0 0 0 0 0 0 0]\n",
      "[0] [13922] [13923] [20882]\n",
      "[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]\n",
      "[0] [20882] [20883] [27842]\n",
      "[0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 0 0 0 0 0]\n",
      "[0] [27842] [27843] [34802]\n",
      "[0 0 0 1 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0]\n",
      "[0] [34802] [34803] [41762]\n",
      "[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0]\n",
      "\n",
      "alt\n",
      "[0] [6962] [6963] [13922]\n",
      "[0 0 0 0 0 0 0 0 1 0] [0 0 1 0 0 0 0 0 0 0]\n",
      "[0] [13922] [13923] [20882]\n",
      "[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]\n",
      "[0] [20882] [20883] [27842]\n",
      "[0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 0 0 0 0 0]\n",
      "[0] [27842] [27843] [34802]\n",
      "[0 0 0 1 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0]\n",
      "[0] [34802] [34803] [41762]\n",
      "[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0]\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## cv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "cv_pool = Pool(df,y,cat_features=cat_features)\r\n",
    "\r\n",
    "params = {\r\n",
    "         'use_best_model': True,\r\n",
    "         'eval_metric' : 'Recall',\r\n",
    "         \"loss_function\": \"Logloss\",\r\n",
    "         'early_stopping_rounds': 100,\r\n",
    "         'verbose': 50,\r\n",
    "         'iterations': 2000,\r\n",
    "         'seed': 2021,\r\n",
    "         'startified': True,\r\n",
    "         'as_pandas': True,\r\n",
    "         'type': 'TimeSeries'\r\n",
    "}\r\n",
    "\r\n",
    "cv_score =cv(pool=cv_pool, \r\n",
    "   params=params, \r\n",
    "   dtrain=None, \r\n",
    "   iterations=2000, \r\n",
    "   num_boost_round=None,\r\n",
    "   fold_count=5, \r\n",
    "   nfold=None,\r\n",
    "   inverted=False,\r\n",
    "   partition_random_seed=0,\r\n",
    "   seed=2021, \r\n",
    "   shuffle=False, \r\n",
    "   logging_level=None, \r\n",
    "   stratified=True,\r\n",
    "   as_pandas=True,\r\n",
    "   type='TimeSeries')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-F1-mean</th>\n",
       "      <th>test-F1-std</th>\n",
       "      <th>train-F1-mean</th>\n",
       "      <th>train-F1-std</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>test-Logloss-std</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "      <th>train-Logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660234</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.628580</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.598386</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.600561</td>\n",
       "      <td>0.006937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.572443</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.574281</td>\n",
       "      <td>0.005280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.548483</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.550048</td>\n",
       "      <td>0.005760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>0.120758</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.280924</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.244545</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.207284</td>\n",
       "      <td>0.006843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>0.121312</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.282253</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.244574</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.207173</td>\n",
       "      <td>0.006899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>0.122547</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.282494</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.244520</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.207063</td>\n",
       "      <td>0.006966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>0.119446</td>\n",
       "      <td>0.016485</td>\n",
       "      <td>0.283570</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.244538</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.206988</td>\n",
       "      <td>0.007005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>0.118826</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.283504</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.244605</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.007091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterations  test-F1-mean  test-F1-std  train-F1-mean  train-F1-std  \\\n",
       "0             0      0.000000     0.000000       0.000000      0.000000   \n",
       "1             1      0.000000     0.000000       0.000000      0.000000   \n",
       "2             2      0.000000     0.000000       0.000741      0.001284   \n",
       "3             3      0.000000     0.000000       0.000371      0.000642   \n",
       "4             4      0.002953     0.005115       0.006238      0.009857   \n",
       "..          ...           ...          ...            ...           ...   \n",
       "184         184      0.120758     0.017292       0.280924      0.003395   \n",
       "185         185      0.121312     0.017374       0.282253      0.004709   \n",
       "186         186      0.122547     0.015863       0.282494      0.003678   \n",
       "187         187      0.119446     0.016485       0.283570      0.004286   \n",
       "188         188      0.118826     0.015504       0.283504      0.003290   \n",
       "\n",
       "     test-Logloss-mean  test-Logloss-std  train-Logloss-mean  \\\n",
       "0             0.660234          0.003815            0.659794   \n",
       "1             0.629065          0.003617            0.628580   \n",
       "2             0.598386          0.009334            0.600561   \n",
       "3             0.572443          0.008103            0.574281   \n",
       "4             0.548483          0.007865            0.550048   \n",
       "..                 ...               ...                 ...   \n",
       "184           0.244545          0.004970            0.207284   \n",
       "185           0.244574          0.004972            0.207173   \n",
       "186           0.244520          0.004856            0.207063   \n",
       "187           0.244538          0.004805            0.206988   \n",
       "188           0.244605          0.004896            0.206897   \n",
       "\n",
       "     train-Logloss-std  \n",
       "0             0.004020  \n",
       "1             0.003901  \n",
       "2             0.006937  \n",
       "3             0.005280  \n",
       "4             0.005760  \n",
       "..                 ...  \n",
       "184           0.006843  \n",
       "185           0.006899  \n",
       "186           0.006966  \n",
       "187           0.007005  \n",
       "188           0.007091  \n",
       "\n",
       "[189 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function catboost.core.cv(pool=None, params=None, dtrain=None, iterations=None, num_boost_round=None, fold_count=None, nfold=None, inverted=False, partition_random_seed=0, seed=None, shuffle=True, logging_level=None, stratified=None, as_pandas=True, metric_period=None, verbose=None, verbose_eval=None, plot=False, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, metric_update_interval=0.5, folds=None, type='Classical', return_models=False, log_cout=<ipykernel.iostream.OutStream object at 0x000001B50082C1C0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001B50082CF70>)>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_score[cv_score['test-Logloss-mean'].min() == cv_score['test-Logloss-mean']]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}