{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testa allt mellan himmel och jord \"\"\"\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "pref = '../'\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\peter\\\\Documents\\\\MyProjects\\\\PyProj\\\\Trav\\\\spel')\n",
    "import typ as tp\n",
    "import travdata as td\n",
    "import time\n",
    "import concurrent.futures\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 260)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "metric_list = sorted(sklearn.metrics.SCORERS.keys())\n",
    "(metric_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Läs in all data och tvätta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                           Läs in all data och tvätta                        #\n",
    "###############################################################################\n",
    "def läs_in_data(förbered=True, lägg_till=True):\n",
    "    v75 = td.v75(pref=pref)\n",
    "    df_work = v75.get_df()\n",
    "    # display(df_work.columns.to_list())\n",
    "    \n",
    "    df,enc = v75.förbered_data(missing_num=True)  # num hanteras av catboost\n",
    "    df = v75.lägg_till_kolumner(df)\n",
    "    return df,v75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,v75 = läs_in_data()\n",
    "print(df.datum.max())\n",
    "\n",
    "df[['datum','avd', 'bana','häst', 'rel_kr',\t'streck_avst',\t'rel_rank',\t'h1_samma_bana','h1_bana',\t'h2_samma_bana',\t'h3_samma_bana',\t'h1_samma_kusk',\t'h2_samma_kusk',\t'h3_samma_kusk']].tail(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa Next_datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_data.csv\n",
      "Loading dataframe from the file: ../all_data.csv\n",
      "plac finns i df\n",
      "Handling missing categoricals\n",
      "Handling missing numerics\n",
      "Använd mod.skapa_modeller i stället\n",
      "Använd mod.skapa_modeller i stället\n",
      "Använd mod.skapa_modeller i stället\n",
      "Använd mod.skapa_modeller i stället\n",
      "Använd mod.skapa_modeller i stället\n",
      "Använd mod.skapa_modeller i stället\n",
      "max_datum i df 2023-01-28\n"
     ]
    }
   ],
   "source": [
    "df, v75 = läs_in_data()\n",
    "print('max_datum i df', df.datum.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_datum 2016-09-10 next_datum 2016-09-17 test_datum 2016-09-18\n"
     ]
    }
   ],
   "source": [
    "gap=0  # om vi skall testa på en omgång som inte är nästa omgång\n",
    "step=1 # hur många omgångar vi hoppa till för nästa test\n",
    "alla_datum = df.datum.unique()\n",
    "curr_datum = alla_datum[120]\n",
    "# beräkna next_datum med step och gap\n",
    "ix1 = np.where(alla_datum == curr_datum)[0][0]\n",
    "next_datum = alla_datum[ix1 + step]  # nästa curr_datum learn until including\n",
    "test_datum = alla_datum[ix1 + step+1 + gap] # nästa omgång att testa på\n",
    "\n",
    "print('curr_datum', curr_datum, 'next_datum', next_datum, 'test_datum', test_datum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(scores):\n",
    "    \"\"\" Compute normalized weights to use to get the meta column. \n",
    "    Args:\n",
    "        (list) scores: list of (i.e. F1 or AUC) scores from L2 models on validation set\n",
    "    Returns:\n",
    "        (list) weights: weights to use to get the meta column    \n",
    "    \"\"\"\n",
    "    weights = scores / np.sum(scores)\n",
    "    return weights\n",
    "\n",
    "\n",
    "cat1L2_dict = {'AUC': 0.71232, 'F1': 0.34766, 'Acc': 0.80825, 'MAE': 0.19175}\n",
    "cat1L2 = list(cat1L2_dict.values())\n",
    "cat2L2 = [0.71488, 0.35033, 0.80898, 0.19102]\n",
    "xgb1L2 = [0.59313, 0.22062, 0.77084, 0.22916 ]\n",
    "xgb2L2 = [0.61568, 0.24473, 0.77818, 0.22182]\n",
    "\n",
    "lists = [cat1L2, cat2L2, xgb1L2, xgb2L2]\n",
    "AUC, F1, Acc, MAE = [], [], [], []\n",
    "for l in lists:\n",
    "    AUC.append(l[0])\n",
    "    F1.append(l[1])\n",
    "    Acc.append(l[2])\n",
    "    MAE.append(l[3])\n",
    "\n",
    "AUC=compute_weights(AUC)\n",
    "F1=compute_weights(F1)\n",
    "Acc=compute_weights(Acc)\n",
    "MAE=compute_weights(MAE)\n",
    "print('weights')\n",
    "print(np.round(AUC, 4),np.round(np.sum(AUC),1))\n",
    "print(np.round(F1, 4),np.round(np.sum(F1),1))\n",
    "print(np.round(Acc, 4),np.round(np.sum(Acc),1))\n",
    "print(np.round(MAE, 4),np.round(np.sum(MAE),1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# läs in xgb-modellerna och testa vilka features de är fit på\n",
    "modeller = ['xgb1L1','xgb2L1', 'xgb1L2', 'xgb2L2']\n",
    "\n",
    "for model in modeller:\n",
    "    print(model, end=': ')\n",
    "    with open(f'{pref}modeller/{model}.model', 'rb') as f:\n",
    "        xgb_model = pickle.load(f)\n",
    "    print(len(xgb_model.get_booster().feature_names))\n",
    "    print(xgb_model.get_booster().feature_names)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testa random grid search xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random grid search för xgb\n",
    "def randomGrid_xgb(use_model):\n",
    "    \n",
    "    ############ Inledande skit #####################################################\n",
    "    modell_dict = {'cat1': {'#hästar': False, '#motst': 3, 'motst_diff': True, 'streck': False},\n",
    "                   'cat2': {'#hästar': True,  '#motst': 3, 'motst_diff': True, 'streck': True},\n",
    "                   'xgb1': {'#hästar': False, '#motst': 3, 'motst_diff': True, 'streck': False},\n",
    "                   'xgb2': {'#hästar': True,  '#motst': 3, 'motst_diff': True, 'streck': True}\n",
    "                   }\n",
    "\n",
    "\n",
    "    L1_modeller = dict()\n",
    "    L2_modeller = dict()\n",
    "\n",
    "    for key, typ in modell_dict.items():\n",
    "        L1_key = key + 'L1'\n",
    "        model = tp.Typ(L1_key, typ['#hästar'], typ['#motst'],\n",
    "                    typ['motst_diff'], typ['streck'])\n",
    "        L1_modeller[L1_key] = model\n",
    "\n",
    "        L2_key = key + 'L2'\n",
    "        model = tp.Typ(L2_key, typ['#hästar'], typ['#motst'],\n",
    "                    typ['motst_diff'], typ['streck'])\n",
    "        L2_modeller[L2_key] = model\n",
    "\n",
    "    print('keys and names i modeller')\n",
    "    # print keys in dict modeller\n",
    "    for key, typ in L1_modeller.items():\n",
    "        assert key == typ.name, \"key and value.name should be the same in modeller\"\n",
    "        print(key)\n",
    "\n",
    "    print('keys and names i meta_modeller')\n",
    "    for key, typ in L2_modeller.items():\n",
    "        assert key == typ.name, \"key and value.name should be the same in meta_modeller\"\n",
    "        print(key)\n",
    "\n",
    "    # Läs in NUM_FEATURES.txt och CAT_FEATURES.txt\n",
    "    with open(pref+'CAT_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        cat_features = f.read().split()\n",
    "\n",
    "    # läs in NUM_FEATURES.txt till num_features\n",
    "    with open(pref+'NUM_FEATURES.txt', 'r', encoding='utf-8') as f:\n",
    "        num_features = f.read().split()\n",
    "    use_features = cat_features + num_features\n",
    "    \n",
    "    print('use_features:\\n', use_features)\n",
    "    \n",
    "    ############ SLUT Inledande skit #################################################\n",
    "    typ=L1_modeller[use_model]\n",
    "\n",
    "    grid = {'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "            #  'iterations': [100, 200, 300, 400, 500],   n_estimators\n",
    "             'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "              'n_estimators': [100, 200, 300, 400, 500],\n",
    "              'reg_lambda': [0.1, 1, 2, 3, 4],\n",
    "              'subsample': [0.5, 0.8, 1]}\n",
    "\n",
    "\n",
    "    if not typ.streck:\n",
    "        print('remove streck')\n",
    "        use_features.remove('streck')\n",
    "\n",
    "    \n",
    "    df,_=v75.förbered_data(extra=True)\n",
    "    X = typ.prepare_for_model(df.drop(['y'],axis=1))\n",
    "    y = df.y.copy()\n",
    "    \n",
    "    # xgb_encoder till ENC\n",
    "    with open(pref+'xgb_encoder.pkl', 'rb') as f:\n",
    "        ENC = pickle.load(f)\n",
    "        \n",
    "    X, ENC = tp.prepare_for_xgboost(X, encoder=ENC)\n",
    "    display(X[use_features].head(1))\n",
    "\n",
    "    \n",
    "    assert X[use_features].isnull().sum().sum() == 0, 'there are NaN values in cat_features'\n",
    "    # assert that all cat_features are numeric\n",
    "    non_numeric_columns = X[use_features].select_dtypes(exclude=[np.number]).columns\n",
    "    assert len(non_numeric_columns)==0, f\"Följande kolumner är inte numeriska: {non_numeric_columns}\"\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc', random_state=2023)\n",
    "\n",
    "    display(f'XGB Randomized search')\n",
    "    grid_search_result = RandomizedSearchCV(model, param_distributions=grid,\n",
    "                                                    cv=tscv.split(X[use_features]),\n",
    "                                                    scoring=['roc_auc','neg_log_loss'],\n",
    "                                                    random_state=2023,\n",
    "                                                    refit='roc_auc',\n",
    "                                                    n_jobs=-1,\n",
    "                                                    verbose=1)\n",
    "    grid_search_result.fit(X[use_features], y)\n",
    "\n",
    "    return grid_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result = randomGrid_xgb('xgb1L1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_search_result.best_params_)\n",
    "display(grid_search_result.best_score_)\n",
    "ix = grid_search_result.best_index_\n",
    "display(-grid_search_result.cv_results_['mean_test_neg_log_loss'][ix])\n",
    "display(grid_search_result.cv_results_['mean_test_roc_auc'] [ix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fejka_resultat_tabell():\n",
    "    ### init resutat-tabell\n",
    "    df_resultat = pd.DataFrame(columns = ['datum', 't1_7', 't1_6', 't1_5', 't1_kostn', 't1_utd', 't1_vinst' ])\n",
    "    df_resultat.set_index('datum',drop=True, inplace=True)\n",
    "    df_resultat.loc['2019-08-01'] = [0, 0, 8,300,200, -100]\n",
    "    df_resultat.loc['2019-08-08'] = [0, 0, 0,600,200, -400]\n",
    "    df_resultat.loc['2019-08-15'] = [0, 1, 2,900,600, -300]\n",
    "    df_resultat.loc['2019-08-22'] = [1, 2, 8,1200,1000, -200]\n",
    "    df_resultat.sort_index(ascending=True,inplace=True)\n",
    "    # 3. plotta\n",
    "    print(df_resultat.head())\n",
    "    # bigger plot\n",
    "    df_resultat.plot(kind='line',  y='t1_vinst', rot=45, legend=True, figsize=(20,10))\n",
    "    return df_resultat\n",
    "\n",
    "df_resultat = fejka_resultat_tabell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTA DETTA I IPYNB\n",
    "# testar diverse lösningar till v75 (streamlit)\n",
    "# import module for randomforrest\n",
    "from sklearn.ensemble import RandomForestRegressor as rf\n",
    "\n",
    "def kelly(proba, streck, odds):  # proba = prob winning, streck i % = streck\n",
    "    # läs in streck_to_odds.pkl\n",
    "    import pickle\n",
    "    with open(pref+'rf_streck_odds.pkl', 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "        \n",
    "    if odds is None:\n",
    "        o = rf.predict(streck.copy())\n",
    "    else:\n",
    "        o = rf.predict(streck.copy())\n",
    "\n",
    "    # for each values > 40 in odds set to 1\n",
    "    o[o > 40] = 1\n",
    "    return (o*proba - (1-proba))/o\n",
    "\n",
    "def compute_total_insats(veckans_rad):\n",
    "    summa = veckans_rad.groupby('avd').avd.count().prod() / 2\n",
    "    return summa\n",
    "\n",
    "def beräkna_utdelning(datum, sjuor, sexor, femmor, df_utdelning):\n",
    "    min_utdelning = df_utdelning.loc[df_utdelning.datum==datum,['7rätt', '6rätt','5rätt']]\n",
    "    \n",
    "    return (min_utdelning['7rätt'] * sjuor + min_utdelning['6rätt'] * sexor + min_utdelning['5rätt'] * femmor).values[0]\n",
    "\n",
    "def ta_fram_rad(veckans_rad_, spik_strategi,kelly_strategi, max_cost=300, min_avst=0.25):\n",
    "    \"\"\" Denna funktion tar fram en rad för typ-modeller (ej meta-modell)\n",
    "    df nnehåller _en omgång_\n",
    "    _spik_strategi_: None - inget, '1a' - forcera 1 spik, '2a' - forcera 2 spikar, '1b' - 1 spik endast om klar favorit, '2b' - spikar för endast klara favoriter \n",
    "    _kelly_strategi_: None - ingen kelly, 1 - kelly varannan gång om positiv\n",
    "    \"\"\"\n",
    "\n",
    "    veckans_rad = veckans_rad_.copy()\n",
    "    veckans_rad['välj'] = False   # inga rader valda ännu\n",
    "    veckans_rad['spik'] = False   # inga spikar valda ännu\n",
    "    # a) ta ut en häst i varje avd - markera valda i df\n",
    "    \n",
    "    for avd in veckans_rad.avd.unique():\n",
    "        # max av proba i veckans_rad \n",
    "        max_proba = veckans_rad[veckans_rad.avd == avd]['proba'].max()\n",
    "        veckans_rad.loc[(veckans_rad.avd == avd) & (veckans_rad.proba == max_proba), 'välj'] = True\n",
    "    \n",
    "    # b) leta 1-2 spikar om så begärs - markera valda i df\n",
    "    spikad_avd = []\n",
    "    if spik_strategi:\n",
    "        print('spik_strategi', spik_strategi)\n",
    "        assert spik_strategi in ['1a','1b','2a','2b'], \"spik_strategi måste ha något av värdena i listn\"\n",
    "        # Hitta spik-kandidater\n",
    "        if spik_strategi[0] in ['1','2']:\n",
    "            spik1 = veckans_rad.nlargest(1,'proba').index[0]   # largest in dataset\n",
    "            avd = veckans_rad.loc[spik1,'avd']\n",
    "            no2 = veckans_rad.query(\"avd==@avd\").nlargest(2, 'proba').index[1]  # second in avd\n",
    "            print(f'hösta proba={veckans_rad.loc[spik1, \"proba\"]} i avd={avd}; no2 i avd={avd} är {veckans_rad.loc[no2,\"proba\"]}')\n",
    "            avstånd = veckans_rad.loc[spik1, 'proba'] - veckans_rad.loc[no2, 'proba']\n",
    "            print('avst', avstånd)\n",
    "            if (spik_strategi[1] == 'b') and (avstånd > min_avst):\n",
    "                print('strategi', spik_strategi[1], 'valde spik i avd',avd)\n",
    "                # add avd to a list\n",
    "                spikad_avd.append(avd)\n",
    "                \n",
    "                veckans_rad.loc[spik1,'spik'] = True\n",
    "                veckans_rad.loc[spik1, 'välj'] = True\n",
    "            elif spik_strategi[1] == 'a':\n",
    "                print('strategi',spik_strategi[1], 'valde spik i avd',avd)\n",
    "                spikad_avd.append(avd)\n",
    "                veckans_rad.loc[spik1,['spik']]= True\n",
    "                veckans_rad.loc[spik1, 'välj']= True\n",
    "                \n",
    "        if spik_strategi[0] == '2':\n",
    "            spik2 = veckans_rad.nlargest(2,'proba').index[1] # second in dataset\n",
    "            avd = veckans_rad.loc[spik2, 'avd']\n",
    "            no2 = veckans_rad.query(\"avd==@avd\").nlargest(2, 'proba').index[1]  # second in avd\n",
    "            print(f'näst högsta proba={veckans_rad.loc[spik2, \"proba\"]} i avd={avd}; no2 i avd={avd} är {veckans_rad.loc[no2,\"proba\"]}')\n",
    "            avstånd = veckans_rad.loc[spik2, 'proba'] - veckans_rad.loc[no2, 'proba']\n",
    "            print('avst',avstånd)\n",
    "            if (spik_strategi[1] =='b') and (avstånd > min_avst):\n",
    "                print('strategi', spik_strategi[1], 'valde spik i avd', avd)    \n",
    "                spikad_avd.append(avd)\n",
    "                veckans_rad.loc[spik2, 'spik'] = True\n",
    "                veckans_rad.loc[spik2, 'välj'] = True\n",
    "            elif spik_strategi[1] == 'a':\n",
    "                print('strategi', spik_strategi[1], 'i avd',avd)\n",
    "                spikad_avd.append(avd)\n",
    "                veckans_rad.loc[spik2,'spik'] = True\n",
    "                veckans_rad.loc[spik2,'välj'] = True\n",
    "                \n",
    "    # c) sortera upp i proba-ordning. Om kelly skapa en sortering efter kelly-ordning\n",
    "    veckans_rad = veckans_rad.sort_values(by=['proba'], ascending=False)\n",
    "    veckans_rad = veckans_rad.reset_index(drop=True)\n",
    "    if kelly_strategi == '1':\n",
    "        veckans_kelly = veckans_rad.sort_values(by=['kelly'], ascending=False)\n",
    "        veckans_kelly = veckans_kelly.reset_index(drop=True)\n",
    "    \n",
    "    cost = 0.5 # 1 rad\n",
    "    while cost < max_cost:\n",
    "        # d) plocka en och en - först proba sedan ev positiv kelly markera som valda i df\n",
    "        curr_index = veckans_rad.query(\"välj==False and avd not in @spikad_avd\").nlargest(1,'proba').index\n",
    "        veckans_rad.loc[curr_index,'välj'] = True\n",
    "        # e) avbryt vid 300:-\n",
    "        cost = compute_total_insats(veckans_rad.query(\"välj==True\"))\n",
    "        if  cost > max_cost:\n",
    "            veckans_rad.loc[curr_index, 'välj'] = False  # ta tillbaks den sist spelade\n",
    "            break\n",
    "        if kelly_strategi == '1' and veckans_rad.query(\"välj==False and avd not in @spikad_avd and kelly > 0\").shape[0] > 0:\n",
    "            curr_index = veckans_rad.query(\"välj==False and avd not in @spikad_avd and kelly > 0\").nlargest(1,'kelly').index\n",
    "            cost = compute_total_insats(veckans_rad.query(\"välj==True\"))\n",
    "            veckans_rad.loc[curr_index, 'välj'] = True\n",
    "            if  cost > max_cost:\n",
    "                veckans_rad.loc[curr_index, 'välj'] = False  # ta tillbaks den sist spelade\n",
    "                break\n",
    "    cost = compute_total_insats(veckans_rad.query(\"välj==True\"))\n",
    "    return veckans_rad, cost\n",
    "\n",
    "def rätta_rad(df, datum, df_utdelning ):\n",
    "    \"\"\"\n",
    "    Räkna ut antal 5:or, 6:or resp. 7:or\n",
    "    Hämta ev utdelning\n",
    "    Spara datum, resultat, utdelning och rad-kostnad\n",
    "    \"\"\"\n",
    "    sjuor, sexor, femmor, utdelning = 0,0,0,0\n",
    "    \n",
    "    min_tabell = df[['y', 'avd', 'häst', 'rel_rank', 'välj']].copy()\n",
    "    min_tabell.sort_values(by=['avd', 'y'], ascending=False,inplace=True)\n",
    "\n",
    "    # 1. om jag har max 7 rätt\n",
    "    if min_tabell.query('välj==True and y==1').y.sum() == 7:\n",
    "        sjuor=1\n",
    "        sexor = (min_tabell.groupby('avd').välj.sum()).sum()-7\n",
    "        # antal femmor\n",
    "        ant1 = min_tabell.query('avd==1 and välj==True').välj.sum()-1\n",
    "        ant2 = min_tabell.query('avd==2 and välj==True').välj.sum()-1\n",
    "        ant3 = min_tabell.query('avd==3 and välj==True').välj.sum()-1\n",
    "        ant4 = min_tabell.query('avd==4 and välj==True').välj.sum()-1\n",
    "        ant5 = min_tabell.query('avd==5 and välj==True').välj.sum()-1\n",
    "        ant6 = min_tabell.query('avd==6 and välj==True').välj.sum()-1\n",
    "        ant7 = min_tabell.query('avd==7 and välj==True').välj.sum()-1\n",
    "        femmor = ant1*ant2+ant1*ant2+ant1*ant3+ant1*ant4+ant1*ant5+ant1*ant6+ant1*ant7 +\\\n",
    "                ant2*ant3+ant2*ant4+ant2*ant5+ant2*ant6+ant2*ant7 + \\\n",
    "                ant3*ant4+ant3*ant5+ant3*ant6+ant3*ant7 + \\\n",
    "                ant4*ant5+ant4*ant6+ant4*ant7 + \\\n",
    "                ant5*ant6+ant5*ant7 + \\\n",
    "                ant6*ant7\n",
    "\n",
    "    # 2. jag har max 6 rätt\n",
    "    if min_tabell.query('välj==True and y==1').y.sum() == 6:\n",
    "        avd_fel = min_tabell.loc[((min_tabell.välj==False) & (min_tabell.y==1)),'avd'].values[0]\n",
    "        print(min_tabell.query('avd== @avd_fel').välj.sum())\n",
    "        sexor = min_tabell.query('avd==@avd_fel').välj.sum()\n",
    "        # antal femmor\n",
    "        femmor_fel, femmor_rätt = 0,0\n",
    "        for avd in range(1,8):\n",
    "            if avd == avd_fel:\n",
    "                femmor_fel += min_tabell.loc[min_tabell.avd==avd_fel].välj.sum()\n",
    "                \n",
    "            femmor_rätt += min_tabell.query('avd==@avd and välj==True').välj.sum()-1\n",
    "        print(f'femmor_rätt = {femmor_rätt} femmor_fel = {femmor_fel}')    \n",
    "        femmor = femmor_fel * femmor_rätt\n",
    "\n",
    "    # 3. jag har max 5 rätt\n",
    "    if min_tabell.query('välj==True and y==1').y.sum() == 5:\n",
    "        avd_fel = min_tabell.loc[((min_tabell.välj==False) & (min_tabell.y==1)),'avd'].values\n",
    "        femmor = min_tabell.loc[min_tabell.avd==avd_fel[0]].välj.sum() * min_tabell.loc[min_tabell.avd==avd_fel[1]].välj.sum()\n",
    "    \n",
    "    # 4. utdelning \n",
    "    \n",
    "    return sjuor, sexor, femmor, beräkna_utdelning(datum, sjuor,sexor,femmor, df_utdelning)\n",
    "\n",
    "###############################################################################\n",
    "#      main logic v75\n",
    "def testa_main_v75():\n",
    "    df_utdelning = pd.read_csv(pref+'/utdelning.csv')\n",
    "    test1 = tp.Typ('test1',  True,    True,     False,       0,  False,          0,        False,    True, pref=pref)\n",
    "\n",
    "    startdatum = '1900-01-01'\n",
    "    # 0. ta fram startdatum  (datum=startdatum)\n",
    "    curr_datix = len(df.datum.unique()) - 200      # ca 3 å3 tillbaks\n",
    "    startdatum = df.datum.unique()[curr_datix]   # ca 3 år tillbaks\n",
    "    datum = startdatum\n",
    "    # 1. learn fram till datum\n",
    "    print(f'learn fram till {datum}')\n",
    "    X = df.query(f'datum < @datum').copy()\n",
    "    y = X.y\n",
    "    X = X.drop('y', axis=1)\n",
    "    X_test = df.query(f'datum > @datum').copy()\n",
    "    y_test = X_test.y\n",
    "    X_test = X_test.drop('y', axis=1)\n",
    "    X_curr = df.query(f'datum == @datum').copy()\n",
    "    veckans_rad = X_curr[['datum','avd','häst','bana','kusk','streck','streck_avst','rel_rank','y']].copy()\n",
    "    y_curr = X_curr.y\n",
    "    X_curr = X_curr.drop(['y'], axis=1)\n",
    "\n",
    "    params = {\"depth\": 2, \"l2_leaf_reg\": 3,\"iterations\": 500, \"learning_rate\": 0.008}\n",
    "    model = test1.learn(X, y, X_test=X_test, y_test=y_test,save=True, params=params)\n",
    "    print(X.shape)\n",
    "    print(X_curr.shape)\n",
    "    veckans_rad['proba'] = test1.predict(X_curr)\n",
    "    veckans_rad['kelly'] = kelly(veckans_rad.proba, veckans_rad[['streck']], None)\n",
    "\n",
    "    # 2. ta fram rad för datum, rätta och spara\n",
    "    # inkluderar spik_strategi,kelly_strategi,\n",
    "    veckans_rad, kostnad = ta_fram_rad(veckans_rad, '2b', '1', min_avst=0.3)\n",
    "\n",
    "    sjuor, sexor, femmor, utdelning = rätta_rad(veckans_rad, datum, df_utdelning)\n",
    "    print('kostnad',kostnad, 'utdelning', utdelning)\n",
    "\n",
    "    display(\"SPARA RESLUTAT\")\n",
    "    # 3. plotta\n",
    "\n",
    "        # loopa över olika setup\n",
    "\n",
    "    # 4. startdatum+1\n",
    "    # 5. gå till 1\n",
    "\n",
    "    return datum, kostnad, utdelning, sjuor, sexor, femmor, veckans_rad\n",
    "\n",
    "def testa_resultat(datum,kostnad, utdelning, sjuor, sexor, femmor):\n",
    "    df_resultat = pd.DataFrame(\n",
    "        columns=['datum', 't1_7', 't1_6', 't1_5', 't1_kostn', 't1_utd', 't1_vinst'])\n",
    "    df_resultat.set_index('datum', drop=True, inplace=True)\n",
    "    dict = {'t1_7': sjuor, 't1_6': sexor, 't1_5': femmor,\n",
    "            't1_kostn': kostnad, 't1_utd': utdelning, 't1_vinst': utdelning-kostnad}\n",
    "    dict = [sjuor, sexor, femmor, kostnad,  utdelning,  utdelning-kostnad]\n",
    "    # set last row to dict\n",
    "    df_resultat.loc[datum] = dict\n",
    "    datum = '2022-07-08'\n",
    "    df_resultat.loc[datum] = [sjuor+1, sexor, femmor,\n",
    "                            kostnad,  utdelning,  utdelning-kostnad]\n",
    "\n",
    "    df_resultat.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    df_resultat\n",
    "\n",
    "    return df_resultat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum ,kostnad, utdelning, sjuor, sexor, femmor, veckans_rad = testa_main_v75()\n",
    "df_resultat = testa_resultat(datum,kostnad, utdelning, sjuor, sexor, femmor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kolla_värdena(veckans_rad, sjuor, sexor, femmor, kostnad, utdelning):\n",
    "    ##### kolla värdena\n",
    "    print(kostnad)\n",
    "    print('sjuor',sjuor,'sexor', sexor, 'femmor', femmor)\n",
    "    print(len(veckans_rad.query(\"välj==True\")))\n",
    "    veckans_rad.query(\"välj==True\").sort_values(by=['avd','proba'],ascending=[True,False])\n",
    "    # veckans_rad.query(\"välj==True\").sort_values(by='avd')\n",
    "    # min_utdelning = df_utdelning.loc[df_utdelning.datum == datum, ['7rätt', '6rätt', '5rätt']]\n",
    "    # print(min_utdelning)\n",
    "    # min_utdelning['7rätt'] * sjuor + min_utdelning['6rätt'] * sexor + min_utdelning['5rätt'] * femmor\n",
    "\n",
    "kolla_värdena(veckans_rad, sjuor, sexor, femmor, kostnad, utdelning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_meta_proba(X_):\n",
    "    sm = X_.copy()\n",
    "    # select the highest meta_predict per avd\n",
    "    sm['first'] = sm.groupby('avd')['meta_predict'].transform(\n",
    "        lambda x: x.nlargest(2).reset_index(drop=True)[0])\n",
    "    sm['second'] = sm.groupby('avd')['meta_predict'].transform(\n",
    "        lambda x: x.nlargest(2).reset_index(drop=True)[1])\n",
    "\n",
    "    sm = sm.query(\"(first==meta_predict or second==meta_predict)\").copy()\n",
    "    sm['diff'] = sm['first'] - sm['second']\n",
    "\n",
    "    # drop duplicates per avd\n",
    "    sm = sm.drop_duplicates(subset='avd', keep='first')\n",
    "\n",
    "    sm.sort_values(by='diff', ascending=False, inplace=True)\n",
    "    # sm.to_csv('mesta_diff_per_avd.csv')\n",
    "    return sm\n",
    "\n",
    "\n",
    "def diff_streck(df, curr_datum='2022-07-30'):\n",
    "    larg = df.query('datum == curr_datum').groupby(\n",
    "        ['avd'])['streck'].nlargest(2)\n",
    "    larg = larg.reset_index().drop('level_1', axis=1)\n",
    "\n",
    "    # select the first row for each avd\n",
    "    first = larg.groupby(['avd']).head(1).set_index('avd')\n",
    "    second = larg.groupby(['avd']).tail(1).set_index('avd')\n",
    "\n",
    "    diff = first.sub(second, axis=0)\n",
    "    # print(diff)\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hx_samma_bana(df_, datum='2022-07-30'):\n",
    "    df = df_.copy()\n",
    "    return df[df.datum == datum].bana == df[df.datum == datum].h1_bana\n",
    "# hx_samma_bana(df)\n",
    "\n",
    "\n",
    "def hx_samma_kusk(df_, datum='2022-07-30'):\n",
    "    df = df_.copy()\n",
    "    return df[df.datum == datum].kusk == df[df.datum == datum].h1_kusk\n",
    "\n",
    "\n",
    "hx_samma_kusk(df).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta ut max srteck per avd\n",
    "def max_streck_per_avd(df_):\n",
    "    df = df_.copy()\n",
    "    df['max_streck'] = df.groupby(\n",
    "        ['datum', 'avd']).streck.transform(lambda x: x.max())\n",
    "\n",
    "    df['streck_avst'] = df.max_streck - df.streck\n",
    "    df.drop(['max_streck'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "max_streck_per_avd(df)[['häst', 'avd', 'streck_avst', 'streck', ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plats i streck per avd\n",
    "def plats_i_streck_per_avd(df_):\n",
    "    df = df_.copy()\n",
    "    # sortera streck per datum,avd\n",
    "    # df = df.sort_values(by=['datum', 'avd', 'streck'])\n",
    "    # ranking per avd\n",
    "    df['rank'] = df.groupby(['datum', 'avd'])['streck'].rank(ascending=False, method='dense')\n",
    "    df.sort_values(by=['datum', 'avd', 'rank'], inplace=True)\n",
    "    return df\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "plats_i_streck_per_avd(df)[['avd','häst','streck','streck_avst','rank']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what row has max kr in total in df\n",
    "df['välj']=False\n",
    "kelly_strategi = '1'\n",
    "df.loc[df.nlargest(2,'rel_kr').index,'välj'] = True\n",
    "\n",
    "df.loc[df.query(\"välj==False\").nlargest(1,'rel_kr').index,'välj'] = True\n",
    "\n",
    "if df.query(\"välj==False and rel_rank > 0\").shape[0] > 0:\n",
    "    df.loc[df.query(\"välj==False and rel_rank > 0\").nlargest(1, 'rel_rank').index, 'välj'] = True\n",
    "\n",
    "df.välj.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a new row by appending the new values to df_resultat\n",
    "# last_row = df_resultat.iloc[-1] + [sjuor,sexor,femmor,kostnad,utdelning,utdelning-kostnad]\n",
    "# datum='2022-08-26'\n",
    "# # add the new row to df_resultat with loc datum\n",
    "# df_resultat.loc[datum] = last_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utred varför rf blir så dålig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# write the scores\n",
    "def display_scores(y_true, y_pred, spelade):\n",
    "    print('AUC', round(roc_auc_score(y_true, y_pred), 5), 'F1', round(f1_score(y_true, y_pred), 5), 'Acc', round(\n",
    "        accuracy_score(y_true, y_pred), 5), 'MAE', round(mean_absolute_error(y_true, y_pred), 5), '\\n', spelade)\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def find_threshold(y_pred, fr, to, margin):\n",
    "    \"\"\" hitta threshold som ger 2.5 spelade per avdelning \"\"\"\n",
    "    thresh = 0\n",
    "    cnt = 0\n",
    "    # make a binary search\n",
    "    while cnt < 1000:\n",
    "        thresh = (fr + to) / 2\n",
    "        antal_spelade_per_avd = 12 * sum(y_pred > thresh)/len(y_pred)\n",
    "        if (antal_spelade_per_avd > (2.5 - margin)) and (antal_spelade_per_avd < (2.5 + margin)):\n",
    "            break\n",
    "\n",
    "        if antal_spelade_per_avd > 2.5:\n",
    "            fr = thresh-0.00001\n",
    "        else:\n",
    "            to = thresh+0.00001\n",
    "        cnt += 1\n",
    "\n",
    "    print('ant', cnt, 'thresh', round(thresh, 4))\n",
    "    if cnt >= 1000:\n",
    "        print('threshold not found', 'fr', round(fr, 6), 'to', round(to, 6))\n",
    "\n",
    "    return thresh\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, typ, fr=0.0, to=0.9, margin=0.001):\n",
    "    thresh = round(find_threshold(y_pred,fr,to,margin), 4)\n",
    "    print(f'Threshold: {thresh}\\n')\n",
    "    y_pred = (y_pred > thresh).astype(int)\n",
    "    # confusion_matrix_graph(y_true, y_pred, f'{typ} threshold={thresh}')\n",
    "\n",
    "    #### Sedan: confusion matrix graph ####\n",
    "    title = f'{typ} threshold={thresh}'\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred,)\n",
    "    fig, ax = plt.subplots()\n",
    "    # make it bigger\n",
    "    fig.set_size_inches(10, 10)\n",
    "    sns.set(font_scale=2.0)\n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", linewidths=.5,\n",
    "                square=True, cmap='Blues_r')\n",
    "\n",
    "    # increase font size\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    # plot fig\n",
    "    plt.show()\n",
    "    \n",
    "    _=display_scores(y_true, y_pred, f'spelade per lopp: {round(12 * sum(y_pred)/len(y_pred),4)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train = pd.read_csv(pref + 'rf_train.csv')\n",
    "rf_validate = pd.read_csv(pref + 'rf_validate.csv')\n",
    "rf_y_pred = pd.read_csv(pref + 'rf_y_pred.csv')\n",
    "rf_y_true = pd.read_csv(pref + 'rf_y_true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "with open(pref+'optimera/params_rf.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "        params = params['params']\n",
    "        # st.write(params)\n",
    "\n",
    "rf_model = RandomForestClassifier(**params, n_jobs=6, random_state=2022)\n",
    "rf_fit=rf_model.fit(rf_train.drop('y',axis=1), rf_train.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = rf_fit.predict_proba(rf_validate)[:,1]\n",
    "print('förbereder rf plot')\n",
    "plot_confusion_matrix(rf_y_true, new_pred,\n",
    "                      'rf', fr=0.0, to=1.0, margin=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d733caf4ffc39d0fbd9a2ba54ef4b7d515956d8048931f8241efe3827fb2d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
