{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kolla om CatBoost ranking kan ge något"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from catboost import CatBoostClassifier,Pool,cv,utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sklearn_pandas==2.0.4\n",
    "# !pip3 install catboost==0.24.4\n",
    "# !pip uninstall scikit-learn -y\n",
    "# !pip3 install scikit-learn==0.24.1 \n",
    "# import sklearn\n",
    "# sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_new_features(df):\n",
    "    ### delta ###\n",
    "    def delta(dat1, dat2): # delta är dat1-dat2\n",
    "        dat1 = pd.to_datetime(dat1)\n",
    "        dat2 = pd.to_datetime(dat2)\n",
    "        delta= dat1-dat2\n",
    "        return delta.dt.days\n",
    "    \n",
    "    df['delta0']=delta(df.datum.copy(),df.h1_dat.copy())\n",
    "    df['delta1']=delta(df.h1_dat, df.h2_dat)\n",
    "    df['delta2']=delta(df.h2_dat, df.h3_dat)\n",
    "    df['delta3']=delta(df.h3_dat, df.h4_dat)\n",
    "    df['delta4']=delta(df.h4_dat, df.h5_dat)\n",
    "    \n",
    "    ### performance ###\n",
    "    df['h1_perf'] = (30-df.h1_plac*2)*df.h1_pris\n",
    "    df['h2_perf'] = (30-df.h2_plac*2)*df.h2_pris\n",
    "    df['h3_perf'] = (30-df.h3_plac*2)*df.h3_pris\n",
    "    df['h4_perf'] = (30-df.h4_plac*2)*df.h4_pris\n",
    "    df['h5_perf'] = (30-df.h5_plac*2)*df.h5_pris\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique(df):\n",
    "    for feat in df:\n",
    "        if len(df[feat].unique())==1:\n",
    "            df.drop(feat,axis=1,inplace=True)\n",
    "    return df       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(df):\n",
    "    df.corr()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df,remove_mer=[]):\n",
    "    #remove_mer=['h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf']\n",
    "    df.drop(['avd','vodds','podds','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1,inplace=True)\n",
    "    df.drop(remove_mer,axis=1,inplace=True)\n",
    "    \n",
    "    df=check_unique(df.copy())\n",
    "    df=check_corr(df.copy())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spara max av Recall, F1, AUC etc till efter learning\n",
    "def spara(model,test_pool,results):\n",
    "    dic=model.eval_metrics(test_pool,['Recall','F1','AUC','Accuracy','Precision'])\n",
    "    dic['best']=model.get_feature_importance(prettified=True)['Feature Id'][0]\n",
    "    dic['worst']=model.get_feature_importance(prettified=True)['Feature Id'].iloc[-1]\n",
    "    results.loc[len(results),:]=[None,None,None,None,None,None,None]\n",
    "    results.loc[len(results)-1,:] = [np.max(dic['Recall']),np.max(dic['F1']),np.max(dic['AUC']),np.max(dic['Accuracy']),np.max(dic['Precision']),dic['best'],dic['worst']]\n",
    "    print('       Recall  F1   AUC   Precision Accuracy')\n",
    "    print('mean:',round(np.mean(results['Recall']),4),round(np.mean(results['F1']),4),round(np.mean(results['AUC']),4),\n",
    "          round(np.mean(results['Accuracy']),4),round(np.mean(results['Precision']),4))\n",
    "    print('std: ',round(np.std(results['Recall']),4),round(np.std(results['F1']),4),round(np.std(results['AUC']),4),\n",
    "          round(np.std(results['Accuracy']),4),round(np.std(results['Precision']),4))\n",
    "    print(f\"best={dic['best']} worst={dic['worst']}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_cat_features(df,cat):\n",
    "    # for c in cat:\n",
    "    #     df.loc[df[c].isna()][c] = 'UNK'\n",
    "\n",
    "    df['start']=(df.start=='AUTOSTART').astype(bool)\n",
    "    return df\n",
    "\n",
    "\n",
    "### split så att allt fram till split_dat blir train och endast en split_dat (en omgång) blir test ###\n",
    "def one_split(df,split_dat):\n",
    "\n",
    "    print(f'len(df)={len(df)}')\n",
    "    \n",
    "    X_train = df.loc[df.datum<fr_dat].copy()\n",
    "    y_train = X_train.plac==1\n",
    "    X_train.drop(['datum','plac'],axis=1,inplace=True)\n",
    "    X_test = df.loc[df.datum==fr_dat].copy()\n",
    "    y_test = X_test.plac==1\n",
    "    X_test.drop(['datum','plac'],axis=1,inplace=True)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_catB(model,train_pool,test_pool): \n",
    "    model.fit(train_pool,eval_set=test_pool, use_best_model=True, verbose=50) \n",
    "    model\n",
    "\n",
    "def estimator_loop(df,cat_features): \n",
    "    results = pd.DataFrame(columns=['Recall','AUC','F1','Precision','Accuracy','best','worst'])\n",
    "    \n",
    "    model = CatBoostClassifier(iterations= 1000, auto_class_weights= 'Balanced', \n",
    "            eval_metric= 'Recall', loss_function= 'Logloss', early_stopping_rounds= 100,\n",
    "            random_seed= 2021)\n",
    "\n",
    "    alla_datum = df.datum.unique()\n",
    "    alla_datum = np.sort(alla_datum)\n",
    "    cut_ix = int(len(alla_datum)*0.75)\n",
    "    print(len(alla_datum[cut_ix:]))\n",
    "    for nr,datum in enumerate(alla_datum[cut_ix:]):\n",
    "        print(nr,'av',len(alla_datum[cut_ix:]))\n",
    "        X_train,X_test,y_train,y_test = one_split(df,datum)\n",
    "    \n",
    "        # print('X_train',X_train,'\\ny_train',y_train)\n",
    "        train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "        test_pool = Pool(data=X_test, label=y_test,cat_features=cat_features)\n",
    "        run_catB(model,train_pool, test_pool)\n",
    "\n",
    "        results=spara(model,test_pool,results)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_types(X):\n",
    "    cat_features = X.select_dtypes('O').columns.to_list()\n",
    "    int_features = X.select_dtypes('int64').columns.to_list()\n",
    "    float_features = X.select_dtypes(float).columns.to_list()\n",
    "    \n",
    "    return cat_features,int_features,float_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('all_data.csv')\n",
    "results = pd.DataFrame(columns=['AUC','Accuracy','F1','best','worst'])\n",
    "\n",
    "#### min egen cv loop ####\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# sortera på datum, avd\n",
    "df.sort_values(by=['datum','avd'],inplace=True)\n",
    "\n",
    "# addera features\n",
    "data = comp_new_features(df)\n",
    "\n",
    "# ta bort alla features som inte kan användas och som jag inte vill ha\n",
    "data = remove_features(data,remove_mer=[])\n",
    "cat_features,int_features,float_features=feature_types(data.drop('datum',axis=1))\n",
    "data=handle_cat_features(data,cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator_loop(data,cat_features)\n",
    "\n",
    "end = time.time()\n",
    "delta = end - start\n",
    "print(f'took {round(delta/60,2)} minutes to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in ['Recall','F1','AUC','Precision','Accuracy']:\n",
    "    list = [np.mean(results[x]),np.mean(results1[x]),np.mean(results2[x]),np.mean(results3[x]),np.mean(results4[x]),np.mean(results5[x]),np.mean(results6[x])]\n",
    "    print(list.index(max(list)))\n",
    "    \n",
    "results    \n"
   ]
  },
  {
   "source": [
    "## Kör en engångs körning utan iteration för att jämföra med PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations= 1000, auto_class_weights= 'Balanced', \n",
    "            eval_metric= 'Recall', loss_function= 'Logloss', early_stopping_rounds= 100,\n",
    "            random_seed= 2021)\n",
    "\n",
    "alla_datum = df.datum.unique()\n",
    "alla_datum = np.sort(alla_datum)\n",
    "cut_ix = int(len(alla_datum)*0.75)\n",
    "startvecka=alla_datum[cut_ix]\n",
    "print('startvecka',startvecka)\n",
    "print(len(alla_datum[cut_ix:]))\n",
    "X_train = data[data.datum<alla_datum[cut_ix]].copy()\n",
    "X_test = data[data.datum >= alla_datum[cut_ix]].copy()\n",
    "y_train = (X_train.plac==1)*1;X_train.drop(['datum','plac'],axis=1,inplace=True)\n",
    "y_test  = (X_test.plac==1)*1;X_test.drop(['datum','plac'],axis=1,inplace=True)\n",
    "cat_features,int_features,float_features =feature_types(X_train)\n",
    "# cat_features.remove('datum')\n",
    "\n",
    "train_pool = Pool(X_train,y_train,cat_features=cat_features)\n",
    "test_pool = Pool(X_test,y_test,cat_features=cat_features)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "run_catB(model,train_pool, test_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### JÄMFÖR RESULTAT MED PCA NEDAN  -  Slutsats: Skit i PCA!\n",
    "dic=model.eval_metrics(test_pool,['AUC','Accuracy','F1','Recall','Precision'])\n",
    "print(f\"EJ PCA:\\tAUC={round(np.max(dic['AUC']),4)}\\tAcc={round(np.max(dic['Accuracy']),4)}\\tF1={round(np.max(dic['F1']),4)} \\t Recall={round(np.max(dic['Recall']),4)}\\tPrecision={round(np.max(dic['Precision']),4)}\")\n",
    "confusion_matrix(y_test,model.predict(X_test))"
   ]
  },
  {
   "source": [
    "# gammalt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcv(X):\n",
    "    X_train, X_test, y_train, y_test = split_data(X)\n",
    "    cat_features, int_features, float_features = feature_types(X_train)\n",
    "    # cat_features= []\n",
    "    int_features=['kr', 'dist', 'lopp_dist', 'ålder']\n",
    "    selected_features = cat_features + float_features  +int_features\n",
    "    cv_pool = Pool(data=X_train[selected_features], label=y_train,cat_features=cat_features)\n",
    "    params = {'iterations': 2000, 'auto_class_weights': 'Balanced', \n",
    "            'eval_metric': 'Accuracy', 'loss_function': 'Logloss', 'early_stopping_rounds': 200,\n",
    "            'use_best_model': True, 'random_seed': 2021,\n",
    "            }\n",
    "\n",
    "    model = CatBoostClassifier( )\n",
    "    scores = cv(cv_pool, params, shuffle = False, fold_count = 30, type='TimeSeries', verbose = 50 )\n",
    "# catcv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catfit(train_pool, test_pool):\n",
    "    model = CatBoostClassifier(  \n",
    "        iterations=2000, \n",
    "        auto_class_weights='Balanced',             \n",
    "        eval_metric = 'Recall',\n",
    "        loss_function = 'Logloss',\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=2021,\n",
    "        )\n",
    "\n",
    "    model.fit(train_pool,\n",
    "        eval_set=test_pool,\n",
    "        use_best_model=True,\n",
    "        #plot=True,\n",
    "        verbose = False,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "\n",
    "Utom 'vodds','podds','h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf'   \n",
    "        AUC     F1  Accuracy  \n",
    "mean: 0.8225 0.7805 0.7795  \n",
    "std:  0.0887 0.0991 0.0827  \n",
    "  \n",
    "Utom 'vodds','podds','h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf', 'h1_perf'  \n",
    "        AUC     F1  Accuracy  \n",
    "mean: 0.8245 0.7777 0.7769  \n",
    "std:  0.0920 0.1028 0.0849  \n",
    "\n",
    " utom 'vodds','podds'  \n",
    "           AUC       F1    Accuracy    \n",
    "mean     0.8242    0.7767    0.7759  \n",
    "std      0.0889    0.1045    0.0876  \n",
    "\n",
    "Utom 'vodds','podds','h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf'   \n",
    "        AUC     F1  Accuracy  \n",
    "mean: 0.8226 0.7756 0.7753  \n",
    "std:  0.0897 0.1044 0.0869  \n",
    "\n",
    "Utom 'vodds','podds','h5_perf','h5_auto','h4_perf','h4_auto', 'h3_perf', 'h2_perf', 'h1_perf','delta4'   \n",
    "       AUC     F1  Accuracy  \n",
    "mean  0.8206 0.7762 0.775  \n",
    "std  0.09 0.1039 0.0865  \n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PCA ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omkodning(data,feat,mapTo):\n",
    "    omk=data[feat].map(mapTo)\n",
    "    omk.fillna(0,inplace=True)\n",
    "    return omk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('all_data.csv')\n",
    "####\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# sortera på datum, avd\n",
    "df.sort_values(by=['datum','avd'],inplace=True)\n",
    "\n",
    "# addera features\n",
    "X = comp_new_features(df)\n",
    "\n",
    "# ta bort alla features som inte kan användas och som jag inte vill ha\n",
    "X = remove_features(X,remove_mer=[])\n",
    "cat_features,int_features,float_features=feature_types(X.drop('datum',axis=1))\n",
    "handle_cat_features(data,cat_features)\n",
    "\n",
    "### häst omkodning ###\n",
    "mapTo = X.häst.value_counts().to_dict()\n",
    "X['chäst']=omkodning(X,'häst',mapTo)\n",
    "X.drop('häst',axis=1,inplace=True)\n",
    "\n",
    "### kusk omkodning ###\n",
    "kusk_dict = X.kusk.value_counts().to_dict()\n",
    "X['ckusk']=X.kusk.map(kusk_dict)\n",
    "X['ch1_kusk']=X.h1_kusk.map(kusk_dict)\n",
    "# X['ch1_kusk'].fillna(0,inplace=True)\n",
    "X['ch2_kusk']=X.h2_kusk.map(kusk_dict)\n",
    "# X['ch2_kusk'].fillna(0,inplace=True)\n",
    "X['ch3_kusk']=X.h3_kusk.map(kusk_dict)\n",
    "# X['ch3_kusk'].fillna(0,inplace=True)\n",
    "X['ch4_kusk']=X.h4_kusk.map(kusk_dict)\n",
    "# X['ch4_kusk'].fillna(0,inplace=True)\n",
    "X['ch5_kusk']=X.h5_kusk.map(kusk_dict)\n",
    "# X['ch5_kusk'].fillna(0,inplace=True)\n",
    "print(X.ch5_kusk.isna().sum())\n",
    "\n",
    "X.drop(['kusk','h1_kusk','h2_kusk','h3_kusk','h4_kusk','h5_kusk'],axis=1,inplace=True)\n",
    "### bana omkodning ###\n",
    "bana_dict = X.bana.value_counts().to_dict()\n",
    "X['cbana']=X.bana.map(bana_dict)\n",
    "X['ch1_bana']=X.h1_bana.str.upper().map(bana_dict)\n",
    "# X['ch1_bana'].fillna(0,inplace=True)\n",
    "X['ch2_bana']=X.h2_bana.str.upper().map(bana_dict)\n",
    "# X['ch2_bana'].fillna(0,inplace=True)\n",
    "X['ch3_bana']=X.h3_bana.str.upper().map(bana_dict)\n",
    "# X['ch3_bana'].fillna(0,inplace=True)\n",
    "X['ch4_bana']=X.h4_bana.str.upper().map(bana_dict)\n",
    "# X['ch4_bana'].fillna(0,inplace=True)\n",
    "X['ch5_bana']=X.h5_bana.str.upper().map(bana_dict)\n",
    "# X['ch5_bana'].fillna(0,inplace=True)\n",
    "\n",
    "print(X.ch5_bana.unique(),'\\n',X.ch5_bana.isna().sum())\n",
    "\n",
    "X.drop(['bana','h1_bana','h2_bana','h3_bana','h4_bana','h5_bana'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alla_datum = df.datum.unique()\n",
    "alla_datum = np.sort(alla_datum)\n",
    "cut_ix = int(len(alla_datum)*0.75)\n",
    "startvecka=alla_datum[cut_ix]\n",
    "print('startvecka',startvecka)\n",
    "print(len(alla_datum[cut_ix:]))\n",
    "print(f'train har {cut_ix} veckor av {len(alla_datum)}')\n",
    "X_train = X[X.datum<alla_datum[cut_ix]].copy()\n",
    "X_test = X[X.datum >= alla_datum[cut_ix]].copy()\n",
    "y_train = (X_train.plac==1)*1;X_train.drop('plac',axis=1,inplace=True)\n",
    "y_test  = (X_test.plac==1)*1;X_test.drop('plac',axis=1,inplace=True)\n",
    "cat_features,int_features,float_features =feature_types(X_train)\n",
    "cat_features.remove('datum')\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import gen_features\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "gen_numeric = gen_features(\n",
    "    columns=[float_features,int_features],\n",
    "    classes=[\n",
    "        {\n",
    "            \"class\": SimpleImputer,\n",
    "            \"strategy\": \"mean\"\n",
    "        },\n",
    "        {\n",
    "            \"class\": StandardScaler\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print('gen_numeric',gen_numeric)\n",
    "gen_gender = (\n",
    "    [\"kön\",\"start\"],\n",
    "    [\n",
    "        SimpleImputer(strategy=\"constant\",fill_value='UNK'),\n",
    "        OneHotEncoder()\n",
    "    ],\n",
    ")\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "preprocess_mapper = DataFrameMapper(\n",
    "    [\n",
    "        gen_gender,\n",
    "        *gen_numeric,\n",
    "    ],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess_mapper),\n",
    "    # (\"estimator\", RandomForestClassifier(n_estimators=100, max_depth=6,class_weight='balanced'))\n",
    "    (\"estimator\", CatBoostClassifier(iterations=1000, \n",
    "        auto_class_weights='Balanced',             \n",
    "        eval_metric = 'Accuracy',\n",
    "        loss_function = 'Logloss',\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=2021,verbose=False))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train*1)\n",
    "preds = pipeline.predict(X_test)\n",
    "print('preds',preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipeline['estimator']\n",
    "model.get_feature_importance(prettified=True)\n",
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,precision_score,recall_score\n",
    "test_pool=Pool(preprocess_mapper.transform(X_test),label=y_test,cat_features=[])\n",
    "preds=model.predict(test_pool)   #[:,0]\n",
    "preds=preds*1\n",
    "print (preds)\n",
    "print(X_test.shape,y_test.shape,preds.shape)\n",
    "print('Acc',round(accuracy_score(y_test,preds*1),4), 'F1',round(f1_score(y_test,preds*1),4),'Pre', round(precision_score(y_test,preds*1),4), 'Rec',round(recall_score(y_test,preds*1),4))\n",
    "confusion_matrix(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(415)/(1095+415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_PCA(nr,X_train,X_test,y_train,y_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # df.sort_values(by=['datum','avd'],inplace=True)\n",
    "    # ta bort alla features som inte kan användas\n",
    "    # X = df.drop(['avd','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1)\n",
    "    # ta bort features som jag inte vill ha\n",
    "    # X = X.drop(['vodds','podds'],axis=1)\n",
    "    # X.dropna(inplace=True)\n",
    "    # X.plac=(X.plac==1)*1\n",
    "    # veckor = X.datum.unique() # blir en datum per vecka\n",
    "    # startvecka = int(len(veckor) / 1.4)\n",
    "    # X_train = X[X.datum<veckor[startvecka]].copy()\n",
    "    # y_train = X_train.plac; X_train.drop(['plac','datum'],axis=1,inplace=True)\n",
    "    # X_test = X[X.datum >=veckor[startvecka]].copy()\n",
    "    # y_test = X_test.plac ; X_test.drop(['datum','plac'],axis=1,inplace=True)\n",
    "          \n",
    "    cat_features,int_features,float_features=feature_types(X_train)\n",
    "    # X_train = X_train[int_features+float_features]\n",
    "    # X_test = X_test[int_features+float_features]\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    pca = PCA(nr)\n",
    "\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X_train,X_test,y_train,y_test):\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=[])\n",
    "    test_pool  = Pool(data=X_test, label=y_test,cat_features=[])\n",
    "    model = catfit(train_pool,test_pool) \n",
    "    return model,train_pool,test_pool\n",
    "# model.get_feature_importance(prettified=True)\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.drop('datum',axis=1,inplace=True)\n",
    "#X_test.drop('datum',axis=1,inplace=True)\n",
    "\n",
    "for nr in [65,55,47,45,40,35,30,20,10,5]:\n",
    "    pX_train=preprocess_mapper.fit_transform(X_train)\n",
    "    pX_test=preprocess_mapper.transform(X_test)\n",
    "    pX_train,pX_test,y_train,y_test=run_with_PCA(nr,pX_train,pX_test,y_train,y_test)\n",
    "    model,train_pool,test_pool=get_model(pX_train,pX_test,y_train,y_test)\n",
    "    dic=model.eval_metrics(test_pool,['AUC','Accuracy','F1','Recall','Precision'])\n",
    "\n",
    "# print(np.max(dic['AUC']),np.max(dic['F1']),np.max(dic['Accuracy']))\n",
    "\n",
    "    print(f\"PCAnr={nr} \\tAUC={round(np.max(dic['AUC']),4)}\\tAcc={round(np.max(dic['Accuracy']),4)}\\tF1={round(np.max(dic['F1']),4)} \\t Recall={round(np.max(dic['Recall']),4)}\\tPrecision={round(np.max(dic['Precision']),4)}\")\n"
   ]
  },
  {
   "source": [
    "EJ PCA:\tAUC=0.7916\tAcc=0.7342\tF1=0.7517 \t Recall=0.8122\tPrecision=0.7228  \n",
    "conf Matrix  \n",
    "6111, 3197  \n",
    "163,  705"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['avd','h1_dat','h2_dat','h3_dat','h4_dat','h5_dat'],axis=1)\n",
    "# ta bort features som jag inte vill ha\n",
    "X = X.drop(['vodds','podds'],axis=1)\n",
    "X.dropna(inplace=True)\n",
    "X.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.utils import get_confusion_matrix\n",
    "# print(model.predict_proba(test_pool)[:,0])\n",
    "y_test.values\n",
    "get_confusion_matrix(model,test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(X)-2: {len(X.columns)-2} \\ncat: {cat_features}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.iloc[:10,:10]\n",
    "test_df['plac'] = (df.iloc[:10,:].plac==1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import gen_features\n",
    "\n",
    "numeric_features = [[\"avd\"], [\"streck\"], [\"vodds\"], [\"podds\"], [\"kr\"],[\"spår\"]]\n",
    "\n",
    "gen_numeric = gen_features(\n",
    "    columns=numeric_features,\n",
    "    classes=[\n",
    "        {\n",
    "            \"class\": SimpleImputer,\n",
    "            \"strategy\": \"mean\"\n",
    "        },\n",
    "        {\n",
    "            \"class\": StandardScaler\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "mapper = DataFrameMapper([\n",
    "    (['vodds','podds','spår'], sklearn.preprocessing.StandardScaler()),\n",
    "    ('plac', sklearn.preprocessing.LabelBinarizer()),\n",
    "     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.fit_transform(test_df)\n",
    "\n",
    "# sklearn.preprocessing.LabelBinarizer(test_df.plac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}